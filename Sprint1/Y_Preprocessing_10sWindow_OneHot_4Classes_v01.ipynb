{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECG Multi-Label Preprocessing (Self-Contained)\n",
    "Fully independent notebook: loads raw data, processes, creates 5-class one-hot labels, uploads to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wfdb neurokit2 scikit-learn scipy matplotlib pandas numpy huggingface-hub h5py -q\n",
    "!apt-get update && apt-get install -y p7zip-full\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wfdb\n",
    "import h5py\n",
    "from scipy.signal import butter, filtfilt, welch, resample\n",
    "import neurokit2 as nk\n",
    "from datetime import datetime\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to True to download from Huggingface else use Google Drive\n",
    "USE_HF = True\n",
    "\n",
    "if USE_HF:\n",
    "  from huggingface_hub import snapshot_download\n",
    "  local_dir = snapshot_download(\n",
    "      repo_id=\"kiril-buga/ECG-database\",\n",
    "      repo_type=\"dataset\",\n",
    "      local_dir=\"/content/ECG-database/\" # Specify the desired download directory\n",
    "  )\n",
    "  print(\"Downloaded to:\", local_dir)\n",
    "\n",
    "    DATA_PATH = f\"{local_dir}/data/\"\n",
    "    ARTIFACT_DIR = f\"{local_dir}/artifacts/\"\n",
    "\n",
    "else:\n",
    "    # Detect environment and mount drive if Colab\n",
    "    IN_COLAB = False\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        IN_COLAB = True\n",
    "        drive.mount('/content/drive/')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Set paths\n",
    "    if IN_COLAB:\n",
    "        DATA_PATH = \"/content/drive/MyDrive/DeepLearningECG/data/\"\n",
    "        ARTIFACT_DIR = \"/content/drive/MyDrive/DeepLearningECG/artifacts/\"\n",
    "    else:\n",
    "        DATA_PATH = \"../DeepLearningECG/data/\"\n",
    "        ARTIFACT_DIR = \"../DeepLearningECG/artifacts/\"\n",
    "\n",
    "ECG_DIR = os.path.join(DATA_PATH, \"Child_ecg/\")\n",
    "OUT_DIR = os.path.join(ARTIFACT_DIR, \"multilabel_v2\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Colab: {IN_COLAB}\")\n",
    "print(f\"DATA: {DATA_PATH}\")\n",
    "print(f\"OUTPUT: {OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd DATA_PATH && 7z x Child_ecg.zip\n",
    "print(\"✓ Extraction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = os.path.join(DATA_PATH, 'AttributesDictionary.csv')\n",
    "\n",
    "if os.path.exists(csv_path):\n",
    "    df_attr = pd.read_csv(csv_path)\n",
    "    print(f\"Loaded CSV: {df_attr.shape}\")\n",
    "else:\n",
    "    from huggingface_hub import hf_hub_download\n",
    "    print(\"Downloading CSV from Hugging Face...\")\n",
    "    csv_file = hf_hub_download(\n",
    "        repo_id=\"kiril-buga/ECG-database\",\n",
    "        filename=\"AttributesDictionary.csv\",\n",
    "        repo_type=\"dataset\"\n",
    "    )\n",
    "    df_attr = pd.read_csv(csv_file)\n",
    "    print(f\"Loaded CSV: {df_attr.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_bandpass(x, fs, lowcut=0.5, highcut=40.0):\n",
    "    if x.ndim == 1:\n",
    "        x = x[:, None]\n",
    "    nyq = 0.5 * fs\n",
    "    b, a = butter(4, [lowcut/nyq, highcut/nyq], btype=\"band\")\n",
    "    return np.column_stack([filtfilt(b, a, x[:, i]) for i in range(x.shape[1])])\n",
    "\n",
    "def band_power(f, Pxx, fmin, fmax):\n",
    "    mask = (f >= fmin) & (f <= fmax)\n",
    "    return np.trapz(Pxx[mask], f[mask]) if np.any(mask) else 0.0\n",
    "\n",
    "HAS_NK = True\n",
    "try:\n",
    "    import neurokit2\n",
    "except:\n",
    "    HAS_NK = False\n",
    "\n",
    "def compute_qc(sig, meta, pSQI_mean, bSQI_mean):\n",
    "    \"\"\"Compute QC metrics.\"\"\"\n",
    "    qc = {\"pSQI_mean\": pSQI_mean, \"bSQI_mean\": bSQI_mean}\n",
    "    \n",
    "    fs = meta.get(\"fs\", None)\n",
    "    if fs is None:\n",
    "        fs = getattr(meta, \"fs\", None)\n",
    "    if fs is None:\n",
    "        raise ValueError(\"Missing fs\")\n",
    "    \n",
    "    if sig.ndim == 1:\n",
    "        sig = sig[:, None]\n",
    "    \n",
    "    n_samples, n_leads = sig.shape\n",
    "    qc[\"n_samples\"] = int(n_samples)\n",
    "    qc[\"n_leads\"] = int(n_leads)\n",
    "    qc[\"duration_sec\"] = n_samples / fs\n",
    "    \n",
    "    lead = sig[:, 0]\n",
    "    n_nans = np.isnan(lead).sum()\n",
    "    qc[\"nan_fraction\"] = float(n_nans / len(lead))\n",
    "    \n",
    "    lead_clean = lead.copy()\n",
    "    if n_nans > 0:\n",
    "        not_nan = ~np.isnan(lead_clean)\n",
    "        if not np.any(not_nan):\n",
    "            return {**qc, \"qc_pass\": False, \"fail_reason\": \"all_nan\"}\n",
    "        lead_clean[~not_nan] = np.interp(np.flatnonzero(~not_nan), \n",
    "                                          np.flatnonzero(not_nan), lead_clean[not_nan])\n",
    "    \n",
    "    amp = lead_clean\n",
    "    qc[\"amp_mean\"] = float(np.mean(amp))\n",
    "    qc[\"amp_std\"] = float(np.std(amp))\n",
    "    q1, q99 = np.percentile(amp, [1, 99])\n",
    "    qc[\"amp_robust_range\"] = float(q99 - q1)\n",
    "    \n",
    "    f, Pxx = welch(amp, fs=fs, nperseg=min(4096, len(amp)))\n",
    "    qc[\"baseline_wander_ratio\"] = band_power(f, Pxx, 0.0, 0.5) / (band_power(f, Pxx, 0.5, 40.0) + 1e-8)\n",
    "    qc[\"powerline_ratio\"] = band_power(f, Pxx, 48.0, 52.0) / (band_power(f, Pxx, 40.0, 60.0) + 1e-8)\n",
    "    \n",
    "    reasons = []\n",
    "    if qc[\"duration_sec\"] < 8.0: reasons.append(\"too_short\")\n",
    "    if qc[\"nan_fraction\"] > 0.01: reasons.append(\"too_many_nans\")\n",
    "    if not (0.05 < qc[\"amp_robust_range\"] < 10.0): reasons.append(\"amp_out_of_range\")\n",
    "    if qc[\"baseline_wander_ratio\"] > 0.5: reasons.append(\"baseline_wander\")\n",
    "    if qc[\"powerline_ratio\"] > 0.5: reasons.append(\"powerline_noise\")\n",
    "    if pSQI_mean < 0.2: reasons.append(\"low_pSQI\")\n",
    "    if bSQI_mean < 0.8: reasons.append(\"low_bSQI\")\n",
    "    \n",
    "    qc[\"qc_pass\"] = len(reasons) == 0\n",
    "    qc[\"fail_reason\"] = \";\".join(reasons) if reasons else \"\"\n",
    "    \n",
    "    return qc\n",
    "\n",
    "print(\"✓ Processing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing & Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def preprocess_record(sig, meta, target_fs=500.0):\n    fs = meta.get(\"fs\", None) or getattr(meta, \"fs\", None)\n    if sig.ndim == 1:\n        sig = sig[:, None]\n    \n    sig_bp = apply_bandpass(sig, fs=fs)\n    if fs == target_fs:\n        return sig_bp, fs\n    \n    n_samples = sig_bp.shape[0]\n    n_new = int(round(n_samples / fs * target_fs))\n    sig_res = np.column_stack([resample(sig_bp[:, i], n_new) for i in range(sig_bp.shape[1])])\n    return sig_res, target_fs\n\ndef window_record(sig, fs, window_sec=10.0, step_sec=5.0, target_samples=None):\n    \"\"\"\n    Create windows from preprocessed ECG signal.\n    Pads/truncates to fixed sample length to ensure consistent shapes.\n    \n    Parameters:\n    - sig: (n_samples, n_leads) signal array\n    - fs: sampling frequency (Hz)\n    - window_sec: window duration in seconds\n    - step_sec: step duration in seconds  \n    - target_samples: target number of samples per window (default: window_sec * fs)\n    \n    Returns:\n    - List of windows with shape (target_samples, n_leads)\n    \"\"\"\n    if sig.ndim == 1:\n        sig = sig[:, None]\n    \n    n_samples = sig.shape[0]\n    n_leads = sig.shape[1]\n    win_len = int(window_sec * fs)\n    step_len = int(step_sec * fs)\n    \n    if target_samples is None:\n        target_samples = win_len\n    \n    windows = []\n    start = 0\n    \n    while start + win_len <= n_samples:\n        segment = sig[start:start + win_len, :]\n        \n        # Skip windows with too many NaNs\n        if np.isnan(segment).mean() > 0.05:\n            start += step_len\n            continue\n        \n        # Normalize each channel\n        seg_norm = segment.copy()\n        for ch in range(seg_norm.shape[1]):\n            x = seg_norm[:, ch]\n            m, s = np.nanmean(x), np.nanstd(x)\n            seg_norm[:, ch] = (x - m) / (s if s > 1e-6 else 1.0)\n        \n        # Pad or truncate to target_samples\n        if seg_norm.shape[0] < target_samples:\n            pad_len = target_samples - seg_norm.shape[0]\n            seg_norm = np.pad(seg_norm, ((0, pad_len), (0, 0)), mode='constant', constant_values=0)\n        elif seg_norm.shape[0] > target_samples:\n            seg_norm = seg_norm[:target_samples, :]\n        \n        windows.append(seg_norm.astype(np.float16))  # Use float16 for compression\n        start += step_len\n    \n    return windows\n\nprint(\"✓ Preprocessing functions defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICD Code Parsing & Disease Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICD_TO_DISEASE = {\n",
    "    'I40.0': 'Myocarditis', 'I40.9': 'Myocarditis', 'I41.4': 'Myocarditis',\n",
    "    'I42.0': 'Cardiomyopathy', 'I42.2': 'Cardiomyopathy', 'I42.9': 'Cardiomyopathy', 'Q28.4': 'Cardiomyopathy',\n",
    "    'M30.3': 'Kawasaki',\n",
    "    'Q21.1': 'CHD', 'Q21.2': 'CHD', 'Q21.3': 'CHD', 'Q22.1': 'CHD', 'Q25.0': 'CHD', 'Q25.6': 'CHD', 'I27.9': 'CHD',\n",
    "}\n",
    "\n",
    "DISEASE_CLASSES = ['Myocarditis', 'Cardiomyopathy', 'Kawasaki', 'CHD', 'Healthy']\n",
    "CLASS_IDX = {c: i for i, c in enumerate(DISEASE_CLASSES)}\n",
    "\n",
    "def parse_icd(s):\n",
    "    if pd.isna(s):\n",
    "        return []\n",
    "    return [p.strip().replace(\"'\", \"\") for p in str(s).split(\";\") if p.strip()]\n",
    "\n",
    "def clean_icd(code):\n",
    "    if pd.isna(code):\n",
    "        return None\n",
    "    code_str = str(code).strip()\n",
    "    if ')' in code_str:\n",
    "        code_str = code_str.split(')')[-1].strip()\n",
    "    return code_str or None\n",
    "\n",
    "def parse_sqi(s):\n",
    "    if pd.isna(s):\n",
    "        return {}\n",
    "    out = {}\n",
    "    for item in str(s).split(\";\"):\n",
    "        if \":\" in item:\n",
    "            k, v = item.split(\":\")\n",
    "            try:\n",
    "                out[k.replace(\"'\", \"\").strip()] = float(v)\n",
    "            except:\n",
    "                pass\n",
    "    return out\n",
    "\n",
    "# Parse ICD codes\n",
    "df_attr[\"ICD_list\"] = df_attr[\"ICD-10 code\"].apply(parse_icd)\n",
    "df_attr[\"ICD_primary\"] = df_attr[\"ICD_list\"].apply(lambda x: x[0] if x else None)\n",
    "df_attr[\"ICD_primary_clean\"] = df_attr[\"ICD_primary\"].apply(clean_icd)\n",
    "df_attr[\"disease\"] = df_attr[\"ICD_primary_clean\"].apply(lambda x: ICD_TO_DISEASE.get(x, 'Healthy') if x else 'Healthy')\n",
    "\n",
    "# Parse SQI\n",
    "for col in [\"pSQI\", \"basSQI\", \"bSQI\"]:\n",
    "    df_attr[f\"{col}_dict\"] = df_attr[col].apply(parse_sqi)\n",
    "    df_attr[f\"{col}_mean\"] = df_attr[f\"{col}_dict\"].apply(lambda d: np.mean(list(d.values())) if d else np.nan)\n",
    "\n",
    "print(\"Disease distribution:\")\n",
    "print(df_attr['disease'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def process_all_hdf5(df, ecg_dir, max_records=None, target_samples=5000, target_channels=12, output_file=None):\n    \"\"\"\n    Process ECG records and save directly to HDF5 with compression.\n    Memory-efficient: streams windows to disk without stacking.\n    \n    Parameters:\n    - output_file: Path to save HDF5 file (default: OUT_DIR/ecg_data.h5)\n    \n    Compression: gzip level 4 + float16 reduces 15GB → ~1.5-2GB\n    \"\"\"\n    if output_file is None:\n        output_file = os.path.join(OUT_DIR, \"ecg_data.h5\")\n    \n    # First pass: count total windows\n    print(\"Pass 1: Counting total windows...\")\n    total_windows = 0\n    qc_list_prepass = []\n    \n    iterator = df.iloc[:max_records].iterrows() if max_records else df.iterrows()\n    total_records = max_records if max_records else len(df)\n    \n    for idx, row in iterator:\n        if (idx + 1) % 100 == 0:\n            print(f\"  Scanning [{idx + 1}/{total_records}]...\")\n        \n        fname = row[\"Filename\"]\n        path = os.path.join(ecg_dir, fname)\n        \n        try:\n            sig, meta = wfdb.rdsamp(path)\n        except:\n            continue\n        \n        meta_dict = meta if isinstance(meta, dict) else meta.__dict__\n        sig = np.asarray(sig)\n        \n        qc = compute_qc(sig, meta_dict, float(row[\"pSQI_mean\"]), float(row[\"bSQI_mean\"]))\n        qc_list_prepass.append(qc)\n        \n        if not qc[\"qc_pass\"]:\n            continue\n        \n        sig_proc, fs = preprocess_record(sig, meta_dict)\n        windows = window_record(sig_proc, fs, target_samples=target_samples)\n        total_windows += len(windows)\n    \n    print(f\"\\n✓ Estimated {total_windows} windows\")\n    \n    # Second pass: write to HDF5 with compression\n    print(f\"\\nPass 2: Writing to HDF5 with compression ({output_file})...\")\n    \n    with h5py.File(output_file, 'w') as h5f:\n        # Create datasets with gzip compression (level 4 = good balance speed/compression)\n        X_dset = h5f.create_dataset(\n            'X', \n            shape=(total_windows, target_samples, target_channels), \n            dtype=np.float16,  # Use float16 instead of float32\n            compression='gzip',\n            compression_opts=4\n        )\n        y_dset = h5f.create_dataset(\n            'y', \n            shape=(total_windows, len(DISEASE_CLASSES)), \n            dtype=np.int32,\n            compression='gzip',\n            compression_opts=4\n        )\n        diseases_dset = h5f.create_dataset(\n            'diseases', \n            shape=(total_windows,), \n            dtype=h5py.string_dtype(encoding='utf-8'),\n            compression='gzip',\n            compression_opts=4\n        )\n        \n        # Store metadata\n        h5f.attrs['target_samples'] = target_samples\n        h5f.attrs['target_channels'] = target_channels\n        h5f.attrs['disease_classes'] = DISEASE_CLASSES\n        h5f.attrs['data_format'] = 'float16 + gzip'\n        \n        qc_list = []\n        all_diseases = []\n        window_idx = 0\n        \n        # Iterate through records again\n        iterator = df.iloc[:max_records].iterrows() if max_records else df.iterrows()\n        \n        for idx, row in iterator:\n            if (idx + 1) % 10 == 0:\n                print(f\"  [{idx + 1}/{total_records}] {window_idx}/{total_windows} windows written...\")\n            \n            fname = row[\"Filename\"]\n            disease = row[\"disease\"]\n            path = os.path.join(ecg_dir, fname)\n            \n            try:\n                sig, meta = wfdb.rdsamp(path)\n            except Exception as e:\n                qc_list.append({\"Filename\": fname, \"disease\": disease, \"qc_pass\": False, \"fail_reason\": str(e)})\n                continue\n            \n            meta_dict = meta if isinstance(meta, dict) else meta.__dict__\n            sig = np.asarray(sig)\n            original_channels = sig.shape[1]\n            \n            qc = compute_qc(sig, meta_dict, float(row[\"pSQI_mean\"]), float(row[\"bSQI_mean\"]))\n            qc[\"Filename\"] = fname\n            qc[\"disease\"] = disease\n            qc[\"original_channels\"] = original_channels\n            \n            if not qc[\"qc_pass\"]:\n                qc_list.append(qc)\n                continue\n            \n            sig_proc, fs = preprocess_record(sig, meta_dict)\n            windows = window_record(sig_proc, fs, target_samples=target_samples)\n            \n            # Pad and write windows directly to HDF5\n            for window in windows:\n                if window.shape[1] < target_channels:\n                    pad_channels = target_channels - window.shape[1]\n                    window = np.pad(window, ((0, 0), (0, pad_channels)), mode='constant', constant_values=0)\n                elif window.shape[1] > target_channels:\n                    window = window[:, :target_channels]\n                \n                # Write to HDF5 (automatically compressed)\n                X_dset[window_idx] = window\n                y_dset[window_idx, CLASS_IDX[disease]] = 1\n                diseases_dset[window_idx] = disease\n                \n                window_idx += 1\n                all_diseases.append(disease)\n            \n            qc[\"n_windows\"] = len(windows)\n            qc_list.append(qc)\n    \n    print(f\"\\n✓ Saved to {output_file}\")\n    print(f\"  X shape: (windows, samples, channels) = ({window_idx}, {target_samples}, {target_channels})\")\n    print(f\"  Data format: float16 + gzip compression\")\n    \n    return output_file, pd.DataFrame(qc_list)\n\n# Run processing\nprint(\"Processing ECG records...\")\nh5_file, df_qc = process_all_hdf5(df_attr, ECG_DIR, max_records=None)\n\n# Verify file\nwith h5py.File(h5_file, 'r') as h5f:\n    print(f\"\\nDataset shapes:\")\n    print(f\"  X: {h5f['X'].shape} (dtype: {h5f['X'].dtype})\")\n    print(f\"  y: {h5f['y'].shape}\")\n    print(f\"  diseases: {h5f['diseases'].shape}\")\n    \n    print(f\"\\nDisease distribution:\")\n    for i, cls in enumerate(DISEASE_CLASSES):\n        count = h5f['y'][:, i].sum()\n        print(f\"  {cls}: {count}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save QC summary and metadata\n",
    "df_qc[['Filename', 'disease', 'qc_pass', 'n_windows', 'original_channels']].to_csv(\n",
    "    os.path.join(OUT_DIR, \"qc_summary.csv\"), index=False\n",
    ")\n",
    "\n",
    "with open(os.path.join(OUT_DIR, \"disease_classes.json\"), \"w\") as f:\n",
    "    json.dump({\n",
    "        \"classes\": DISEASE_CLASSES, \n",
    "        \"class_idx\": CLASS_IDX, \n",
    "        \"icd_map\": ICD_TO_DISEASE,\n",
    "        \"data_format\": \"hdf5\",\n",
    "        \"hdf5_file\": \"ecg_data.h5\"\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"✓ Saved to {OUT_DIR}\")\n",
    "print(f\"  - ecg_data.h5 (HDF5 format)\")\n",
    "print(f\"  - qc_summary.csv\")\n",
    "print(f\"  - disease_classes.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Hugging Face (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPLOAD_TO_HF = False  # Set to True to upload\n",
    "\n",
    "if UPLOAD_TO_HF:\n",
    "    from huggingface_hub import HfApi, login\n",
    "    \n",
    "    print(\"Logging into Hugging Face...\")\n",
    "    login()\n",
    "    \n",
    "    api = HfApi()\n",
    "    print(\"Uploading to HF...\")\n",
    "    api.upload_folder(\n",
    "        folder_path=OUT_DIR,\n",
    "        repo_id=\"kiril-buga/ECG-database\",\n",
    "        repo_type=\"dataset\",\n",
    "        path_in_repo=\"multilabel_v2\",\n",
    "        commit_message=\"Multi-label preprocessed data\"\n",
    "    )\n",
    "    print(\"✓ Uploaded to HF\")\n",
    "else:\n",
    "    print(\"To upload: set UPLOAD_TO_HF=True and have HF write token\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}