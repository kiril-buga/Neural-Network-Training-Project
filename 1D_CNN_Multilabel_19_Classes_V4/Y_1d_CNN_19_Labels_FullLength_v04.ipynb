{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kiril-buga/Neural-Network-Training-Project/blob/main/1D_CNN_Multilabel_19_Classes_V4/Y_1d_CNN_19_Labels_FullLength_v04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gp3Ir4FFy028"
   },
   "source": [
    "# Enhanced 1D CNN for 19-Class Multi-Label ECG Classification (v04)\n",
    "## This additional experiment is exploratory\n",
    "\n",
    "**Key Features:**\n",
    "- 19-class multi-label classification (18 cardiac diseases + Healthy)\n",
    "- **Full-length variable recordings** (5-120 seconds, no windowing)\n",
    "    \"- **Simplified architecture** for better generalization on imbalanced data\n",
    "- Focal loss for class imbalance\n",
    "- **Outputs show disease names only** (not ICD codes)\n",
    "\n",
    "    \"**Architecture:** 128→256→256 filters (simplified, no SE/Attention/Residual)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fz5AeL22y02_"
   },
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JN2A1TUpy02_",
    "outputId": "836e814b-4766-4e73-9213-f3cc613bb3c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m708.4/708.4 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m144.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install wfdb neurokit2 h5py tensorflow scikit-learn matplotlib seaborn pandas numpy scipy -q\n",
    "\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7yd7XDNcy03B",
    "outputId": "46061242-00f6-496e-8be6-92ef982e033b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "Num GPUs Available: 1\n",
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import wfdb\n",
    "from scipy.signal import butter, filtfilt, resample\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, hamming_loss, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Num GPUs Available: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OBVb-NJuy03C",
    "outputId": "4edd82af-354c-42a3-94f4-13d556d20b94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Augmentation functions defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MEMORY OPTIMIZATION: ECG Augmentation Functions\n",
    "# ============================================================================\n",
    "# OPTIONAL: These functions enable data augmentation for minority classes\n",
    "# Currently DISABLED by default to avoid training overhead\n",
    "# Enable by setting augment=True in create_optimized_dataset() calls\n",
    "def augment_ecg_noise(signal, snr_db=25):\n",
    "    \"\"\"Add Gaussian noise with specified SNR\"\"\"\n",
    "    signal_power = np.mean(signal ** 2)\n",
    "    noise_power = signal_power / (10 ** (snr_db / 10))\n",
    "    noise = np.random.normal(0, np.sqrt(noise_power), signal.shape)\n",
    "    return signal + noise\n",
    "\n",
    "def augment_ecg_time_shift(signal, fs=500, max_shift_ms=200):\n",
    "    \"\"\"Shift signal in time by random amount\"\"\"\n",
    "    max_shift_samples = int(max_shift_ms * fs / 1000)\n",
    "    shift = np.random.randint(-max_shift_samples, max_shift_samples + 1)\n",
    "    if shift > 0:\n",
    "        return np.pad(signal, ((shift, 0), (0, 0)), mode='edge')[:-shift]\n",
    "    elif shift < 0:\n",
    "        return np.pad(signal, ((0, -shift), (0, 0)), mode='edge')[-shift:]\n",
    "    return signal\n",
    "\n",
    "def augment_ecg_amplitude(signal, scale_range=(0.9, 1.1)):\n",
    "    \"\"\"Scale amplitude randomly\"\"\"\n",
    "    scale = np.random.uniform(*scale_range)\n",
    "    return signal * scale\n",
    "\n",
    "def augment_ecg_baseline_wander(signal, fs=500, freq_range=(0.1, 0.5), amplitude=0.05):\n",
    "    \"\"\"Add realistic baseline wander\"\"\"\n",
    "    freq = np.random.uniform(*freq_range)\n",
    "    t = np.arange(signal.shape[0]) / fs\n",
    "    wander = amplitude * np.sin(2 * np.pi * freq * t)\n",
    "    return signal + wander[:, np.newaxis]\n",
    "\n",
    "def augment_ecg(signal, p=0.8):\n",
    "    \"\"\"\n",
    "    Apply random augmentations to ECG signal.\n",
    "\n",
    "    Args:\n",
    "        signal: (length, channels) array\n",
    "        p: probability of applying each augmentation\n",
    "\n",
    "    Returns:\n",
    "        Augmented signal\n",
    "    \"\"\"\n",
    "    signal = signal.copy()\n",
    "\n",
    "    if np.random.random() < p:\n",
    "        signal = augment_ecg_noise(signal, snr_db=np.random.uniform(20, 30))\n",
    "    if np.random.random() < p * 0.7:\n",
    "        signal = augment_ecg_time_shift(signal)\n",
    "    if np.random.random() < p * 0.6:\n",
    "        signal = augment_ecg_amplitude(signal)\n",
    "    if np.random.random() < p * 0.4:\n",
    "        signal = augment_ecg_baseline_wander(signal)\n",
    "\n",
    "    return signal\n",
    "\n",
    "print(\"✓ Augmentation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PGgbfbd4y03C",
    "outputId": "49b70a4c-3259-4a33-f776-d39f7cfa0096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LazyHDF5Loader class defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MEMORY OPTIMIZATION: Lazy HDF5 Data Loader\n",
    "# ============================================================================\n",
    "# Loads samples on-demand instead of loading everything into RAM\n",
    "# RAM savings: ~18 GB → ~0.5 GB\n",
    "\n",
    "class LazyHDF5Loader:\n",
    "    \"\"\"\n",
    "    Lazy loader for HDF5 data - keeps file open and loads samples on-demand.\n",
    "    Dramatically reduces RAM usage.\n",
    "    \"\"\"\n",
    "    def __init__(self, h5_file_path):\n",
    "        self.h5_file_path = h5_file_path\n",
    "        self.h5_file = None\n",
    "        self._load_metadata()\n",
    "\n",
    "    def _load_metadata(self):\n",
    "        \"\"\"Load only metadata (labels, lengths, bin indices)\"\"\"\n",
    "        with h5py.File(self.h5_file_path, 'r') as h5f:\n",
    "            self.y_all = []\n",
    "            self.lengths_all = []\n",
    "            self.bin_indices = []  # (bin_name, index_within_bin)\n",
    "\n",
    "            for bin_name in sorted(h5f.keys()):\n",
    "                if not bin_name.startswith('bin_'):\n",
    "                    continue\n",
    "\n",
    "                grp = h5f[bin_name]\n",
    "                y_bin = grp['y'][:]  # Labels are small, OK to load\n",
    "                lengths_bin = grp['lengths'][:]\n",
    "\n",
    "                for i in range(len(y_bin)):\n",
    "                    self.y_all.append(y_bin[i])\n",
    "                    self.lengths_all.append(lengths_bin[i])\n",
    "                    self.bin_indices.append((bin_name, i))\n",
    "\n",
    "            self.y_all = np.array(self.y_all)\n",
    "            self.lengths_all = np.array(self.lengths_all)\n",
    "\n",
    "        print(f\"✓ Loaded metadata for {len(self.y_all)} samples (minimal RAM)\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_all)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Load a single sample on-demand\"\"\"\n",
    "        if self.h5_file is None:\n",
    "            self.h5_file = h5py.File(self.h5_file_path, 'r')\n",
    "\n",
    "        bin_name, bin_idx = self.bin_indices[idx]\n",
    "        grp = self.h5_file[bin_name]\n",
    "\n",
    "        # Load only this sample\n",
    "        X = grp['X'][bin_idx, :self.lengths_all[idx], :]\n",
    "        y = self.y_all[idx]\n",
    "\n",
    "        return X, y, self.lengths_all[idx]\n",
    "\n",
    "    def close(self):\n",
    "        if self.h5_file is not None:\n",
    "            self.h5_file.close()\n",
    "            self.h5_file = None\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "print(\"✓ LazyHDF5Loader class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PAqMyCDcy03D",
    "outputId": "b447faef-3c88-4bac-dc8b-d81a319eb1e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Oversampling calculation function defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MEMORY OPTIMIZATION: Calculate Oversampling Rates\n",
    "# ============================================================================\n",
    "# Uses index-based sampling instead of duplicating data\n",
    "\n",
    "def calculate_oversample_rates(y_data, target_samples_per_class=None):\n",
    "    \"\"\"\n",
    "    Calculate how many times to repeat each sample to balance classes.\n",
    "\n",
    "    Args:\n",
    "        y_data: (n_samples, n_classes) one-hot encoded labels\n",
    "        target_samples_per_class: Target number per class (default: max class count)\n",
    "\n",
    "    Returns:\n",
    "        oversample_rates: Array of repetition counts for each sample\n",
    "    \"\"\"\n",
    "    class_counts = y_data.sum(axis=0)\n",
    "    class_labels = y_data.argmax(axis=1)\n",
    "\n",
    "    if target_samples_per_class is None:\n",
    "        # Use the largest disease class (not Healthy)\n",
    "        healthy_idx = np.where(np.array(DISEASE_NAMES) == 'Healthy')[0][0]\n",
    "        disease_counts = [class_counts[i] for i in range(len(class_counts)) if i != healthy_idx]\n",
    "        target_samples_per_class = int(max(disease_counts))\n",
    "\n",
    "    print(f\"\\nOversampling strategy:\")\n",
    "    print(f\"  Target samples per class: {target_samples_per_class}\")\n",
    "\n",
    "    oversample_rates = np.ones(len(y_data), dtype=np.int32)\n",
    "\n",
    "    for class_idx in range(len(class_counts)):\n",
    "        class_mask = (class_labels == class_idx)\n",
    "        count = int(class_counts[class_idx])\n",
    "\n",
    "        if count == 0:\n",
    "            continue\n",
    "\n",
    "        # Calculate repetition rate\n",
    "        if DISEASE_NAMES[class_idx] == 'Healthy':\n",
    "            # Undersample Healthy to target\n",
    "            rate = 1  # Don't oversample Healthy\n",
    "        else:\n",
    "            # Oversample diseases to target\n",
    "            rate = max(1, int(target_samples_per_class / count))\n",
    "\n",
    "        oversample_rates[class_mask] = rate\n",
    "\n",
    "        print(f\"  {DISEASE_NAMES[class_idx]:40s}: {count:5d} samples × {rate:2d} = {count * rate:5d}\")\n",
    "\n",
    "    return oversample_rates\n",
    "\n",
    "print(\"✓ Oversampling calculation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwKGjKXuy03D"
   },
   "source": [
    "## 1.1 GPU Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i-feSiG1y03D",
    "outputId": "3ba0e292-d1b1-4254-8e35-e27034649d7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GPU memory growth enabled: /physical_device:GPU:0\n",
      "✓ Num GPUs Available: 1\n"
     ]
    }
   ],
   "source": [
    "# GPU Configuration - Optimized for Colab T4 GPU (15 GB)\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Enable memory growth (allocate GPU memory dynamically)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"✓ GPU memory growth enabled: {gpus[0].name}\")\n",
    "        print(f\"✓ Num GPUs Available: {len(gpus)}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU configuration error: {e}\")\n",
    "else:\n",
    "    print(\"⚠️  WARNING: No GPU detected! Training will be VERY slow.\")\n",
    "    print(f\"   Current TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mKaUHWzhy03E",
    "outputId": "c477e63c-7829-4ba8-86a5-56a01e56c439"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️  GPU troubleshooting options available (see cell comments)\n"
     ]
    }
   ],
   "source": [
    "# ALTERNATIVE: If GPU still fails, set memory limit or force CPU\n",
    "\n",
    "# Option 1: Set explicit GPU memory limit (use only 2GB of your 4GB GPU)\n",
    "# Uncomment these lines if the error persists:\n",
    "\n",
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#     try:\n",
    "#         tf.config.set_logical_device_configuration(\n",
    "#             gpus[0],\n",
    "#             [tf.config.LogicalDeviceConfiguration(memory_limit=2048)])  # 2GB limit\n",
    "#         print(\"✓ GPU memory limited to 2048MB\")\n",
    "#     except RuntimeError as e:\n",
    "#         print(f\"Memory limit error: {e}\")\n",
    "\n",
    "# Option 2: Force CPU-only mode (FALLBACK if GPU continues to fail)\n",
    "# Uncomment this line to disable GPU entirely:\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "# print(\"⚠️  GPU DISABLED - Using CPU only (will be slower)\")\n",
    "\n",
    "print(\"ℹ️  GPU troubleshooting options available (see cell comments)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVd__qV5y03E"
   },
   "source": [
    "## 1.1 HuggingFace Setup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_3OmcdRMy03F",
    "outputId": "fda8a679-f317-48bc-db79-80be7a522e52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFace Setup\n",
      "============================================================\n",
      "Please provide your Hugging Face token with WRITE access:\n",
      "Get it from: https://huggingface.co/settings/tokens\n",
      "\n",
      "✓ Logged in to Hugging Face\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model repo exists: Neural-Network-Project/ECG-Disease-Classifier\n",
      "✓ Dataset repo exists: Neural-Network-Project/ECG-database\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace Setup (optional - set ENABLE_HF_UPLOAD=True to enable)\n",
    "# Install huggingface_hub if not already installed\n",
    "try:\n",
    "    from huggingface_hub import HfApi, login, create_repo\n",
    "except ImportError:\n",
    "    print(\"Installing huggingface_hub...\")\n",
    "    import subprocess\n",
    "    subprocess.run(['.venv\\\\Scripts\\\\python.exe', '-m', 'pip', 'install', 'huggingface-hub', '-q'], check=True)\n",
    "    from huggingface_hub import HfApi, login, create_repo\n",
    "\n",
    "# Set this to True to enable HuggingFace uploads\n",
    "ENABLE_HF_UPLOAD = True  # Change to True to enable uploads\n",
    "\n",
    "if ENABLE_HF_UPLOAD:\n",
    "    from getpass import getpass\n",
    "    from google.colab import userdata\n",
    "    import os\n",
    "\n",
    "    print(\"HuggingFace Setup\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Please provide your Hugging Face token with WRITE access:\")\n",
    "    print(\"Get it from: https://huggingface.co/settings/tokens\")\n",
    "    print()\n",
    "\n",
    "    # Try to get token from environment variable first\n",
    "    HF_TOKEN = os.environ.get('HF_TOKEN') or userdata.get('HF_TOKEN')\n",
    "\n",
    "    if not HF_TOKEN:\n",
    "        HF_TOKEN = getpass(\"Enter your HF token (hidden): \")\n",
    "\n",
    "    # Login\n",
    "    try:\n",
    "        login(token=HF_TOKEN, add_to_git_credential=True)\n",
    "        print(\"✓ Logged in to Hugging Face\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Login failed: {e}\")\n",
    "        print(\"HuggingFace uploads will be disabled\")\n",
    "        ENABLE_HF_UPLOAD = False\n",
    "\n",
    "    if ENABLE_HF_UPLOAD:\n",
    "        # Repository names\n",
    "        MODEL_REPO = \"Neural-Network-Project/ECG-Disease-Classifier\"\n",
    "        DATASET_REPO = \"Neural-Network-Project/ECG-database\"\n",
    "\n",
    "        # Initialize API\n",
    "        api = HfApi()\n",
    "\n",
    "        # Ensure repos exist\n",
    "        try:\n",
    "            api.repo_info(repo_id=MODEL_REPO, repo_type=\"model\")\n",
    "            print(f\"✓ Model repo exists: {MODEL_REPO}\")\n",
    "        except:\n",
    "            try:\n",
    "                create_repo(repo_id=MODEL_REPO, repo_type=\"model\", private=False)\n",
    "                print(f\"✓ Created model repo: {MODEL_REPO}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  Could not create model repo: {e}\")\n",
    "\n",
    "        try:\n",
    "            api.repo_info(repo_id=DATASET_REPO, repo_type=\"dataset\")\n",
    "            print(f\"✓ Dataset repo exists: {DATASET_REPO}\")\n",
    "        except:\n",
    "            try:\n",
    "                create_repo(repo_id=DATASET_REPO, repo_type=\"dataset\", private=False)\n",
    "                print(f\"✓ Created dataset repo: {DATASET_REPO}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  Could not create dataset repo: {e}\")\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"HuggingFace uploads disabled (set ENABLE_HF_UPLOAD=True to enable)\")\n",
    "    MODEL_REPO = None\n",
    "    DATASET_REPO = None\n",
    "    api = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ugKkVA2y03F"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA DOWNLOAD FROM HUGGINGFACE\n",
    "# ============================================================================\n",
    "# Downloads dataset from HuggingFace (works in Colab and local)\n",
    "\n",
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Configuration\n",
    "DATASET_REPO = \"Neural-Network-Project/ECG-database\"  # Your HuggingFace dataset repo\n",
    "DOWNLOAD_DATA = True  # Set to False if data already exists locally\n",
    "\n",
    "if DOWNLOAD_DATA:\n",
    "    print(\"Downloading dataset from HuggingFace...\")\n",
    "    print(f\"  Repository: {DATASET_REPO}\")\n",
    "    print(\"  This may take a few minutes...\")\n",
    "    print()\n",
    "\n",
    "    try:\n",
    "        local_dir = snapshot_download(\n",
    "            repo_id=DATASET_REPO,\n",
    "            local_dir=\"../dataset\",\n",
    "            repo_type=\"dataset\",\n",
    "            allow_patterns=[\"data/**\", \"artifacts/**\"],\n",
    "            ignore_patterns=[\"artifacts/training_results/**\"]\n",
    "        )\n",
    "\n",
    "        print(f\"[OK] Dataset downloaded to: {local_dir}\")\n",
    "        print(f\"  - Raw ECG data: {local_dir}/data/Child_ecg/\")\n",
    "        print(f\"  - Metadata CSV: {local_dir}/data/AttributesDictionary.csv\")\n",
    "        print(f\"  - Preprocessed H5: {local_dir}/artifacts/\")\n",
    "        print()\n",
    "\n",
    "        # Set DATA_DIR to downloaded location\n",
    "        DATA_DIR = local_dir + \"/\"\n",
    "        print(f\"[OK] DATA_DIR set to: {DATA_DIR}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to download dataset: {e}\")\n",
    "        print(\"Falling back to local path...\")\n",
    "        DATA_DIR = \"../.data/\"  # Fallback to local\n",
    "\n",
    "else:\n",
    "    print(\"[INFO] Using local data (DOWNLOAD_DATA=False)\")\n",
    "    DATA_DIR = \"../.data/\"  # Local path\n",
    "\n",
    "print(f\"\\nFinal DATA_DIR: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltmZIZdDy03F"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "NOTEBOOK_VERSION = \"v03_balanced\"  # Updated to distinguish from original\n",
    "# DATA_DIR defined by HuggingFace download cell above\n",
    "# If not using HF download, uncomment: # DATA_DIR = \"../.data/\"\n",
    "ECG_DIR = os.path.join(DATA_DIR, \"data/Child_ecg/\")\n",
    "CSV_PATH = os.path.join(DATA_DIR, \"data/AttributesDictionary.csv\")\n",
    "OUTPUT_DIR = os.path.join(DATA_DIR, \"multilabel_19v3/\")  # New output directory\n",
    "HDF5_FILE = os.path.join(DATA_DIR, \"multilabel_19v3/ecg_full_length_19classes.h5\")\n",
    "MODEL_NAME = \"1D_CNN_19Classes_multilabel_19v3\"  # Updated model name\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Training parameters - OPTIMIZED FOR COLAB T4 GPU (15 GB VRAM)\n",
    "MAX_LENGTH = 15000  # Maximum sequence length (30 seconds at 500Hz)\n",
    "BATCH_SIZE = 128  # Optimized for T4 GPU - uses ~7-8 GB VRAM\n",
    "EPOCHS = 100  # EarlyStopping will stop if no improvement\n",
    "LEARNING_RATE = 2e-4\n",
    "TARGET_FS = 500  # Target sampling frequency\n",
    "TARGET_CHANNELS = 12\n",
    "CHECKPOINT_EVERY = 1  # Save checkpoint every 5 epochs\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Data directory: {DATA_DIR}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Model name: {MODEL_NAME}\")\n",
    "print(f\"  Max sequence length: {MAX_LENGTH} samples ({MAX_LENGTH/TARGET_FS:.1f}s)\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "# print(f\"  Gradient accumulation steps: {GRADIENT_ACCUMULATION_STEPS} (effective batch_size={BATCH_SIZE*GRADIENT_ACCUMULATION_STEPS})\")  # Not yet implemented\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Checkpoint every: {CHECKPOINT_EVERY} epochs (HuggingFace)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVtXYC32y03G"
   },
   "source": [
    "## 1.5 Extract Split Archive (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "me3MJgtry03G"
   },
   "outputs": [],
   "source": [
    "# Extract split archives if needed (local environment)\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Check if Child_ecg directory exists and has files\n",
    "def count_files_recursive(directory):\n",
    "    \"\"\"Count all files recursively in a directory\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        return 0\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        count += len(files)\n",
    "    return count\n",
    "\n",
    "ecg_file_count = count_files_recursive(ECG_DIR)\n",
    "\n",
    "print(f\"Checking ECG data extraction...\")\n",
    "print(f\"  ECG directory: {ECG_DIR}\")\n",
    "print(f\"  Files found: {ecg_file_count}\")\n",
    "\n",
    "# If directory doesn't exist or has very few files, extract the archives\n",
    "if ecg_file_count < 1000:  # Expect tens of thousands of files\n",
    "    print(f\"\\n⚠️  Insufficient ECG files detected. Checking for archives to extract...\")\n",
    "\n",
    "    # Look for split archive files\n",
    "    archive_dir = os.path.join(DATA_DIR, \"data\")\n",
    "    zip_file = os.path.join(archive_dir, \"Child_ecg.zip\")\n",
    "    z01_file = os.path.join(archive_dir, \"Child_ecg.z01\")\n",
    "\n",
    "    if os.path.exists(zip_file):\n",
    "        print(f\"\\n✓ Found archive files:\")\n",
    "        print(f\"  - {zip_file}\")\n",
    "        if os.path.exists(z01_file):\n",
    "            print(f\"  - {z01_file}\")\n",
    "\n",
    "        print(f\"\\nExtracting archives...\")\n",
    "        print(f\"This may take several minutes...\")\n",
    "\n",
    "        try:\n",
    "            # For split archives (.z01 + .zip), we need to use 7zip or similar\n",
    "            if os.path.exists(z01_file):\n",
    "                # Try using 7zip if available\n",
    "                try:\n",
    "                    import subprocess\n",
    "                    # Try 7z command on Windows (usually installed with 7-Zip)\n",
    "                    result = subprocess.run(\n",
    "                        ['7z', 'x', zip_file, f'-o{archive_dir}', '-y'],\n",
    "                        capture_output=True,\n",
    "                        text=True\n",
    "                    )\n",
    "                    if result.returncode == 0:\n",
    "                        print(f\"✓ Successfully extracted using 7zip\")\n",
    "                    else:\n",
    "                        raise Exception(\"7zip extraction failed\")\n",
    "                except:\n",
    "                    # Fallback: Try PowerShell Expand-Archive (may not work with split archives)\n",
    "                    print(\"7zip not available, trying PowerShell...\")\n",
    "                    result = subprocess.run(\n",
    "                        ['powershell', '-Command',\n",
    "                         f'Expand-Archive -Path \"{zip_file}\" -DestinationPath \"{archive_dir}\" -Force'],\n",
    "                        capture_output=True,\n",
    "                        text=True\n",
    "                    )\n",
    "                    if result.returncode == 0:\n",
    "                        print(f\"✓ Extraction attempted with PowerShell\")\n",
    "                    else:\n",
    "                        print(f\"PowerShell extraction failed, trying Python zipfile...\")\n",
    "                        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "                            zip_ref.extractall(archive_dir)\n",
    "                        print(f\"✓ Extraction attempted with zipfile\")\n",
    "            else:\n",
    "                # Single zip file, use Python's zipfile\n",
    "                with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(archive_dir)\n",
    "                print(f\"✓ Successfully extracted zip file\")\n",
    "\n",
    "            # Verify extraction\n",
    "            new_file_count = count_files_recursive(ECG_DIR)\n",
    "            print(f\"\\n✓ Extraction complete!\")\n",
    "            print(f\"  Files extracted: {new_file_count}\")\n",
    "\n",
    "            if new_file_count < 1000:\n",
    "                print(f\"\\n⚠️  Warning: Expected more files. Archive may not have extracted completely.\")\n",
    "                print(f\"  Please manually extract {zip_file} using 7-Zip if needed.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error during extraction: {e}\")\n",
    "            print(f\"\\nPlease manually extract the following file:\")\n",
    "            print(f\"  {zip_file}\")\n",
    "            print(f\"To directory: {archive_dir}\")\n",
    "            print(f\"\\nYou can use 7-Zip (https://www.7-zip.org/) to extract split archives.\")\n",
    "            raise\n",
    "    else:\n",
    "        print(f\"\\n❌ Archive files not found!\")\n",
    "        print(f\"  Expected: {zip_file}\")\n",
    "        print(f\"\\nPlease ensure the archive files are present in {archive_dir}\")\n",
    "        raise FileNotFoundError(f\"Archive not found: {zip_file}\")\n",
    "else:\n",
    "    print(f\"✓ ECG data already extracted ({ecg_file_count} files found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTDJeL-Fy03H"
   },
   "source": [
    "## 2. Disease Mapping & Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xs7xCLi3y03H"
   },
   "outputs": [],
   "source": [
    "# ICD codes to Disease Names mapping\n",
    "ICD_TO_DISEASE_NAME = {\n",
    "    'I40.0': 'Fulminant/Viral Myocarditis',\n",
    "    'I40.9': 'Acute Myocarditis',\n",
    "    'I51.4': 'Myocarditis Unspecified',\n",
    "    'I42.0': 'Dilated Cardiomyopathy',\n",
    "    'I42.2': 'Hypertrophic Cardiomyopathy',\n",
    "    'I42.9': 'Cardiomyopathy Unspecified',\n",
    "    'Q24.8': 'Noncompaction Ventricular Myocardium',\n",
    "    'M30.3': 'Kawasaki Disease',\n",
    "    'Q21.0': 'Ventricular Septal Defect',\n",
    "    'Q21.1': 'Atrial Septal Defect',\n",
    "    'Q21.2': 'Atrioventricular Septal Defect',\n",
    "    'Q21.3': 'Tetralogy of Fallot',\n",
    "    'Q22.1': 'Pulmonary Valve Stenosis',\n",
    "    'Q25.0': 'Patent Ductus Arteriosus',\n",
    "    'Q25.6': 'Pulmonary Artery Stenosis',\n",
    "    'I37.0': 'Pulmonary Valve Regurgitation',\n",
    "    'I34.0': 'Mitral Valve Insufficiency',\n",
    "    'Q24.9': 'Congenital Heart Malformation',\n",
    "    'Healthy': 'Healthy'  # Changed from \"No Disease (Healthy)\" to just \"Healthy\"\n",
    "}\n",
    "\n",
    "# Disease names list (for model outputs - NO ICD codes in outputs)\n",
    "DISEASE_NAMES = list(ICD_TO_DISEASE_NAME.values())\n",
    "ICD_CODES = list(ICD_TO_DISEASE_NAME.keys())\n",
    "NUM_CLASSES = len(DISEASE_NAMES)\n",
    "\n",
    "# Disease grouping for interpretability\n",
    "DISEASE_GROUPS = {\n",
    "    'Myocarditis': ['I40.0', 'I40.9', 'I51.4'],\n",
    "    'Cardiomyopathy': ['I42.0', 'I42.2', 'I42.9', 'Q24.8'],\n",
    "    'Kawasaki': ['M30.3'],\n",
    "    'CHD': ['Q21.0', 'Q21.1', 'Q21.2', 'Q21.3', 'Q22.1', 'Q25.0', 'Q25.6', 'I37.0', 'I34.0', 'Q24.9']\n",
    "}\n",
    "\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"\\nDisease Names (outputs will show these names ONLY):\")\n",
    "for i, name in enumerate(DISEASE_NAMES, 1):\n",
    "    print(f\"  {i:2d}. {name}\")\n",
    "\n",
    "print(f\"\\nDisease Groups:\")\n",
    "for group, codes in DISEASE_GROUPS.items():\n",
    "    names = [ICD_TO_DISEASE_NAME[code] for code in codes]\n",
    "    print(f\"  {group}: {len(codes)} diseases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idrFnJFey03H"
   },
   "source": [
    "## 3. Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPpJxUMgy03H"
   },
   "outputs": [],
   "source": [
    "def apply_bandpass(x, fs, lowcut=0.5, highcut=40.0):\n",
    "    \"\"\"Apply bandpass filter to entire ECG signal.\"\"\"\n",
    "    if x.ndim == 1:\n",
    "        x = x[:, None]\n",
    "    nyq = 0.5 * fs\n",
    "    b, a = butter(4, [lowcut/nyq, highcut/nyq], btype=\"band\")\n",
    "    return np.column_stack([filtfilt(b, a, x[:, i]) for i in range(x.shape[1])])\n",
    "\n",
    "\n",
    "def process_full_recording(sig, meta, target_fs=500.0, target_channels=12, max_length=None):\n",
    "    \"\"\"\n",
    "    Process full ECG recording without windowing.\n",
    "    Handles variable lengths (5-120 seconds), with optional truncation.\n",
    "\n",
    "    Args:\n",
    "        sig: ECG signal\n",
    "        meta: Metadata with sampling frequency\n",
    "        target_fs: Target sampling frequency (Hz)\n",
    "        target_channels: Target number of channels\n",
    "        max_length: Maximum length in samples (default: None = no truncation)\n",
    "\n",
    "    Returns:\n",
    "        sig_processed: (n_samples, 12) - variable length\n",
    "        length: actual sample count\n",
    "    \"\"\"\n",
    "    # Get sampling frequency\n",
    "    fs = meta.get(\"fs\", None)\n",
    "    if fs is None:\n",
    "        fs = getattr(meta, \"fs\", None)\n",
    "    if fs is None:\n",
    "        raise ValueError(\"Missing sampling frequency\")\n",
    "\n",
    "    if sig.ndim == 1:\n",
    "        sig = sig[:, None]\n",
    "\n",
    "    # 1. Bandpass filter entire signal (0.5-40 Hz)\n",
    "    sig_bp = apply_bandpass(sig, fs=fs)\n",
    "\n",
    "    # 2. Resample to target_fs if needed\n",
    "    if fs != target_fs:\n",
    "        n_samples = sig_bp.shape[0]\n",
    "        n_new = int(round(n_samples / fs * target_fs))\n",
    "        sig_res = np.column_stack([resample(sig_bp[:, i], n_new)\n",
    "                                   for i in range(sig_bp.shape[1])])\n",
    "    else:\n",
    "        sig_res = sig_bp\n",
    "\n",
    "    # 3. Truncate if max_length specified (MEMORY OPTIMIZATION)\n",
    "    if max_length is not None and sig_res.shape[0] > max_length:\n",
    "        sig_res = sig_res[:max_length, :]\n",
    "\n",
    "    # 4. Standardize to 12 leads (pad with zeros if fewer)\n",
    "    if sig_res.shape[1] < target_channels:\n",
    "        padded = np.zeros((sig_res.shape[0], target_channels), dtype=np.float32)\n",
    "        padded[:, :sig_res.shape[1]] = sig_res\n",
    "        sig_res = padded\n",
    "    elif sig_res.shape[1] > target_channels:\n",
    "        sig_res = sig_res[:, :target_channels]\n",
    "\n",
    "    # 5. Z-score normalization per lead\n",
    "    sig_norm = sig_res.copy()\n",
    "    for ch in range(target_channels):\n",
    "        x = sig_norm[:, ch]\n",
    "        m, s = np.nanmean(x), np.nanstd(x)\n",
    "        sig_norm[:, ch] = (x - m) / (s if s > 1e-6 else 1.0)\n",
    "\n",
    "    actual_length = sig_norm.shape[0]\n",
    "\n",
    "    return sig_norm.astype(np.float16), actual_length\n",
    "\n",
    "\n",
    "def parse_icd_codes(s):\n",
    "    \"\"\"Parse semicolon-separated ICD codes from CSV.\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return []\n",
    "    return [p.strip().replace(\"'\", \"\").replace(\")\", \"\").split(\")\")[-1]\n",
    "            for p in str(s).split(\";\") if p.strip()]\n",
    "\n",
    "\n",
    "print(\"✓ Preprocessing functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7J-Y3BXy03H"
   },
   "source": [
    "## 4. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lg6MIJfJy03H"
   },
   "outputs": [],
   "source": [
    "# Load CSV metadata\n",
    "print(\"Loading metadata...\")\n",
    "df_attr = pd.read_csv(CSV_PATH)\n",
    "print(f\"Loaded {len(df_attr)} ECG records\")\n",
    "\n",
    "# Parse ICD codes\n",
    "df_attr[\"ICD_list\"] = df_attr[\"ICD-10 code\"].apply(parse_icd_codes)\n",
    "print(f\"Sample ICD codes: {df_attr['ICD_list'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEqHmU7zy03I"
   },
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# CHECK FOR PREPROCESSED DATA (Local or HuggingFace)\n",
    "# =============================================================\n",
    "\n",
    "# First, try to download from HuggingFace if not available locally\n",
    "if not os.path.exists(HDF5_FILE) and DATASET_REPO:\n",
    "    print(\"Checking HuggingFace for preprocessed data...\")\n",
    "    try:\n",
    "        from huggingface_hub import hf_hub_download\n",
    "\n",
    "        # Try to download the HDF5 file\n",
    "        downloaded_h5 = hf_hub_download(\n",
    "            repo_id=DATASET_REPO,\n",
    "            filename=\"multilabel_19v3/ecg_full_length_19classes.h5\",\n",
    "            repo_type=\"dataset\",\n",
    "            local_dir=DATA_DIR,\n",
    "        )\n",
    "        print(\"✓ Downloaded preprocessed HDF5 from HuggingFace\")\n",
    "\n",
    "        # Optional: ensure HDF5_FILE points to the downloaded file\n",
    "        # (uncomment if needed, depending on how HDF5_FILE is defined)\n",
    "        # HDF5_FILE = downloaded_h5\n",
    "\n",
    "        # Try to download metadata\n",
    "        try:\n",
    "            downloaded_meta = hf_hub_download(\n",
    "                repo_id=DATASET_REPO,\n",
    "                filename=\"multilabel_19v3/metadata.json\",\n",
    "                repo_type=\"dataset\",\n",
    "                local_dir=DATA_DIR,\n",
    "            )\n",
    "            print(\"✓ Downloaded metadata from HuggingFace\")\n",
    "        except Exception:\n",
    "            print(\"  (metadata not found, continuing without it)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  HuggingFace download failed: {e}\")\n",
    "        print(\"  Will generate preprocessed data locally...\")\n",
    "\n",
    "# Now check if HDF5 exists (either already local or just downloaded)\n",
    "if os.path.exists(HDF5_FILE):\n",
    "    print(f\"✓ Found existing HDF5 file: {HDF5_FILE}\")\n",
    "    print(\"Loading preprocessed data...\")\n",
    "\n",
    "    with h5py.File(HDF5_FILE, 'r') as h5f:\n",
    "        print(f\"\\nHDF5 Contents:\")\n",
    "        print(f\"  Bins: {list(h5f.keys())}\")\n",
    "\n",
    "        total_samples = 0\n",
    "        for bin_name in h5f.keys():\n",
    "            if bin_name.startswith('bin_'):\n",
    "                grp = h5f[bin_name]\n",
    "                bin_samples = grp['X'].shape[0]\n",
    "                total_samples += bin_samples\n",
    "                print(f\"  {bin_name}: {bin_samples} samples\")\n",
    "\n",
    "        print(f\"\\n  Total samples: {total_samples}\")\n",
    "        print(f\"  Number of classes: {h5f.attrs['num_classes']}\")\n",
    "        print(f\"  Sampling rate: {h5f.attrs['sampling_rate']} Hz\")\n",
    "\n",
    "else:\n",
    "    print(f\"HDF5 file not found. Creating preprocessed dataset...\")\n",
    "    print(f\"This may take 30-60 minutes...\\n\")\n",
    "\n",
    "    # Process all recordings\n",
    "    recordings = []\n",
    "    labels = []\n",
    "    lengths = []\n",
    "    filenames = []\n",
    "\n",
    "    print(\"Processing ECG recordings...\")\n",
    "    for idx, row in df_attr.iterrows():\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print(f\"  Processed {idx + 1}/{len(df_attr)} records...\")\n",
    "\n",
    "        fname = row[\"Filename\"]\n",
    "        path = os.path.join(ECG_DIR, fname)\n",
    "\n",
    "        try:\n",
    "            sig, meta = wfdb.rdsamp(path)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        meta_dict = meta if isinstance(meta, dict) else meta.__dict__\n",
    "        sig = np.asarray(sig)\n",
    "\n",
    "        # Process full recording\n",
    "        try:\n",
    "            sig_proc, length = process_full_recording(\n",
    "                sig, meta_dict, max_length=MAX_LENGTH\n",
    "            )\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        # Create multi-label vector\n",
    "        label_vec = np.zeros(NUM_CLASSES, dtype=np.int32)\n",
    "        icd_list = row[\"ICD_list\"]\n",
    "\n",
    "        if not icd_list or all(pd.isna(icd_list)):\n",
    "            # Healthy case\n",
    "            label_vec[ICD_CODES.index('Healthy')] = 1\n",
    "        else:\n",
    "            # Disease case\n",
    "            found_disease = False\n",
    "            for icd in icd_list:\n",
    "                if icd in ICD_CODES:\n",
    "                    label_vec[ICD_CODES.index(icd)] = 1\n",
    "                    found_disease = True\n",
    "\n",
    "            # If no matching disease found, mark as healthy\n",
    "            if not found_disease:\n",
    "                label_vec[ICD_CODES.index('Healthy')] = 1\n",
    "\n",
    "        recordings.append(sig_proc)\n",
    "        labels.append(label_vec)\n",
    "        lengths.append(length)\n",
    "        filenames.append(fname)\n",
    "\n",
    "    print(f\"\\n✓ Processed {len(recordings)} valid recordings\")\n",
    "\n",
    "    # Save to HDF5 with length binning\n",
    "    print(\"\\nSaving to HDF5 with length binning...\")\n",
    "\n",
    "    labels_array = np.array(labels)\n",
    "    lengths_array = np.array(lengths)\n",
    "\n",
    "    # Define bins: <10s, 10-30s, 30-60s, 60-120s (in samples @ 500Hz)\n",
    "    length_bins = [(0, 5000), (5000, 15000), (15000, 30000), (30000, 60000)]\n",
    "\n",
    "    with h5py.File(HDF5_FILE, 'w') as h5f:\n",
    "        for bin_idx, (min_len, max_len) in enumerate(length_bins):\n",
    "            mask = (lengths_array >= min_len) & (lengths_array < max_len)\n",
    "            if not np.any(mask):\n",
    "                continue\n",
    "\n",
    "            bin_recordings = [recordings[i] for i in np.where(mask)[0]]\n",
    "            bin_labels = labels_array[mask]\n",
    "            bin_lengths = lengths_array[mask]\n",
    "            bin_filenames = [filenames[i] for i in np.where(mask)[0]]\n",
    "\n",
    "            # Pad to max length in this bin\n",
    "            max_bin_len = bin_lengths.max()\n",
    "            padded_sigs = np.zeros(\n",
    "                (len(bin_recordings), max_bin_len, 12), dtype=np.float16\n",
    "            )\n",
    "            for i, rec in enumerate(bin_recordings):\n",
    "                padded_sigs[i, :len(rec), :] = rec\n",
    "\n",
    "            # Create group\n",
    "            grp = h5f.create_group(f'bin_{bin_idx}')\n",
    "            grp.create_dataset('X', data=padded_sigs,\n",
    "                               compression='gzip', compression_opts=4)\n",
    "            grp.create_dataset('y', data=bin_labels,\n",
    "                               compression='gzip', compression_opts=4)\n",
    "            grp.create_dataset('lengths', data=bin_lengths,\n",
    "                               compression='gzip', compression_opts=4)\n",
    "            grp.attrs['min_len'] = min_len\n",
    "            grp.attrs['max_len'] = max_len\n",
    "            grp.attrs['count'] = len(bin_recordings)\n",
    "\n",
    "            print(\n",
    "                f\"  bin_{bin_idx} ({min_len/500:.1f}-{max_len/500:.1f}s): \"\n",
    "                f\"{len(bin_recordings)} samples\"\n",
    "            )\n",
    "\n",
    "        # Global metadata\n",
    "        h5f.attrs['num_classes'] = NUM_CLASSES\n",
    "        h5f.attrs['disease_names'] = DISEASE_NAMES  # Store disease names\n",
    "        h5f.attrs['icd_codes'] = ICD_CODES\n",
    "        h5f.attrs['sampling_rate'] = TARGET_FS\n",
    "        h5f.attrs['target_channels'] = TARGET_CHANNELS\n",
    "        h5f.attrs['total_samples'] = len(recordings)\n",
    "\n",
    "    print(f\"\\n✓ Saved to {HDF5_FILE}\")\n",
    "\n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'disease_names': DISEASE_NAMES,\n",
    "        'icd_codes': ICD_CODES,\n",
    "        'icd_to_disease_name': ICD_TO_DISEASE_NAME,\n",
    "        'disease_groups': DISEASE_GROUPS,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'sampling_rate': TARGET_FS,\n",
    "        'target_channels': TARGET_CHANNELS,\n",
    "        'created': datetime.now().isoformat(),\n",
    "    }\n",
    "\n",
    "    metadata_path = os.path.join(OUTPUT_DIR, 'metadata.json')\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "\n",
    "    print(f\"✓ Saved metadata to {metadata_path}\")\n",
    "\n",
    "    # ============================================================\n",
    "    # UPLOAD TO HUGGINGFACE (so it can be reused next time)\n",
    "    # ============================================================\n",
    "    if ENABLE_HF_UPLOAD and DATASET_REPO:\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"Uploading preprocessed data to HuggingFace...\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        try:\n",
    "            from huggingface_hub import HfApi\n",
    "            api = HfApi()\n",
    "\n",
    "            # Upload HDF5 file\n",
    "            print(\n",
    "                f\"\\n1. Uploading HDF5 file \"\n",
    "                f\"({os.path.getsize(HDF5_FILE) / 1024**3:.2f} GB)...\"\n",
    "            )\n",
    "            api.upload_file(\n",
    "                path_or_fileobj=HDF5_FILE,\n",
    "                path_in_repo=\"multilabel_19v3/ecg_full_length_19classes.h5\",\n",
    "                repo_id=DATASET_REPO,\n",
    "                repo_type=\"dataset\",\n",
    "                commit_message=(\n",
    "                    \"Add preprocessed HDF5 dataset \"\n",
    "                    \"(19 classes, full length)\"\n",
    "                ),\n",
    "            )\n",
    "            print(\"   ✓ HDF5 uploaded\")\n",
    "\n",
    "            # Upload metadata\n",
    "            print(\"\\n2. Uploading metadata...\")\n",
    "            api.upload_file(\n",
    "                path_or_fileobj=metadata_path,\n",
    "                path_in_repo=\"multilabel_19v3/metadata.json\",\n",
    "                repo_id=DATASET_REPO,\n",
    "                repo_type=\"dataset\",\n",
    "                commit_message=\"Add dataset metadata\",\n",
    "            )\n",
    "            print(\"   ✓ Metadata uploaded\")\n",
    "\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(\"✓ Successfully uploaded to HuggingFace!\")\n",
    "            print(f\"  Repository: {DATASET_REPO}\")\n",
    "            print(\"  Path: multilabel_19v3/\")\n",
    "            print(\"  Next time, this data will be available for download\")\n",
    "            print(\"=\" * 70)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n⚠️  Warning: Could not upload to HuggingFace: {e}\")\n",
    "            print(\"   Data is saved locally and can be used for training\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  HuggingFace upload disabled (ENABLE_HF_UPLOAD=False)\")\n",
    "        print(\"   Data saved locally only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNYRFVCfy03I"
   },
   "source": [
    "## 5. Enhanced Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5gkQ_95y03J"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SIMPLIFIED MODEL: Basic Conv Blocks (No SE/Attention/Residual)\n",
    "# ============================================================================\n",
    "# Research shows simpler models generalize better on imbalanced medical data\n",
    "\n",
    "def conv_block(x, filters, kernel_size=3, pool_size=2, dropout=0.3):\n",
    "    \"\"\"\n",
    "    Simple convolutional block: Conv -> BN -> Pool -> Dropout\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor\n",
    "        filters: Number of filters\n",
    "        kernel_size: Convolution kernel size\n",
    "        pool_size: Pooling size (None to skip pooling)\n",
    "        dropout: Dropout rate\n",
    "\n",
    "    Returns:\n",
    "        Output tensor\n",
    "    \"\"\"\n",
    "    x = layers.Conv1D(filters, kernel_size, padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    if pool_size is not None:\n",
    "        x = layers.MaxPooling1D(pool_size)(x)\n",
    "\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "print(\"[OK] Simplified conv_block defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "basoondzy03J"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SIMPLIFIED MODEL ARCHITECTURE\n",
    "# ============================================================================\n",
    "# Removed: SE blocks, Temporal Attention, Residual connections, Lambda layers\n",
    "# Benefits: 3x faster training, less overfitting, better generalization\n",
    "\n",
    "def build_simplified_model(input_shape=(None, 12), num_classes=19):\n",
    "    \"\"\"\n",
    "    Simplified CNN for ECG multi-label classification.\n",
    "\n",
    "    Architecture: 64 -> 128 -> 256 -> 256 -> GAP -> Dense(128) -> Output\n",
    "\n",
    "    Args:\n",
    "        input_shape: (sequence_length, channels) - None for variable length\n",
    "        num_classes: Number of output classes\n",
    "\n",
    "    Returns:\n",
    "        Keras Model\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape, name='ecg_input')\n",
    "\n",
    "    # Initial conv\n",
    "    x = layers.Conv1D(64, 7, padding='same', activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    # Block 1: 128 filters\n",
    "    x = conv_block(x, 128, kernel_size=5, pool_size=2, dropout=0.3)\n",
    "\n",
    "    # Block 2: 256 filters\n",
    "    x = conv_block(x, 256, kernel_size=3, pool_size=2, dropout=0.4)\n",
    "\n",
    "    # Block 3: 256 filters (no pooling - preserve temporal resolution)\n",
    "    x = conv_block(x, 256, kernel_size=3, pool_size=None, dropout=0.4)\n",
    "\n",
    "    # Global pooling - handles variable length sequences\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Single dense layer\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    # Multi-label output (sigmoid for independent probabilities)\n",
    "    outputs = layers.Dense(num_classes, activation='sigmoid', name='disease_output')(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs, name='simplified_ecg_cnn')\n",
    "\n",
    "    return model\n",
    "\n",
    "print(\"[OK] Simplified model architecture defined\")\n",
    "\n",
    "# Build model\n",
    "print(\"\\nBuilding simplified model...\")\n",
    "model = build_simplified_model(input_shape=(None, TARGET_CHANNELS), num_classes=NUM_CLASSES)\n",
    "\n",
    "print(f\"\\nModel: {model.name}\")\n",
    "print(f\"  Input shape: (None, {TARGET_CHANNELS})\")\n",
    "print(f\"  Output shape: ({NUM_CLASSES},)\")\n",
    "print(f\"  Total parameters: {model.count_params():,}\")\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLNvRho0y03J"
   },
   "source": [
    "## 6. Focal Loss & Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4qlcs4cmy03J"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FOCAL LOSS IMPLEMENTATION\n",
    "# ============================================================================\n",
    "# Focal Loss is better suited for class imbalance than binary_crossentropy.\n",
    "# It focuses training on hard-to-classify examples (minority classes).\n",
    "# Combined with moderate oversampling, this provides robust handling of imbalance.\n",
    "\n",
    "class FocalLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"\n",
    "    Focal loss for handling class imbalance.\n",
    "    FL(p_t) = -alpha_t * (1 - p_t)^gamma * log(p_t)\n",
    "\n",
    "    Focuses on hard-to-classify examples by down-weighting easy ones.\n",
    "\n",
    "    Args:\n",
    "        alpha: Weighting factor for positive class (default: 0.25)\n",
    "        gamma: Focusing parameter - higher values focus more on hard examples (default: 2.0)\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Clip predictions to prevent log(0)\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Binary cross entropy (base loss)\n",
    "        bce = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n",
    "\n",
    "        # Focal term: (1 - p_t)^gamma - down-weights easy examples\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        focal_weight = tf.pow(1 - p_t, self.gamma)\n",
    "\n",
    "        # Apply alpha weighting for class balance\n",
    "        alpha_weight = y_true * self.alpha + (1 - y_true) * (1 - self.alpha)\n",
    "\n",
    "        # Combine: alpha * focal_weight * BCE\n",
    "        focal_loss = alpha_weight * focal_weight * bce\n",
    "\n",
    "        return tf.reduce_mean(tf.reduce_sum(focal_loss, axis=-1))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"alpha\": self.alpha, \"gamma\": self.gamma})\n",
    "        return config\n",
    "\n",
    "print(\"✓ Focal Loss implemented (alpha=0.25, gamma=2.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BE38n61Hy03J"
   },
   "source": [
    "## 6.1 HuggingFace Checkpoint Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FF9lXsxdy03J"
   },
   "outputs": [],
   "source": [
    "# HuggingFace Checkpoint Callback (optional - requires ENABLE_HF_UPLOAD=True)\n",
    "if ENABLE_HF_UPLOAD and MODEL_REPO:\n",
    "    class HuggingFaceCheckpoint(tf.keras.callbacks.Callback):\n",
    "        \"\"\"\n",
    "        Save checkpoints to Hugging Face Hub every N epochs.\n",
    "        Handles training interruptions by allowing resume.\n",
    "        \"\"\"\n",
    "        def __init__(self, repo_id, every_n_epochs=10):\n",
    "            super().__init__()\n",
    "            self.repo_id = repo_id\n",
    "            self.every_n_epochs = every_n_epochs\n",
    "            self.api = HfApi()\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            if (epoch + 1) % self.every_n_epochs == 0:\n",
    "                epoch_num = epoch + 1\n",
    "\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"Epoch {epoch_num}: Saving checkpoint to Hugging Face...\")\n",
    "                print(f\"{'='*60}\")\n",
    "\n",
    "                # Save model locally\n",
    "                checkpoint_path = os.path.join(OUTPUT_DIR, f\"checkpoint_epoch_{epoch_num}.keras\")\n",
    "                self.model.save(checkpoint_path, save_format='keras')\n",
    "\n",
    "                # Upload to HuggingFace\n",
    "                try:\n",
    "                    self.api.upload_file(\n",
    "                        path_or_fileobj=checkpoint_path,\n",
    "                        path_in_repo=f\"checkpointsv4/checkpoint_epoch_{epoch_num}.keras\",\n",
    "                        repo_id=self.repo_id,\n",
    "                        repo_type=\"model\",\n",
    "                        commit_message=f\"Checkpoint at epoch {epoch_num}\"\n",
    "                    )\n",
    "                    print(f\"✓ Checkpoint uploaded: epoch {epoch_num}\")\n",
    "\n",
    "                    # Save metrics\n",
    "                    import json\n",
    "                    metrics = {\n",
    "                        'epoch': epoch_num,\n",
    "                        'loss': float(logs.get('loss', 0)),\n",
    "                        'val_loss': float(logs.get('val_loss', 0)),\n",
    "                        'accuracy': float(logs.get('accuracy', 0)),\n",
    "                        'val_accuracy': float(logs.get('val_accuracy', 0)),\n",
    "                        'auc': float(logs.get('auc', 0)),\n",
    "                        'val_auc': float(logs.get('val_auc', 0))\n",
    "                    }\n",
    "\n",
    "                    metrics_path = os.path.join(OUTPUT_DIR, f\"metrics_epoch_{epoch_num}.json\")\n",
    "                    with open(metrics_path, 'w') as f:\n",
    "                        json.dump(metrics, f, indent=2)\n",
    "\n",
    "                    self.api.upload_file(\n",
    "                        path_or_fileobj=metrics_path,\n",
    "                        path_in_repo=f\"checkpointsv4/metrics_epoch_{epoch_num}.json\",\n",
    "                        repo_id=self.repo_id,\n",
    "                        repo_type=\"model\",\n",
    "                        commit_message=f\"Metrics for epoch {epoch_num}\"\n",
    "                    )\n",
    "\n",
    "                    # Clean up local checkpoint (keep only metrics)\n",
    "                    os.remove(checkpoint_path)\n",
    "\n",
    "                    print(f\"✓ Metrics uploaded\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error uploading to HF: {e}\")\n",
    "                    # Keep local checkpoint if upload failed\n",
    "                    if os.path.exists(checkpoint_path):\n",
    "                        print(f\"✓ Local checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "                print(f\"{'='*60}\\n\")\n",
    "\n",
    "    print(\"✓ HuggingFaceCheckpoint callback defined\")\n",
    "else:\n",
    "    print(\"Skipping HF checkpoint (ENABLE_HF_UPLOAD=False or not configured)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kj8SoiGyy03K"
   },
   "source": [
    "## 6.2 Resume from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSZN5nJey03K"
   },
   "outputs": [],
   "source": [
    "# Try to resume from last checkpoint (requires ENABLE_HF_UPLOAD=True)\n",
    "RESUME_TRAINING = True\n",
    "INITIAL_EPOCH = 0\n",
    "RESUME_CHECKPOINT_PATH = None\n",
    "\n",
    "if ENABLE_HF_UPLOAD and MODEL_REPO:\n",
    "    def get_latest_checkpoint(repo_id):\n",
    "        try:\n",
    "            files = api.list_repo_files(repo_id=repo_id, repo_type=\"model\")\n",
    "            checkpoints = [f for f in files if f.startswith(\"checkpointsv4/checkpoint_epoch_\") and f.endswith(\".keras\")]\n",
    "\n",
    "            if not checkpoints:\n",
    "                return None, 0\n",
    "\n",
    "            epochs = [int(f.split(\"_\")[-1].replace(\".keras\", \"\")) for f in checkpoints]\n",
    "            latest_epoch = max(epochs)\n",
    "            latest_file = f\"checkpointsv4/checkpoint_epoch_{latest_epoch}.keras\"\n",
    "\n",
    "            return latest_file, latest_epoch\n",
    "        except Exception as e:\n",
    "            print(f\"Could not check for checkpointsv4: {e}\")\n",
    "            return None, 0\n",
    "\n",
    "    # Check for existing checkpoint\n",
    "    latest_checkpoint, start_epoch = get_latest_checkpoint(MODEL_REPO)\n",
    "\n",
    "    if latest_checkpoint:\n",
    "        print(f\"Found checkpoint: {latest_checkpoint}\")\n",
    "        print(f\"Resuming from epoch {start_epoch}\")\n",
    "\n",
    "        try:\n",
    "            from huggingface_hub import hf_hub_download\n",
    "            checkpoint_path = hf_hub_download(\n",
    "                repo_id=MODEL_REPO,\n",
    "                filename=latest_checkpoint,\n",
    "                repo_type=\"model\",\n",
    "                cache_dir=os.path.join(OUTPUT_DIR, \"hf_cache\")\n",
    "            )\n",
    "\n",
    "            # Load model - will be assigned after model architecture is defined\n",
    "            RESUME_TRAINING = True\n",
    "            INITIAL_EPOCH = start_epoch\n",
    "            RESUME_CHECKPOINT_PATH = checkpoint_path\n",
    "            print(\"✓ Will resume from checkpoint\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint: {e}\")\n",
    "            print(\"Will train from scratch\")\n",
    "    else:\n",
    "        print(\"No checkpoint found. Training from scratch.\")\n",
    "else:\n",
    "    print(\"Not using HuggingFace - training from scratch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiS_yoFIy03K"
   },
   "source": [
    "## 6.3 Class Balancing & Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpoI0dsly03L"
   },
   "outputs": [],
   "source": [
    "# ECG Data Augmentation Functions\n",
    "def augment_ecg_signal(ecg, augmentation_type='noise'):\n",
    "    \"\"\"\n",
    "    Apply augmentation to ECG signal.\n",
    "\n",
    "    Types:\n",
    "    - 'noise': Add Gaussian noise\n",
    "    - 'scale': Amplitude scaling\n",
    "    - 'shift': Time shifting\n",
    "    - 'stretch': Time stretching\n",
    "    \"\"\"\n",
    "    ecg_aug = ecg.copy()\n",
    "\n",
    "    if augmentation_type == 'noise':\n",
    "        # Add small Gaussian noise (SNR ~20-30dB)\n",
    "        noise_level = 0.05 * np.std(ecg_aug)\n",
    "        noise = np.random.normal(0, noise_level, ecg_aug.shape)\n",
    "        ecg_aug = ecg_aug + noise\n",
    "\n",
    "    elif augmentation_type == 'scale':\n",
    "        # Amplitude scaling (0.9-1.1x)\n",
    "        scale_factor = np.random.uniform(0.9, 1.1)\n",
    "        ecg_aug = ecg_aug * scale_factor\n",
    "\n",
    "    elif augmentation_type == 'shift':\n",
    "        # Time shift (up to 5% of length)\n",
    "        shift_samples = int(0.05 * len(ecg_aug) * np.random.uniform(-1, 1))\n",
    "        ecg_aug = np.roll(ecg_aug, shift_samples, axis=0)\n",
    "\n",
    "    elif augmentation_type == 'stretch':\n",
    "        # Time stretching (0.95-1.05x speed)\n",
    "        from scipy.signal import resample\n",
    "        stretch_factor = np.random.uniform(0.95, 1.05)\n",
    "        new_length = int(len(ecg_aug) * stretch_factor)\n",
    "        ecg_stretched = np.column_stack([resample(ecg_aug[:, i], new_length)\n",
    "                                         for i in range(ecg_aug.shape[1])])\n",
    "        # Crop or pad to original length\n",
    "        if len(ecg_stretched) > len(ecg_aug):\n",
    "            start = (len(ecg_stretched) - len(ecg_aug)) // 2\n",
    "            ecg_aug = ecg_stretched[start:start+len(ecg_aug)]\n",
    "        else:\n",
    "            pad_before = (len(ecg_aug) - len(ecg_stretched)) // 2\n",
    "            pad_after = len(ecg_aug) - len(ecg_stretched) - pad_before\n",
    "            ecg_aug = np.pad(ecg_stretched, ((pad_before, pad_after), (0, 0)), mode='edge')\n",
    "\n",
    "    return ecg_aug.astype(np.float16)\n",
    "\n",
    "\n",
    "print(\"✓ Augmentation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_NQSpMEey03L"
   },
   "outputs": [],
   "source": [
    "# Data loading function\n",
    "def load_data_from_hdf5(h5_file):\n",
    "    \"\"\"\n",
    "    Load all data from HDF5 file.\n",
    "    Returns X (list of variable-length arrays), y, lengths\n",
    "    \"\"\"\n",
    "    X_all = []\n",
    "    y_all = []\n",
    "    lengths_all = []\n",
    "\n",
    "    with h5py.File(h5_file, 'r') as h5f:\n",
    "        for bin_name in sorted(h5f.keys()):\n",
    "            if not bin_name.startswith('bin_'):\n",
    "                continue\n",
    "\n",
    "            grp = h5f[bin_name]\n",
    "            X_bin = grp['X'][:]\n",
    "            y_bin = grp['y'][:]\n",
    "            lengths_bin = grp['lengths'][:]\n",
    "\n",
    "            # Trim each recording to actual length\n",
    "            for i in range(len(X_bin)):\n",
    "                X_all.append(X_bin[i, :lengths_bin[i], :])\n",
    "                y_all.append(y_bin[i])\n",
    "                lengths_all.append(lengths_bin[i])\n",
    "\n",
    "    return X_all, np.array(y_all), np.array(lengths_all)\n",
    "\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data from HDF5...\")\n",
    "X_data_original, y_data_original, lengths_data_original = load_data_from_hdf5(HDF5_FILE)\n",
    "\n",
    "print(f\"Loaded {len(X_data_original)} samples\")\n",
    "print(f\"Length range: {lengths_data_original.min()/TARGET_FS:.1f}s - {lengths_data_original.max()/TARGET_FS:.1f}s\")\n",
    "print(f\"Mean length: {lengths_data_original.mean()/TARGET_FS:.1f}s\")\n",
    "\n",
    "# Analyze class distribution BEFORE balancing\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLASS DISTRIBUTION - BEFORE BALANCING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class_counts_before = y_data_original.sum(axis=0).astype(int)\n",
    "healthy_idx = ICD_CODES.index('Healthy')\n",
    "\n",
    "for i, (name, count) in enumerate(zip(DISEASE_NAMES, class_counts_before)):\n",
    "    print(f\"{name:40s}: {count:5d} samples\")\n",
    "\n",
    "print(f\"\\nTotal samples: {len(X_data_original)}\")\n",
    "print(f\"Healthy samples: {class_counts_before[healthy_idx]} ({100*class_counts_before[healthy_idx]/len(X_data_original):.1f}%)\")\n",
    "print(f\"Disease samples: {len(X_data_original) - class_counts_before[healthy_idx]}\")\n",
    "\n",
    "# Find largest disease class (excluding Healthy)\n",
    "disease_counts = [class_counts_before[i] for i in range(NUM_CLASSES) if i != healthy_idx]\n",
    "max_disease_count = max(disease_counts)\n",
    "print(f\"Largest disease class: {max_disease_count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_d1hAHUy03L"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MEMORY OPTIMIZATION: This cell is no longer needed!\n",
    "# ============================================================================\n",
    "# The old approach created X_data_balanced which duplicated ~9.5 GB of data.\n",
    "# New approach uses lazy loading + index-based oversampling.\n",
    "# This cell can be deleted.\n",
    "\n",
    "print(\"⚠️ This cell has been replaced by lazy loading approach\")\n",
    "print(\"See cells above for new implementation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FSlxzpMTy03M"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MEMORY OPTIMIZATION: Load data using lazy loader\n",
    "# ============================================================================\n",
    "# No more X_data_balanced duplication!\n",
    "\n",
    "print(\"\\nInitializing lazy HDF5 loader...\")\n",
    "lazy_loader = LazyHDF5Loader(HDF5_FILE)\n",
    "\n",
    "print(f\"Loaded metadata for {len(lazy_loader)} samples\")\n",
    "print(f\"Length range: {lazy_loader.lengths_all.min()/TARGET_FS:.1f}s - {lazy_loader.lengths_all.max()/TARGET_FS:.1f}s\")\n",
    "print(f\"Mean length: {lazy_loader.lengths_all.mean()/TARGET_FS:.1f}s\")\n",
    "\n",
    "# Calculate class distribution\n",
    "class_counts = lazy_loader.y_all.sum(axis=0)\n",
    "print(f\"\\nClass distribution before balancing:\")\n",
    "for i, (name, count) in enumerate(zip(DISEASE_NAMES, class_counts)):\n",
    "    print(f\"{name:40s}: {int(count):5d} samples\")\n",
    "\n",
    "healthy_idx = np.where(np.array(DISEASE_NAMES) == 'Healthy')[0][0]\n",
    "print(f\"\\nTotal samples: {len(lazy_loader)}\")\n",
    "print(f\"Healthy samples: {int(class_counts[healthy_idx])} ({100*class_counts[healthy_idx]/len(lazy_loader):.1f}%)\")\n",
    "print(f\"Disease samples: {len(lazy_loader) - int(class_counts[healthy_idx])}\")\n",
    "\n",
    "# Split data using indices only (no data duplication!)\n",
    "indices = np.arange(len(lazy_loader))\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    indices, test_size=0.3, random_state=42,\n",
    "    stratify=lazy_loader.y_all.argmax(axis=1)\n",
    ")\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx, test_size=0.5, random_state=42,\n",
    "    stratify=lazy_loader.y_all[temp_idx].argmax(axis=1)\n",
    ")\n",
    "\n",
    "print(f\"\\nData split:\")\n",
    "print(f\"  Train: {len(train_idx)} samples\")\n",
    "print(f\"  Val:   {len(val_idx)} samples\")\n",
    "print(f\"  Test:  {len(test_idx)} samples\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# UNDERSAMPLE HEALTHY CLASS IN TRAINING SET\n",
    "# ============================================================================\n",
    "# Healthy class dominates the dataset (11,155 samples vs ~20-1,000 per disease)\n",
    "# We undersample Healthy to 100 samples to create balanced training data\n",
    "\n",
    "print(\"\n",
    "Undersampling Healthy class in training set...\")\n",
    "\n",
    "# Find Healthy class index\n",
    "healthy_idx_in_disease_names = np.where(np.array(DISEASE_NAMES) == 'Healthy')[0][0]\n",
    "\n",
    "# Identify which training samples are Healthy\n",
    "train_labels = lazy_loader.y_all[train_idx]\n",
    "healthy_mask_train = train_labels[:, healthy_idx_in_disease_names] == 1\n",
    "\n",
    "# Separate Healthy and Disease samples in training set\n",
    "healthy_train_samples = train_idx[healthy_mask_train]\n",
    "disease_train_samples = train_idx[~healthy_mask_train]\n",
    "\n",
    "# Randomly select only 100 Healthy samples\n",
    "np.random.seed(42)  # For reproducibility\n",
    "healthy_target = 100\n",
    "if len(healthy_train_samples) > healthy_target:\n",
    "    selected_healthy = np.random.choice(healthy_train_samples, size=healthy_target, replace=False)\n",
    "else:\n",
    "    selected_healthy = healthy_train_samples\n",
    "    print(f\"  Warning: Only {len(healthy_train_samples)} Healthy samples available (target: {healthy_target})\")\n",
    "\n",
    "# Recombine: all disease samples + 100 Healthy samples\n",
    "train_idx = np.concatenate([disease_train_samples, selected_healthy])\n",
    "np.random.shuffle(train_idx)  # Shuffle to mix Healthy and Disease samples\n",
    "\n",
    "print(f\"✓ Training set undersampled:\")\n",
    "print(f\"  Disease samples: {len(disease_train_samples)}\")\n",
    "print(f\"  Healthy samples: {len(selected_healthy)} (reduced from {len(healthy_train_samples)})\")\n",
    "print(f\"  Total training samples (before oversampling): {len(train_idx)}\")\n",
    "\n",
    "# Calculate oversampling rates\n",
    "oversample_rates = calculate_oversample_rates(lazy_loader.y_all, target_samples_per_class=100)\n",
    "\n",
    "print(\"\\n✓ Data ready for training (lazy loading enabled!)\")\n",
    "print(\"🎯 Memory savings: ~18 GB → ~0.5 GB!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zMk98P73y03M"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MEMORY OPTIMIZATION: Create dataset with lazy loading + augmentation\n",
    "# ============================================================================\n",
    "\n",
    "def create_optimized_dataset(lazy_loader, indices, oversample_rates, batch_size=16,\n",
    "                            shuffle=True, augment=False, augment_prob=0.8):\n",
    "    \"\"\"\n",
    "    Create tf.data.Dataset with lazy loading and on-the-fly augmentation.\n",
    "\n",
    "    Args:\n",
    "        lazy_loader: LazyHDF5Loader instance\n",
    "        indices: Sample indices to include\n",
    "        oversample_rates: How many times to repeat each sample\n",
    "        batch_size: Batch size\n",
    "        shuffle: Whether to shuffle\n",
    "        augment: Whether to apply augmentation\n",
    "        augment_prob: Probability of augmentation per sample\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset ready for training\n",
    "    \"\"\"\n",
    "    def generator():\n",
    "        # Create expanded index list based on oversample rates\n",
    "        expanded_indices = []\n",
    "        for idx in indices:\n",
    "            rate = oversample_rates[idx]\n",
    "            expanded_indices.extend([idx] * rate)\n",
    "\n",
    "        # Shuffle expanded indices if requested\n",
    "        if shuffle:\n",
    "            np.random.shuffle(expanded_indices)\n",
    "\n",
    "        for idx in expanded_indices:\n",
    "            X, y, length = lazy_loader[idx]\n",
    "\n",
    "            # Apply augmentation for oversampled copies\n",
    "            if augment and oversample_rates[idx] > 1:\n",
    "                if np.random.random() < augment_prob:\n",
    "                    X = augment_ecg(X, p=augment_prob)\n",
    "\n",
    "            yield X.astype(np.float32), y.astype(np.float32)\n",
    "\n",
    "    # Infer output signature from first sample\n",
    "    sample_X, sample_y, _ = lazy_loader[indices[0]]\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, sample_X.shape[1]), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(len(DISEASE_NAMES),), dtype=tf.float32)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # IMPORTANT: Add repeat() so dataset cycles for multiple epochs\n",
    "    dataset = dataset.repeat()\n",
    "\n",
    "    # Pad to fixed length for batching\n",
    "    dataset = dataset.padded_batch(\n",
    "        batch_size,\n",
    "        padded_shapes=([MAX_LENGTH, 12], [len(DISEASE_NAMES)]),\n",
    "        padding_values=(0.0, 0.0)\n",
    "    )\n",
    "\n",
    "    return dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Create datasets\n",
    "print(\"\\nCreating optimized datasets...\")\n",
    "# Create oversample rates for validation and test (no oversampling!)\n",
    "# Validation and test should use the natural data distribution\n",
    "val_oversample_rates = np.ones(len(lazy_loader), dtype=np.int32)\n",
    "test_oversample_rates = np.ones(len(lazy_loader), dtype=np.int32)\n",
    "\n",
    "# Create datasets\n",
    "print(\"\n",
    "Creating optimized datasets...\")\n",
    "train_dataset = create_optimized_dataset(\n",
    "    lazy_loader, train_idx, oversample_rates,\n",
    "    batch_size=BATCH_SIZE, shuffle=True, augment=True  # Training: augmentation + oversampling\n",
    ")\n",
    "val_dataset = create_optimized_dataset(\n",
    "    lazy_loader, val_idx, val_oversample_rates,  # No oversampling!\n",
    "    batch_size=BATCH_SIZE, shuffle=False, augment=False  # No augmentation!\n",
    ")\n",
    "test_dataset = create_optimized_dataset(\n",
    "    lazy_loader, test_idx, test_oversample_rates,  # No oversampling!\n",
    "    batch_size=BATCH_SIZE, shuffle=False, augment=False  # No augmentation!\n",
    ")\n",
    "\n",
    "print(\"✓ Datasets created with lazy loading\")\n",
    "print(\"✓ Training set: Ready (augmentation enabled, oversampling enabled)\")\n",
    "print(\"✓ Validation set: Ready (NO augmentation, NO oversampling - natural distribution)\")\n",
    "print(\"✓ Test set: Ready (NO augmentation, NO oversampling - natural distribution)\")\n",
    "\n",
    "# Calculate steps per epoch for model.fit()\n",
    "# With .repeat(), we must tell Keras when an epoch ends\n",
    "train_samples_total = sum(oversample_rates[train_idx])\n",
    "val_samples_total = len(val_idx)  # FIXED: No oversampling for validation!\n",
    "steps_per_epoch = train_samples_total // BATCH_SIZE\n",
    "validation_steps = val_samples_total // BATCH_SIZE\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Total training samples (after oversampling): {train_samples_total}\")\n",
    "print(f\"  Total validation samples (natural distribution): {val_samples_total}\")\n",
    "print(f\"  Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"  Validation steps: {validation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DrSHPiC_6SW"
   },
   "outputs": [],
   "source": [
    "# Initialize resume training variables (set defaults if not already defined)\n",
    "# These can be overwritten by the checkpoint check cell if HuggingFace is enabled\n",
    "if 'RESUME_TRAINING' not in dir():\n",
    "    RESUME_TRAINING = False\n",
    "if 'RESUME_CHECKPOINT_PATH' not in dir():\n",
    "    RESUME_CHECKPOINT_PATH = None\n",
    "if 'INITIAL_EPOCH' not in dir():\n",
    "    INITIAL_EPOCH = 0\n",
    "\n",
    "# Build or load model\n",
    "if RESUME_TRAINING and RESUME_CHECKPOINT_PATH is not None:\n",
    "    print(\"Loading model from checkpoint...\")\n",
    "    model = keras.models.load_model(\n",
    "        RESUME_CHECKPOINT_PATH,\n",
    "        custom_objects={'FocalLoss': FocalLoss},  # Required for custom loss\n",
    "        safe_mode=False  # Required for Lambda layers\n",
    "    )\n",
    "    print(\"✓ Model loaded from checkpoint\")\n",
    "else:\n",
    "    if RESUME_TRAINING and RESUME_CHECKPOINT_PATH is None:\n",
    "        print(\"⚠️ RESUME_TRAINING is True but no checkpoint path set. Compiling new model...\")\n",
    "    else:\n",
    "        print(\"Compiling model...\")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss=FocalLoss(alpha=0.25, gamma=2.0),  # Focal Loss for class imbalance\n",
    "        metrics=[\n",
    "            keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "            keras.metrics.AUC(name='auc', multi_label=True, num_labels=NUM_CLASSES),\n",
    "            keras.metrics.Precision(name='precision'),\n",
    "            keras.metrics.Recall(name='recall')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"[OK] Model compiled\")\n",
    "    print(\"\\nTraining configuration:\")\n",
    "    print(f\"  Optimizer: Adam (lr={LEARNING_RATE})\")\n",
    "    print(f\"  Loss: Focal Loss (alpha=0.25, gamma=2.0)\")\n",
    "    print(f\"  Metrics: accuracy, AUC, precision, recall\")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_auc',\n",
    "        patience=15,\n",
    "        mode='max',\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(OUTPUT_DIR, 'best_model_v3.keras'),\n",
    "        monitor='val_auc',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.CSVLogger(\n",
    "        os.path.join(OUTPUT_DIR, 'training_history.csv')\n",
    "    )\n",
    "]\n",
    "\n",
    "# Add HuggingFace checkpoint callback if enabled\n",
    "if ENABLE_HF_UPLOAD and MODEL_REPO:\n",
    "    callbacks.insert(0, HuggingFaceCheckpoint(\n",
    "        repo_id=MODEL_REPO,\n",
    "        every_n_epochs=CHECKPOINT_EVERY\n",
    "    ))\n",
    "    print(\"✓ HuggingFace checkpoint callback added\")\n",
    "\n",
    "print(f\"✓ Setup complete. Ready to train from epoch {INITIAL_EPOCH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0etCltjTy03M"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTIONAL: RE-COMPILE MODEL (if needed)\n",
    "# ============================================================================\n",
    "# This cell is only needed if you want to re-compile the model with different settings.\n",
    "# The model is already compiled in the previous cell with Focal Loss.\n",
    "# Uncomment below if you need to re-compile:\n",
    "\n",
    "# print(\"Re-compiling model...\")\n",
    "# model.compile(\n",
    "#     optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "#     loss=FocalLoss(alpha=0.25, gamma=2.0),  # Focal Loss for class imbalance\n",
    "#     metrics=[\n",
    "#         keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "#         keras.metrics.AUC(name='auc', multi_label=True, num_labels=NUM_CLASSES),\n",
    "#         keras.metrics.Precision(name='precision'),\n",
    "#         keras.metrics.Recall(name='recall')\n",
    "#     ]\n",
    "# )\n",
    "# print(\"[OK] Model re-compiled with Focal Loss\")\n",
    "\n",
    "print(\"ℹ️ Model already compiled with Focal Loss in previous cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2kTBlDQy03O"
   },
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "480bd04aa442446f8420303189af4fd9",
      "a93cc57ea3904c08b4e626a1449b06ad",
      "7d2ea80bfe64412791a704f8aa424b9e",
      "57a0153b0f2649f189eb6f55a53cc656",
      "8fd47eb5d08140a0a261bfab93f6e561",
      "06bd2fec63d644c09a7e19a69b2d37ee",
      "b08f8846e3a64d4eb60f19412b150768",
      "835ca5848cd142dd82e9f860d2b8f41a",
      "31b264788c484751a25db84c916e7641",
      "88fd5f21ccd7446997ec5e8bef6b462b",
      "e3db52575b9546d4b5a897ad75656198",
      "f1f62158830c4bf7b4a3cd936b9a23f6",
      "b1cd20b195234962ab37cd662ba0be87",
      "2a9512a533dd4ff983690dcab3777427",
      "f1dda0f1a73d47db8849a6c6809bc120",
      "09d46691844d42e7a35b957e4563cf53",
      "af04a3ace6cf4e91846f8258b5b8ead8",
      "c4870e39286f4c4c9d363a448b091e37",
      "e1b9544fcdd5498a8f08de27669f6921",
      "7731f928c751404f939fe6202f278447",
      "ed277113d7c24d639eb87da7d1d0af80",
      "1e546541ccf6452a888e85ef5068853a",
      "107b1c5c5ed84da8b7b249672fcc94c9",
      "1c2d08e7142a4b43b029fb93b792db29",
      "22661724be1d4e20a19fe9ea13ae72d5",
      "cb04fda2dca8445fb1a5ac173a3c2b4c",
      "d3bfc77f75064272992ea3e467a72672",
      "4f29d2776e804a83a44f6de75d128514",
      "db76bc24fa334028804754179efe69e0",
      "403bbd22d22c48d4a4a625e6a31a6c9c",
      "a145958b016349e68a5b77acb492f542",
      "aad83dc6278a4a78938ea0e79e5d2222",
      "d5f6b675493b4d4bafe45c0a74fe5f56",
      "60d839c2090c47e7a390eb981f33011e",
      "4b447baf4cba41ed918f8e1e395e47c1",
      "999a788911aa4ae9bf5e3ce37c3c5d06",
      "e4032076b20d48d1a1ee65225e9caaea",
      "e0ea68de0fdc4b599681d1119482a597",
      "b8598aee33da4450a3065bf184da06ab"
     ]
    },
    "id": "WAVNSRBqy03O",
    "outputId": "8d2b7fb3-99a0-41d5-ee4e-ad03a13fa61a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from epoch 0...\n",
      "\n",
      "Epoch 1/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66s/step - accuracy: 0.5056 - auc: 0.4764 - loss: 6.7030 - precision: 0.0636 - recall: 0.4996  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The `save_format` argument is deprecated in Keras 3. We recommend removing this argument as it can be inferred from the file path. Received: save_format=keras\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Epoch 1: Saving checkpoint to Hugging Face...\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480bd04aa442446f8420303189af4fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93cc57ea3904c08b4e626a1449b06ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2ea80bfe64412791a704f8aa424b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  .../checkpoint_epoch_1.keras:  12%|#2        |  578kB / 4.65MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint uploaded: epoch 1\n",
      "✓ Metrics uploaded\n",
      "============================================================\n",
      "\n",
      "\n",
      "Epoch 1: val_auc improved from -inf to 0.50324, saving model to /dataset/multilabel_19v3/best_model_v3.keras\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2403s\u001b[0m 135s/step - accuracy: 0.5057 - auc: 0.4768 - loss: 6.6895 - precision: 0.0637 - recall: 0.4997 - val_accuracy: 0.4966 - val_auc: 0.5032 - val_loss: 2.3934 - val_precision: 0.0135 - val_recall: 0.1126 - learning_rate: 2.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63s/step - accuracy: 0.5116 - auc: 0.5072 - loss: 6.0342 - precision: 0.0706 - recall: 0.5529  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The `save_format` argument is deprecated in Keras 3. We recommend removing this argument as it can be inferred from the file path. Received: save_format=keras\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Epoch 2: Saving checkpoint to Hugging Face...\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a0153b0f2649f189eb6f55a53cc656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd47eb5d08140a0a261bfab93f6e561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06bd2fec63d644c09a7e19a69b2d37ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  .../checkpoint_epoch_2.keras:  12%|#2        |  564kB / 4.65MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint uploaded: epoch 2\n",
      "✓ Metrics uploaded\n",
      "============================================================\n",
      "\n",
      "\n",
      "Epoch 2: val_auc did not improve from 0.50324\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2383s\u001b[0m 140s/step - accuracy: 0.5116 - auc: 0.5075 - loss: 6.0313 - precision: 0.0706 - recall: 0.5531 - val_accuracy: 0.4933 - val_auc: 0.5004 - val_loss: 2.4167 - val_precision: 0.0138 - val_recall: 0.1159 - learning_rate: 2.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58s/step - accuracy: 0.5205 - auc: 0.5247 - loss: 5.5269 - precision: 0.0733 - recall: 0.5689 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The `save_format` argument is deprecated in Keras 3. We recommend removing this argument as it can be inferred from the file path. Received: save_format=keras\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Epoch 3: Saving checkpoint to Hugging Face...\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08f8846e3a64d4eb60f19412b150768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4870e39286f4c4c9d363a448b091e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db76bc24fa334028804754179efe69e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  .../checkpoint_epoch_3.keras:  12%|#2        |  579kB / 4.65MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint uploaded: epoch 3\n",
      "✓ Metrics uploaded\n",
      "============================================================\n",
      "\n",
      "\n",
      "Epoch 3: val_auc improved from 0.50324 to 0.54754, saving model to /dataset/multilabel_19v3/best_model_v3.keras\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2337s\u001b[0m 137s/step - accuracy: 0.5205 - auc: 0.5248 - loss: 5.5223 - precision: 0.0733 - recall: 0.5690 - val_accuracy: 0.5194 - val_auc: 0.5475 - val_loss: 2.4247 - val_precision: 0.0146 - val_recall: 0.1159 - learning_rate: 2.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m 5/18\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:02\u001b[0m 19s/step - accuracy: 0.5174 - auc: 0.5125 - loss: 5.3640 - precision: 0.0734 - recall: 0.5751"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "print(f\"Starting training from epoch {INITIAL_EPOCH}...\\n\")\n",
    "\n",
    "skip_training = True\n",
    "\n",
    "if skip_training:\n",
    "\n",
    "  # Reconstruct history from saved metrics (epochs 1-17)\n",
    "  import pandas as pd\n",
    "  import json\n",
    "  from huggingface_hub import hf_hub_download\n",
    "\n",
    "  print(\"Reconstructing training history from saved metrics...\\n\")\n",
    "\n",
    "  metrics_data = []\n",
    "  for epoch in range(1, 18):  # Epochs 1 to 17\n",
    "      try:\n",
    "          metrics_file = hf_hub_download(\n",
    "              repo_id=MODEL_REPO,\n",
    "              filename=f\"checkpoints/metrics_epoch_{epoch}.json\",\n",
    "              repo_type=\"model\"\n",
    "          )\n",
    "\n",
    "          with open(metrics_file, 'r') as f:\n",
    "              epoch_metrics = json.load(f)\n",
    "              metrics_data.append(epoch_metrics)\n",
    "\n",
    "          if epoch % 5 == 0:\n",
    "              print(f\"✓ Loaded epoch {epoch}\")\n",
    "\n",
    "      except Exception as e:\n",
    "          print(f\"⚠ Skipping epoch {epoch}: {e}\")\n",
    "          continue\n",
    "\n",
    "  # Convert to history object\n",
    "  history_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "  class History:\n",
    "      def __init__(self, df):\n",
    "          self.history = {}\n",
    "          for col in df.columns:\n",
    "              if col != 'epoch':\n",
    "                  self.history[col] = df[col].tolist()\n",
    "\n",
    "  history = History(history_df)\n",
    "  print(f\"\\n✓ Reconstructed history for {len(history_df)} epochs\")\n",
    "  print(f\"✓ Available metrics: {list(history.history.keys())}\")\n",
    "\n",
    "else:\n",
    "\n",
    "  history = model.fit(\n",
    "      train_dataset,\n",
    "      steps_per_epoch=steps_per_epoch,\n",
    "      validation_data=val_dataset,\n",
    "      validation_steps=validation_steps,\n",
    "      epochs=EPOCHS,\n",
    "      initial_epoch=INITIAL_EPOCH,  # Resume from here if checkpoint exists\n",
    "      callbacks=callbacks,\n",
    "      verbose=1\n",
    "  )\n",
    "\n",
    "  print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AfIarx1hy03O"
   },
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history.history['loss'], label='Train Loss')\n",
    "axes[0, 0].plot(history.history['val_loss'], label='Val Loss')\n",
    "axes[0, 0].set_title('Loss', fontsize=12)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# AUC\n",
    "axes[0, 1].plot(history.history['auc'], label='Train AUC')\n",
    "axes[0, 1].plot(history.history['val_auc'], label='Val AUC')\n",
    "axes[0, 1].set_title('AUC', fontsize=12)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('AUC')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Accuracy\n",
    "axes[1, 0].plot(history.history['accuracy'], label='Train Accuracy')\n",
    "axes[1, 0].plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "axes[1, 0].set_title('Accuracy', fontsize=12)\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Precision & Recall\n",
    "# axes[1, 1].plot(history.history['precision'], label='Train Precision', linestyle='--')\n",
    "# axes[1, 1].plot(history.history['recall'], label='Train Recall', linestyle=':')\n",
    "# axes[1, 1].plot(history.history['val_precision'], label='Val Precision', linestyle='--')\n",
    "# axes[1, 1].plot(history.history['val_recall'], label='Val Recall', linestyle=':')\n",
    "# axes[1, 1].set_title('Precision & Recall', fontsize=12)\n",
    "# axes[1, 1].set_xlabel('Epoch')\n",
    "# axes[1, 1].set_ylabel('Score')\n",
    "# axes[1, 1].legend()\n",
    "# axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'training_history.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Training curves saved to {OUTPUT_DIR}/training_history.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsJ7EwGJy03P"
   },
   "source": [
    "## 8. Evaluation with Disease Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HyUFLem3y03P"
   },
   "outputs": [],
   "source": [
    "# Get predictions on test set (OPTIMIZED)\n",
    "print(\"Generating predictions on test set...\")\n",
    "\n",
    "# Calculate test steps\n",
    "test_samples = len(test_idx)\n",
    "test_steps = test_samples // BATCH_SIZE + (1 if test_samples % BATCH_SIZE != 0 else 0)\n",
    "\n",
    "print(f\"Test samples: {test_samples}\")\n",
    "print(f\"Test steps: {test_steps}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Get ground truth labels\n",
    "y_true_test = lazy_loader.y_all[test_idx]\n",
    "\n",
    "# Predict with specified steps to avoid infinite loop\n",
    "y_pred_proba_test = model.predict(\n",
    "    test_dataset,\n",
    "    steps=test_steps,  # CRITICAL: specify steps\n",
    "    verbose=1\n",
    "    )\n",
    "\n",
    "# Trim predictions to match actual test size (in case of padding)\n",
    "y_pred_proba_test = y_pred_proba_test[:test_samples]\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "y_pred_test = (y_pred_proba_test > 0.5).astype(int)\n",
    "\n",
    "print(f\"\\n✓ Predictions complete\")\n",
    "print(f\"  Test samples: {len(y_true_test)}\")\n",
    "print(f\"  Predictions shape: {y_pred_proba_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BiU3a8Oby03P"
   },
   "outputs": [],
   "source": [
    "# Overall metrics\n",
    "print(\"=\"*80)\n",
    "print(\"OVERALL TEST METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Subset accuracy (exact match)\n",
    "subset_acc = np.mean(np.all(y_true_test == y_pred_test, axis=1))\n",
    "print(f\"Subset Accuracy (Exact Match): {subset_acc:.4f}\")\n",
    "\n",
    "# Hamming loss\n",
    "hamming = hamming_loss(y_true_test, y_pred_test)\n",
    "print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "\n",
    "# F1 scores\n",
    "f1_micro = f1_score(y_true_test, y_pred_test, average='micro', zero_division=0)\n",
    "f1_macro = f1_score(y_true_test, y_pred_test, average='macro', zero_division=0)\n",
    "f1_weighted = f1_score(y_true_test, y_pred_test, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"F1-Score (Micro):    {f1_micro:.4f}\")\n",
    "print(f\"F1-Score (Macro):    {f1_macro:.4f}\")\n",
    "print(f\"F1-Score (Weighted): {f1_weighted:.4f}\")\n",
    "\n",
    "# Precision & Recall\n",
    "prec_micro = precision_score(y_true_test, y_pred_test, average='micro', zero_division=0)\n",
    "rec_micro = recall_score(y_true_test, y_pred_test, average='micro', zero_division=0)\n",
    "\n",
    "print(f\"Precision (Micro):   {prec_micro:.4f}\")\n",
    "print(f\"Recall (Micro):      {rec_micro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PC4Z7b_gy03P"
   },
   "outputs": [],
   "source": [
    "# Per-class metrics with DISEASE NAMES ONLY (no ICD codes)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PER-CLASS METRICS (Grouped by Disease Category)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "per_class_results = {}\n",
    "\n",
    "# Group by disease category\n",
    "for group_name, group_icds in DISEASE_GROUPS.items():\n",
    "    print(f\"\\n{group_name.upper()}:\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for icd in group_icds:\n",
    "        if icd not in ICD_CODES:\n",
    "            continue\n",
    "\n",
    "        idx = ICD_CODES.index(icd)\n",
    "        disease_name = DISEASE_NAMES[idx]  # Get disease name\n",
    "\n",
    "        y_true_class = y_true_test[:, idx]\n",
    "        y_pred_class = y_pred_test[:, idx]\n",
    "        y_pred_proba_class = y_pred_proba_test[:, idx]\n",
    "\n",
    "        support = int(y_true_class.sum())\n",
    "\n",
    "        if support > 0:\n",
    "            precision = precision_score(y_true_class, y_pred_class, zero_division=0)\n",
    "            recall = recall_score(y_true_class, y_pred_class, zero_division=0)\n",
    "            f1 = f1_score(y_true_class, y_pred_class, zero_division=0)\n",
    "\n",
    "            try:\n",
    "                roc_auc = roc_auc_score(y_true_class, y_pred_proba_class)\n",
    "            except:\n",
    "                roc_auc = 0.0\n",
    "\n",
    "            per_class_results[disease_name] = {\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'roc_auc': roc_auc,\n",
    "                'support': support\n",
    "            }\n",
    "\n",
    "            # Print ONLY disease name (no ICD code)\n",
    "            print(f\"\\n{disease_name}:\")  # Changed from showing ICD code\n",
    "            print(f\"  Support:   {support:4d}\")\n",
    "            print(f\"  Precision: {precision:.4f}\")\n",
    "            print(f\"  Recall:    {recall:.4f}\")\n",
    "            print(f\"  F1-Score:  {f1:.4f}\")\n",
    "            print(f\"  ROC-AUC:   {roc_auc:.4f}\")\n",
    "\n",
    "# Healthy class\n",
    "print(f\"\\nHEALTHY:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "idx = ICD_CODES.index('Healthy')\n",
    "disease_name = DISEASE_NAMES[idx]  # \"Healthy\"\n",
    "\n",
    "y_true_class = y_true_test[:, idx]\n",
    "y_pred_class = y_pred_test[:, idx]\n",
    "y_pred_proba_class = y_pred_proba_test[:, idx]\n",
    "\n",
    "support = int(y_true_class.sum())\n",
    "\n",
    "if support > 0:\n",
    "    precision = precision_score(y_true_class, y_pred_class, zero_division=0)\n",
    "    recall = recall_score(y_true_class, y_pred_class, zero_division=0)\n",
    "    f1 = f1_score(y_true_class, y_pred_class, zero_division=0)\n",
    "\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_true_class, y_pred_proba_class)\n",
    "    except:\n",
    "        roc_auc = 0.0\n",
    "\n",
    "    per_class_results[disease_name] = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'support': support\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{disease_name}:\")  # Just \"Healthy\"\n",
    "    print(f\"  Support:   {support:4d}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "    print(f\"  ROC-AUC:   {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nj41lw9y03P"
   },
   "source": [
    "## 9. Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFhZdxDfy03P"
   },
   "outputs": [],
   "source": [
    "def show_prediction_with_names(sample_idx, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Display prediction with DISEASE NAMES ONLY (no ICD codes).\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Sample {sample_idx + 1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # Get data\n",
    "    test_sample_idx = test_idx[sample_idx]\n",
    "    y_true = y_true_test[sample_idx]\n",
    "    y_pred_proba = y_pred_proba_test[sample_idx]\n",
    "\n",
    "    # True labels - DISEASE NAMES ONLY\n",
    "    true_indices = np.where(y_true == 1)[0]\n",
    "    print(f\"\\nTrue Diagnoses ({len(true_indices)}):\")\n",
    "    if len(true_indices) > 0:\n",
    "        for idx in true_indices:\n",
    "            print(f\"  - {DISEASE_NAMES[idx]}\")  # ONLY disease name\n",
    "    else:\n",
    "        print(\"  - None\")\n",
    "\n",
    "    # Predicted labels - DISEASE NAMES ONLY\n",
    "    pred_indices = np.where(y_pred_proba > threshold)[0]\n",
    "    print(f\"\\nPredicted Diagnoses (threshold={threshold}):\")\n",
    "    if len(pred_indices) > 0:\n",
    "        for idx in sorted(pred_indices, key=lambda i: -y_pred_proba[i]):\n",
    "            prob = y_pred_proba[idx]\n",
    "            marker = \"✓\" if y_true[idx] == 1 else \" \"\n",
    "            print(f\"  {marker} {DISEASE_NAMES[idx]}: {prob:.4f}\")  # ONLY disease name\n",
    "    else:\n",
    "        print(\"  - None\")\n",
    "\n",
    "    # Top 5 predictions - DISEASE NAMES ONLY\n",
    "    print(f\"\\nTop 5 Predictions (all probabilities):\")\n",
    "    top_5 = np.argsort(y_pred_proba)[-5:][::-1]\n",
    "    for idx in top_5:\n",
    "        prob = y_pred_proba[idx]\n",
    "        marker = \"✓\" if y_true[idx] == 1 else \" \"\n",
    "        print(f\"  {marker} {DISEASE_NAMES[idx]}: {prob:.4f}\")  # ONLY disease name\n",
    "\n",
    "\n",
    "# Show predictions for random samples\n",
    "print(\"\\nSample Predictions:\")\n",
    "random_samples = np.random.choice(len(test_idx), size=min(5, len(test_idx)), replace=False)\n",
    "for i in random_samples:\n",
    "    show_prediction_with_names(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KlNz05Qiy03P"
   },
   "outputs": [],
   "source": [
    "# Plot ECG signal with predictions\n",
    "def plot_ecg_with_predictions(sample_idx, num_leads=3):\n",
    "    \"\"\"\n",
    "    Plot ECG signal with true and predicted labels (DISEASE NAMES ONLY).\n",
    "    \"\"\"\n",
    "    test_sample_idx = test_idx[sample_idx]\n",
    "    # Use lazy_loader to get the ECG signal (X_data is not defined)\n",
    "    ecg_signal, _, _ = lazy_loader[test_sample_idx]\n",
    "    y_true = y_true_test[sample_idx]\n",
    "    y_pred_proba = y_pred_proba_test[sample_idx]\n",
    "\n",
    "    # Get disease names\n",
    "    true_labels = [DISEASE_NAMES[j] for j in range(NUM_CLASSES) if y_true[j] == 1]\n",
    "    pred_labels = [(DISEASE_NAMES[j], y_pred_proba[j])\n",
    "                  for j in range(NUM_CLASSES) if y_pred_proba[j] > 0.5]\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(num_leads, 1, figsize=(15, 8))\n",
    "    if num_leads == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    time = np.arange(len(ecg_signal)) / TARGET_FS\n",
    "    lead_names = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "\n",
    "    for i in range(num_leads):\n",
    "        axes[i].plot(time, ecg_signal[:, i], linewidth=0.8)\n",
    "        axes[i].set_ylabel(f'Lead {lead_names[i]}', fontsize=10)\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[-1].set_xlabel('Time (s)', fontsize=10)\n",
    "\n",
    "    # Title with disease names\n",
    "    title = f'Sample {sample_idx + 1} - ECG Signal ({len(ecg_signal)/TARGET_FS:.1f}s)\\n'\n",
    "    title += f'True: {\", \".join(true_labels) if true_labels else \"None\"}\\n'\n",
    "    if pred_labels:\n",
    "        pred_str = \", \".join([f\"{l} ({p:.2f})\" for l, p in pred_labels])\n",
    "        title += f'Predicted: {pred_str}'\n",
    "    else:\n",
    "        title += 'Predicted: None'\n",
    "\n",
    "    plt.suptitle(title, fontsize=11, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot a few examples\n",
    "print(\"\\nECG Signal Visualizations:\")\n",
    "for i in random_samples[:3]:\n",
    "    plot_ecg_with_predictions(i, num_leads=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcYD6ifGy03P"
   },
   "source": [
    "## 10. Save Model & Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gzSG8sx9y03Q"
   },
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model_path = os.path.join(OUTPUT_DIR, f'{MODEL_NAME}_final.keras')\n",
    "model.save(final_model_path)\n",
    "print(f\"✓ Model saved to {final_model_path}\")\n",
    "\n",
    "# Save comprehensive metadata with DISEASE NAMES\n",
    "results_metadata = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'version': NOTEBOOK_VERSION,\n",
    "    'created': datetime.now().isoformat(),\n",
    "\n",
    "    # Classes - DISEASE NAMES ONLY in main list\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'disease_names': DISEASE_NAMES,  # Main output list - names only\n",
    "    'icd_codes': ICD_CODES,  # Reference only\n",
    "    'icd_to_disease_name': ICD_TO_DISEASE_NAME,  # Mapping reference\n",
    "    'disease_groups': DISEASE_GROUPS,\n",
    "\n",
    "    # Architecture\n",
    "    'architecture': {\n",
    "        'type': 'Simplified 1D CNN',\n",
    "        'blocks': '128→256→256 filters',\n",
    "        'features': ['Simplified Conv Blocks', 'No SE/Attention/Residual'],\n",
    "        'pooling': 'Global Average Pooling',\n",
    "        'total_params': int(model.count_params())\n",
    "    },\n",
    "\n",
    "    # Training\n",
    "    'training': {\n",
    "        'epochs': len(history.history['loss']),\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'loss': 'Focal Loss (alpha=0.25, gamma=2.0)',\n",
    "        'optimizer': 'Adam'\n",
    "    },\n",
    "\n",
    "    # Data - use lazy_loader instead of X_data/lengths_data\n",
    "    'data': {\n",
    "        'total_samples': len(lazy_loader),\n",
    "        'train_samples': len(train_idx),\n",
    "        'val_samples': len(val_idx),\n",
    "        'test_samples': len(test_idx),\n",
    "        'sampling_rate': TARGET_FS,\n",
    "        'num_channels': TARGET_CHANNELS,\n",
    "        'variable_length': True,\n",
    "        'length_range': f\"{lazy_loader.lengths_all.min()/TARGET_FS:.1f}s - {lazy_loader.lengths_all.max()/TARGET_FS:.1f}s\"\n",
    "    },\n",
    "\n",
    "    # Test metrics\n",
    "    'test_metrics': {\n",
    "        'subset_accuracy': float(subset_acc),\n",
    "        'hamming_loss': float(hamming),\n",
    "        'f1_micro': float(f1_micro),\n",
    "        'f1_macro': float(f1_macro),\n",
    "        'f1_weighted': float(f1_weighted),\n",
    "        'precision_micro': float(prec_micro),\n",
    "        'recall_micro': float(rec_micro)\n",
    "    },\n",
    "\n",
    "    # Per-class results (DISEASE NAMES ONLY)\n",
    "    'per_class_results': per_class_results\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = os.path.join(OUTPUT_DIR, 'model_results.json')\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(results_metadata, f, indent=2)\n",
    "\n",
    "print(f\"✓ Metadata saved to {metadata_path}\")\n",
    "\n",
    "# Upload to HuggingFace (if enabled)\n",
    "if ENABLE_HF_UPLOAD and MODEL_REPO and api:\n",
    "    print(f\"\\nUploading final model to HuggingFace...\")\n",
    "\n",
    "    try:\n",
    "        # Upload model\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=final_model_path,\n",
    "            path_in_repo=\"final_model.keras\",\n",
    "            repo_id=MODEL_REPO,\n",
    "            repo_type=\"model\",\n",
    "            commit_message=\"Final trained model\"\n",
    "        )\n",
    "        print(\"✓ Final model uploaded\")\n",
    "\n",
    "        # Upload metadata\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=metadata_path,\n",
    "            path_in_repo=\"model_results.json\",\n",
    "            repo_id=MODEL_REPO,\n",
    "            repo_type=\"model\",\n",
    "            commit_message=\"Add model metadata\"\n",
    "        )\n",
    "        print(\"✓ Metadata uploaded\")\n",
    "\n",
    "        # Upload training history\n",
    "        history_csv = os.path.join(OUTPUT_DIR, 'training_history.csv')\n",
    "        if os.path.exists(history_csv):\n",
    "            api.upload_file(\n",
    "                path_or_fileobj=history_csv,\n",
    "                path_in_repo=\"training_history.csv\",\n",
    "                repo_id=MODEL_REPO,\n",
    "                repo_type=\"model\",\n",
    "                commit_message=\"Add training history\"\n",
    "            )\n",
    "            print(\"✓ Training history uploaded\")\n",
    "\n",
    "        # Upload training plot\n",
    "        history_plot = os.path.join(OUTPUT_DIR, 'training_history.png')\n",
    "        if os.path.exists(history_plot):\n",
    "            api.upload_file(\n",
    "                path_or_fileobj=history_plot,\n",
    "                path_in_repo=\"training_history.png\",\n",
    "                repo_id=MODEL_REPO,\n",
    "                repo_type=\"model\",\n",
    "                commit_message=\"Add training visualization\"\n",
    "            )\n",
    "            print(\"✓ Training visualization uploaded\")\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"✓ All artifacts uploaded to HuggingFace!\")\n",
    "        print(f\"View at: https://huggingface.co/{MODEL_REPO}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not upload to HF: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: {final_model_path}\")\n",
    "print(f\"Metadata: {metadata_path}\")\n",
    "print(f\"Training history: {os.path.join(OUTPUT_DIR, 'training_history.csv')}\")\n",
    "print(f\"\\nTest F1-Score (Macro): {f1_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZO-Xw8Ly03Q"
   },
   "source": [
    "## 11. Generate Model Card (HuggingFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0BDGdRgy03Q"
   },
   "outputs": [],
   "source": [
    "# Generate model card for HuggingFace (if enabled)\n",
    "if ENABLE_HF_UPLOAD and MODEL_REPO:\n",
    "    disease_names_str = '\\n'.join([f\"{i+1}. {name}\" for i, name in enumerate(DISEASE_NAMES)])\n",
    "\n",
    "    model_card = f\"\"\"---\n",
    "language: en\n",
    "tags:\n",
    "- ecg\n",
    "- cardiology\n",
    "- medical\n",
    "- pediatric\n",
    "- time-series\n",
    "- multi-label-classification\n",
    "- tensorflow\n",
    "- cnn\n",
    "datasets:\n",
    "- Neural-Network-Project/ECG-database\n",
    "metrics:\n",
    "- f1\n",
    "- auc\n",
    "- precision\n",
    "- recall\n",
    "library_name: tensorflow\n",
    "---\n",
    "\n",
    "# ECG Disease Classifier - 19 Cardiac Conditions\n",
    "\n",
    "Multi-label classification model for detecting 19 cardiac conditions from pediatric ECG signals.\n",
    "\n",
    "## Model Description\n",
    "\n",
    "    \"## Model Description\n",
    "\n",
    "\"\n",
    "    \"Simplified 1D CNN for variable-length ECG classification (no SE blocks, attention, or residual connections).\n",
    "\n",
    "\"\n",
    "\n",
    "    \"**Architecture:** 128→256→256 filters (simplified conv blocks)\n",
    "\n",
    "\"\n",
    "**Training:** Focal loss for class imbalance\n",
    "**Input:** Variable-length 12-lead ECG (5-120 seconds at 500 Hz)\n",
    "\n",
    "## Disease Classes\n",
    "\n",
    "{disease_names_str}\n",
    "\n",
    "## Performance\n",
    "\n",
    "- F1-Score (Macro): {f1_macro:.4f}\n",
    "- F1-Score (Micro): {f1_micro:.4f}\n",
    "- Subset Accuracy: {subset_acc:.4f}\n",
    "\n",
    "## Intended Use\n",
    "\n",
    "⚠️ **Research and educational purposes only** - NOT for clinical diagnosis\n",
    "\n",
    "## Training Details\n",
    "\n",
    "- Batch Size: {BATCH_SIZE}\n",
    "- Epochs: {len(history.history['loss'])}\n",
    "- Loss: Focal Loss (α=0.25, γ=2.0)\n",
    "- Optimizer: Adam (lr={LEARNING_RATE})\n",
    "\n",
    "## Citation\n",
    "\n",
    "```bibtex\n",
    "@misc{{ecg-classifier-2025,\n",
    "  author = {{Neural-Network-Project}},\n",
    "  title = {{ECG Disease Classifier}},\n",
    "  year = {{2025}},\n",
    "  publisher = {{Hugging Face}},\n",
    "  url = {{https://huggingface.co/{MODEL_REPO}}}\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    # Save locally\n",
    "    readme_path = os.path.join(OUTPUT_DIR, 'README.md')\n",
    "    with open(readme_path, 'w') as f:\n",
    "        f.write(model_card)\n",
    "\n",
    "    print(\"✓ Model card created\")\n",
    "\n",
    "    # Upload to HuggingFace\n",
    "    try:\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=readme_path,\n",
    "            path_in_repo=\"README.md\",\n",
    "            repo_id=MODEL_REPO,\n",
    "            repo_type=\"model\",\n",
    "            commit_message=\"Add model card\"\n",
    "        )\n",
    "        print(\"✓ Model card uploaded to HuggingFace\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not upload model card: {e}\")\n",
    "else:\n",
    "    print(\"Skipping model card generation (ENABLE_HF_UPLOAD=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-H2PtdsLy03Q"
   },
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements a **simplified 1D CNN** for 19-class multi-label ECG classification:\n",
    "\n",
    "**Key Features:**\n",
    "- ✅ Processes full-length variable recordings (5-120 seconds, no windowing)\n",
    "- ✅ 19 classes (18 cardiac diseases + Healthy)\n",
    "- ✅ **Simplified architecture** for better generalization on imbalanced medical data\n",
    "- ✅ Focal loss for class imbalance\n",
    "- ✅ Healthy class undersampling (100 samples) + disease oversampling (100 per class)\n",
    "- ✅ **Validation/test use natural distribution** (no augmentation or oversampling)\n",
    "- ✅ **All outputs show disease names only** (not ICD codes)\n",
    "\n",
    "**Architecture:**\n",
    "- **Simple Conv Blocks:** 128→256→256 filters\n",
    "- **Each block:** Conv1D → BatchNorm → MaxPooling → Dropout\n",
    "- **Global Average Pooling** for variable-length sequences\n",
    "- **Dense layer:** 128 units → BatchNorm → Dropout → Output (19 classes)\n",
    "\n",
    "**Training Data (Balanced):**\n",
    "- Healthy: 100 samples (undersampled from ~7,800)\n",
    "- Each disease: ~100 samples (oversampled from original counts)\n",
    "- Total: ~1,900 training samples\n",
    "\n",
    "**Validation/Test Data (Natural Distribution):**\n",
    "- Healthy: ~78.6% of samples (realistic)\n",
    "- Diseases: ~21.4% of samples (realistic)\n",
    "- **No augmentation or oversampling** for reliable metrics\n",
    "\n",
    "**Next Steps:**\n",
    "- Train with balanced data\n",
    "- Fine-tune prediction threshold per class\n",
    "- Evaluate on natural test distribution\n",
    "- Deploy for inference"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "application/vnd.jupyter.widget-state+json": {
   "state": {
    "09d46691844d42e7a35b957e4563cf53": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "107b1c5c5ed84da8b7b249672fcc94c9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c2d08e7142a4b43b029fb93b792db29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1e546541ccf6452a888e85ef5068853a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22661724be1d4e20a19fe9ea13ae72d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "2a9512a533dd4ff983690dcab3777427": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "31b264788c484751a25db84c916e7641": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a9512a533dd4ff983690dcab3777427",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f1dda0f1a73d47db8849a6c6809bc120",
      "value": 1
     }
    },
    "403bbd22d22c48d4a4a625e6a31a6c9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_60d839c2090c47e7a390eb981f33011e",
      "placeholder": "​",
      "style": "IPY_MODEL_4b447baf4cba41ed918f8e1e395e47c1",
      "value": "  .../checkpoint_epoch_3.keras: 100%"
     }
    },
    "4b447baf4cba41ed918f8e1e395e47c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4f29d2776e804a83a44f6de75d128514": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "60d839c2090c47e7a390eb981f33011e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7731f928c751404f939fe6202f278447": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_22661724be1d4e20a19fe9ea13ae72d5",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cb04fda2dca8445fb1a5ac173a3c2b4c",
      "value": 1
     }
    },
    "835ca5848cd142dd82e9f860d2b8f41a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f1f62158830c4bf7b4a3cd936b9a23f6",
      "placeholder": "​",
      "style": "IPY_MODEL_b1cd20b195234962ab37cd662ba0be87",
      "value": "Processing Files (1 / 1)      : 100%"
     }
    },
    "88fd5f21ccd7446997ec5e8bef6b462b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_09d46691844d42e7a35b957e4563cf53",
      "placeholder": "​",
      "style": "IPY_MODEL_af04a3ace6cf4e91846f8258b5b8ead8",
      "value": " 4.65MB / 4.65MB, 4.64MB/s  "
     }
    },
    "999a788911aa4ae9bf5e3ce37c3c5d06": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a145958b016349e68a5b77acb492f542": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_999a788911aa4ae9bf5e3ce37c3c5d06",
      "max": 4652170,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e4032076b20d48d1a1ee65225e9caaea",
      "value": 4652170
     }
    },
    "aad83dc6278a4a78938ea0e79e5d2222": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e0ea68de0fdc4b599681d1119482a597",
      "placeholder": "​",
      "style": "IPY_MODEL_b8598aee33da4450a3065bf184da06ab",
      "value": " 4.65MB / 4.65MB            "
     }
    },
    "af04a3ace6cf4e91846f8258b5b8ead8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b08f8846e3a64d4eb60f19412b150768": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_835ca5848cd142dd82e9f860d2b8f41a",
       "IPY_MODEL_31b264788c484751a25db84c916e7641",
       "IPY_MODEL_88fd5f21ccd7446997ec5e8bef6b462b"
      ],
      "layout": "IPY_MODEL_e3db52575b9546d4b5a897ad75656198"
     }
    },
    "b1cd20b195234962ab37cd662ba0be87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b8598aee33da4450a3065bf184da06ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c4870e39286f4c4c9d363a448b091e37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e1b9544fcdd5498a8f08de27669f6921",
       "IPY_MODEL_7731f928c751404f939fe6202f278447",
       "IPY_MODEL_ed277113d7c24d639eb87da7d1d0af80"
      ],
      "layout": "IPY_MODEL_1e546541ccf6452a888e85ef5068853a"
     }
    },
    "cb04fda2dca8445fb1a5ac173a3c2b4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d3bfc77f75064272992ea3e467a72672": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5f6b675493b4d4bafe45c0a74fe5f56": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db76bc24fa334028804754179efe69e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_403bbd22d22c48d4a4a625e6a31a6c9c",
       "IPY_MODEL_a145958b016349e68a5b77acb492f542",
       "IPY_MODEL_aad83dc6278a4a78938ea0e79e5d2222"
      ],
      "layout": "IPY_MODEL_d5f6b675493b4d4bafe45c0a74fe5f56"
     }
    },
    "e0ea68de0fdc4b599681d1119482a597": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1b9544fcdd5498a8f08de27669f6921": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_107b1c5c5ed84da8b7b249672fcc94c9",
      "placeholder": "​",
      "style": "IPY_MODEL_1c2d08e7142a4b43b029fb93b792db29",
      "value": "New Data Upload               : 100%"
     }
    },
    "e3db52575b9546d4b5a897ad75656198": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4032076b20d48d1a1ee65225e9caaea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ed277113d7c24d639eb87da7d1d0af80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d3bfc77f75064272992ea3e467a72672",
      "placeholder": "​",
      "style": "IPY_MODEL_4f29d2776e804a83a44f6de75d128514",
      "value": " 4.65MB / 4.65MB, 4.64MB/s  "
     }
    },
    "f1dda0f1a73d47db8849a6c6809bc120": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f1f62158830c4bf7b4a3cd936b9a23f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   },
   "version_major": 2,
   "version_minor": 0
  },
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
