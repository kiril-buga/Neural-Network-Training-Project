{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kiril-buga/Neural-Network-Training-Project/blob/main/1D_CNN_for_multi_label_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-GY8WyKlNbn-",
    "outputId": "1ee35bed-df4e-4df5-d3c9-8ce874419d94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wfdb in /usr/local/lib/python3.12/dist-packages (4.3.0)\n",
      "Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.13.2)\n",
      "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2025.3.0)\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.10.0)\n",
      "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.0.2)\n",
      "Requirement already satisfied: pandas>=2.2.3 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.32.4)\n",
      "Requirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (1.16.3)\n",
      "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (0.13.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.22.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2025.11.12)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.10.0->wfdb) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp>=3.10.11->wfdb) (4.15.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (2.23)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wfdb\n",
    "# ===== Imports =====\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "USE_HF = True"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# 1D CNN for Multi-Label Classification of Pediatric ECG\n\nThis notebook implements a deep learning model for multi-label classification of cardiovascular diseases in pediatric ECG data, based on the reference paper:\n\n**\"A pediatric ECG database with disease diagnosis covering 11643 children\"** (Scientific Data, 2025)\n\n## Dataset Overview\n\n- **Source**: First Affiliated Hospital of Zhengzhou University (2018-2024)\n- **Patients**: 11,643 children aged 0-14 years\n- **ECG Records**: 14,190 recordings (12,334 with 12 leads, 1,856 with 9 leads)\n- **Sampling Rate**: 500 Hz\n- **Record Length**: 5-120 seconds\n- **Target Diseases**: 18 cardiovascular diseases including:\n  - Myocarditis (Fulminant, Viral, Acute, Unspecified)\n  - Cardiomyopathy (Dilated, Hypertrophic, Unspecified)\n  - Kawasaki disease\n  - Congenital heart diseases (VSD, ASD, Tetralogy of Fallot, etc.)\n\n## Model Architecture\n\nThe implemented 1D CNN features:\n\n1. **Residual Connections**: Three convolutional blocks with skip connections for better gradient flow\n2. **Batch Normalization**: After each convolutional layer for training stability\n3. **Dropout Regularization**: Progressive dropout (0.2 â†’ 0.5) to prevent overfitting\n4. **Global Pooling**: Both average and max pooling to capture diverse features\n5. **Multi-Label Output**: Sigmoid activation for independent disease predictions\n\n## Key Features\n\n- **Data Preprocessing**:\n  - Z-score normalization per lead\n  - Resampling/padding to fixed length (5000 samples = 10 seconds)\n  - High-pass filtering to remove baseline wander (< 0.5 Hz)\n\n- **Multi-Label Classification**:\n  - Binary cross-entropy loss\n  - Comprehensive metrics (AUC, Precision, Recall, F1-Score)\n  - Per-class performance evaluation\n\n- **Visualization**:\n  - Training history plots (loss, AUC, precision, recall)\n  - ROC curves for each disease\n  - ECG signal visualization with predictions\n\n- **Model Persistence**:\n  - Saved trained model (.keras format)\n  - Label mapping (JSON)\n  - Performance summary report\n\n## Notebook Structure\n\n1. Setup and data loading\n2. Label encoding for multi-label classification\n3. Signal preprocessing and normalization\n4. Train/validation/test split\n5. 1D CNN model architecture\n6. Model compilation with multi-label metrics\n7. Training with callbacks (early stopping, learning rate reduction)\n8. Evaluation on test set\n9. Visualization and analysis\n10. Model saving and inference instructions\n\n## Usage\n\nRun all cells sequentially to:\n1. Download and preprocess the pediatric ECG dataset\n2. Train the 1D CNN model\n3. Evaluate performance on test set\n4. Generate visualizations and reports\n5. Save the trained model for inference\n\n---\n\n**Note**: This is a multi-label classification problem where each ECG can have multiple disease diagnoses. The model handles class imbalance through appropriate metrics and evaluation strategies.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## ðŸš€ Quick Start Guide\n\n### Execution Order\n\nRun all cells sequentially from top to bottom:\n\n1. **Setup** (cells 1-6): Install dependencies, download dataset, extract files\n2. **Load Metadata**: Load AttributesDictionary.csv\n3. **Load ECG Data**: Load ECG signals with error handling\n4. **Validate Data**: Check data integrity\n5. **Label Encoding**: Create multi-label binary matrix\n6. **Preprocessing**: Normalize and resample signals\n7. **Model Training**: Build, compile, and train 1D CNN\n8. **Evaluation**: Test set metrics and visualizations\n\n### Common Issues\n\n**Error: `cannot reshape array`**\n- This error occurs with corrupted/mismatched ECG files\n- The updated loading code handles this automatically\n- Files with errors will be skipped with warnings\n- Expected: ~95% of files load successfully\n\n**Error: `Y is not defined`**\n- Make sure to run cells in order\n- The metadata must be loaded before ECG data\n\n### Expected Runtime\n- Data loading: 5-10 minutes\n- Preprocessing: 10-15 minutes  \n- Model training: 1-2 hours (with GPU) or 4-6 hours (CPU only)\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87,
     "referenced_widgets": [
      "cb9c3bceac6a4f1a82c45307fe5608d6",
      "c8c9b8b3177841b0b902629f85afaef5",
      "4a4a813d17b641a39c7485172ee26c2d",
      "8a125fba600f426cbb5b022aacef32cf",
      "8294925094d8402987a6e2a47c89519d",
      "d604c7d0ed2445249185387394f203cb",
      "1656e05929c247e9ae3465c15ed6d93c",
      "5caf253c7dd748f085ca441d88fb6835",
      "4504f1b5682d42acaffb528953b4e139",
      "d5a0972c3b4d4e7782ca2dcb4476a723",
      "c2c6973c8d2a4f3b846c712ca1efd9d7"
     ]
    },
    "id": "iLBADierDe_l",
    "outputId": "d48123d8-5915-414e-b1c8-ed70c31f2504"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb9c3bceac6a4f1a82c45307fe5608d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 21 files:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded to: /root/.cache/huggingface/hub/datasets--Neural-Network-Project--ECG-database/snapshots/37c09929463534a0e5cea0f1b81b10e5d017466e\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "local_dir = snapshot_download(\n",
    "    repo_id=\"Neural-Network-Project/ECG-database\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "print(\"Downloaded to:\", local_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J0Hs2qBrEy_W",
    "outputId": "514c2a3e-7092-4d65-a691-b223208807e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_PATH: /root/.cache/huggingface/hub/datasets--Neural-Network-Project--ECG-database/snapshots/37c09929463534a0e5cea0f1b81b10e5d017466e/data/\n",
      "ARTIFACT_DIR: /root/.cache/huggingface/hub/datasets--Neural-Network-Project--ECG-database/snapshots/37c09929463534a0e5cea0f1b81b10e5d017466e/artifacts/\n",
      "Files in DATA_PATH: ['ECGCode.csv', 'ExampleReadingCode.ipynb', 'Child_ecg.zip', 'AttributesDictionary.csv', 'Child_ecg_full.zip', 'Child_ecg.z01', 'Child_ecg', 'DiseaseCode.csv']\n"
     ]
    }
   ],
   "source": [
    "if USE_HF and local_dir:\n",
    "  # Case 2: You want to download the dataset from Huggingface\n",
    "    DATA_PATH = f\"{local_dir}/data/\"\n",
    "    ARTIFACT_DIR = f\"{local_dir}/artifacts/\"\n",
    "\n",
    "else:\n",
    "  # ===== Detect if running in Google Colab and mount Drive =====\n",
    "  IN_COLAB = False\n",
    "  try:\n",
    "      from google.colab import drive  # type: ignore\n",
    "      IN_COLAB = True\n",
    "  except Exception:\n",
    "      drive = None\n",
    "      IN_COLAB = False\n",
    "\n",
    "  if IN_COLAB:\n",
    "      drive.mount('/content/drive/')\n",
    "\n",
    "  # ===== Define paths =====\n",
    "  if IN_COLAB:\n",
    "      # Case 1: You manually placed the dataset in MyDrive\n",
    "      DATA_PATH = \"/content/drive/MyDrive/DeepLearningECG/data/\"\n",
    "      ARTIFACT_DIR = \"/content/drive/MyDrive/DeepLearningECG/artifacts/\"\n",
    "\n",
    "  else:\n",
    "      # Case 3: Local fallback (if running outside Colab)\n",
    "      DATA_PATH = \"../data/\"\n",
    "      ARTIFACT_DIR = \"../artifacts/\"\n",
    "\n",
    "print(\"DATA_PATH:\", DATA_PATH)\n",
    "print(\"ARTIFACT_DIR:\", ARTIFACT_DIR)\n",
    "print(\"Files in DATA_PATH:\", os.listdir(DATA_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "f12f4adc",
    "outputId": "e7d2f8c4-6340-4713-81a0-86b8636a5f72"
   },
   "outputs": [],
   "source": "# ===== Extract ECG Data (only if not already extracted) =====\nimport os\n\nzip_base_name = \"Child_ecg\"\noutput_zip_path = os.path.join(DATA_PATH, f\"{zip_base_name}.zip\")\nextracted_dir = os.path.join(DATA_PATH, zip_base_name)\n\n# Check if already extracted\nif os.path.exists(extracted_dir) and os.path.isdir(extracted_dir):\n    # Check if directory has files\n    try:\n        subdirs = [d for d in os.listdir(extracted_dir) if os.path.isdir(os.path.join(extracted_dir, d))]\n        if len(subdirs) > 0:\n            print(f\"âœ“ Data already extracted to {extracted_dir}\")\n            print(f\"  Found {len(subdirs)} patient directories (P00, P01, ...)\")\n            print(\"  Skipping extraction...\")\n        else:\n            print(f\"Directory {extracted_dir} exists but is empty. Extracting...\")\n            need_extract = True\n    except Exception as e:\n        print(f\"Error checking directory: {e}\")\n        need_extract = True\nelse:\n    print(f\"Extraction directory not found. Extracting data...\")\n    need_extract = True\n\n# Extract if needed\nif 'need_extract' in locals() and need_extract:\n    # Install p7zip-full for handling various archive formats\n    print(\"Installing p7zip-full...\")\n    !apt-get install -y p7zip-full > /dev/null 2>&1\n    \n    print(f\"Extracting {output_zip_path}...\")\n    # Unzip the concatenated file using 7z\n    # -aoa option overwrites existing files without prompt\n    !7z x {output_zip_path} -o{DATA_PATH} -aoa\n    \n    print(\"âœ“ Extraction complete!\")\nelse:\n    print(\"âœ“ Ready to proceed with data loading\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "kYkdgvFsQ6OK",
    "outputId": "4196e7f0-d8eb-49aa-dd76-4b7f70a0b347"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 324K\n",
      "drwxr-xr-x 1001 root root 24K Apr  9  2025 P00\n",
      "drwx------ 1002 root root 24K Apr  9  2025 P01\n",
      "drwx------ 1002 root root 24K Apr  9  2025 P02\n",
      "drwx------ 1002 root root 32K Apr  9  2025 P03\n",
      "drwx------ 1002 root root 24K Apr  9  2025 P04\n",
      "drwx------ 1002 root root 28K Apr  9  2025 P05\n",
      "drwx------ 1002 root root 24K Apr  9  2025 P06\n",
      "drwx------ 1002 root root 24K Apr  9  2025 P07\n",
      "drwx------ 1002 root root 24K Apr  9  2025 P08\n",
      "drwx------ 1002 root root 28K Apr  9  2025 P09\n",
      "drwx------ 1002 root root 28K Apr  9  2025 P10\n",
      "drwx------  646 root root 20K Apr  9  2025 P11\n"
     ]
    }
   ],
   "source": [
    "# Check if it was unzipped -> P00, P01...\n",
    "!ls -lh {DATA_PATH+\"Child_ecg\"}"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ===== Load Attribute Dictionary (Metadata) =====\nimport wfdb\n\nY = pd.read_csv(os.path.join(DATA_PATH, \"AttributesDictionary.csv\"))\nprint(f\"Loaded {len(Y)} ECG records from AttributesDictionary.csv\")\nprint(f\"\\nDataFrame shape: {Y.shape}\")\nprint(f\"\\nFirst few rows:\")\nprint(Y.head())\nprint(f\"\\nColumns: {list(Y.columns)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## âš ï¸ IMPORTANT: Skip This Cell\n\n**DO NOT RUN THIS CELL** - It's a duplicate of the data loading code above.\n\nThe updated data loading with error handling is in the previous cells. This cell is kept for reference only and will be removed in future versions.\n\nPlease continue to the next cell for label encoding.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mnubev0UOcC_"
   },
   "outputs": [],
   "source": [
    "def load_raw_data(df, path):\n",
    "    filenames = df['Filename']\n",
    "    data = [wfdb.rdsamp(os.path.join(path, f))[0] for f in filenames]\n",
    "    return data\n",
    "\n",
    "def load_Diag(df, path):\n",
    "    Disease_Diag = []\n",
    "    ECG_Diag = []\n",
    "    for filename in df['Filename']:\n",
    "        record = wfdb.rdrecord(os.path.join(path, filename))\n",
    "        message = record.comments\n",
    "        Disease_Diag.append(message[1])\n",
    "        ECG_Diag.append(message[2])\n",
    "    return Disease_Diag, ECG_Diag\n",
    "\n",
    "records = DATA_PATH +\"Child_ecg\"\n",
    "X = load_raw_data(Y, records)                 # list of np arrays, one per record\n",
    "Disease_Diag, ECG_Diag = load_Diag(Y, records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3mwZ7I_PBXh"
   },
   "outputs": [],
   "source": "# ===== Data Preprocessing and Label Encoding =====\nprint(\"Number of ECG records:\", len(X))\nprint(\"Sample shape:\", X[0].shape)\n\n# Parse ICD-10 codes for multi-label classification\ndef parse_icd10_codes(icd_string):\n    \"\"\"Extract individual ICD-10 codes from the string format\"\"\"\n    if pd.isna(icd_string) or icd_string == '':\n        return []\n    # Remove quotes and split by semicolon\n    codes = icd_string.replace(\"'\", \"\").split(';')\n    return [c.strip() for c in codes if c.strip()]\n\n# Extract all disease codes\nall_diseases = []\nfor idx, row in Y.iterrows():\n    codes = parse_icd10_codes(row['ICD-10 code'])\n    all_diseases.extend(codes)\n\n# Get unique diseases and create mapping\nunique_diseases = sorted(list(set(all_diseases)))\nprint(f\"\\nTotal unique disease codes: {len(unique_diseases)}\")\nprint(\"Top 20 most common diseases:\")\nfrom collections import Counter\ndisease_counts = Counter(all_diseases)\nfor disease, count in disease_counts.most_common(20):\n    print(f\"  {disease}: {count}\")\n\n# Focus on the main cardiovascular diseases from the paper\nTARGET_DISEASES = [\n    'I40.0',  # Myocarditis (Fulminant/Viral)\n    'I40.9',  # Acute myocarditis\n    'I51.4',  # Myocarditis unspecified\n    'I42.0',  # Dilated cardiomyopathy\n    'I42.2',  # Hypertrophic cardiomyopathy\n    'I42.9',  # Cardiomyopathy unspecified\n    'Q24.8',  # Noncompaction of ventricular myocardium\n    'M30.3',  # Kawasaki disease\n    'Q21.0',  # Ventricular septal defect\n    'Q21.1',  # Atrial septal defect\n    'Q21.2',  # Atrioventricular septal defect\n    'Q21.3',  # Tetralogy of Fallot\n    'Q22.1',  # Stenosis of right ventricular outflow tract\n    'Q25.0',  # Patent ductus arteriosus\n    'Q25.6',  # Pulmonary stenosis\n    'I37.0',  # Pulmonary valve stenosis\n    'I34.0',  # Mitral valve insufficiency\n    'Q24.9',  # Congenital malformation of heart, unspecified\n]\n\ndisease_to_idx = {disease: idx for idx, disease in enumerate(TARGET_DISEASES)}\nidx_to_disease = {idx: disease for disease, idx in disease_to_idx.items()}\n\nprint(f\"\\nTargeting {len(TARGET_DISEASES)} cardiovascular disease labels\")\n\n# Create multi-label binary matrix\ny_multilabel = np.zeros((len(Y), len(TARGET_DISEASES)), dtype=np.float32)\n\nfor i, icd_codes_str in enumerate(Y['ICD-10 code']):\n    codes = parse_icd10_codes(icd_codes_str)\n    for code in codes:\n        if code in disease_to_idx:\n            y_multilabel[i, disease_to_idx[code]] = 1.0\n\n# Show label statistics\nprint(f\"\\nLabel distribution (out of {len(y_multilabel)} samples):\")\nfor idx, disease in idx_to_disease.items():\n    count = int(y_multilabel[:, idx].sum())\n    percentage = 100 * count / len(y_multilabel)\n    print(f\"  {disease}: {count} ({percentage:.2f}%)\")"
  },
  {
   "cell_type": "markdown",
   "source": "## How to Use the Trained Model for Inference\n\nThe trained model can be used to predict cardiovascular diseases from new ECG data. Here's a complete example:\n\n```python\nimport numpy as np\nimport tensorflow as tf\nimport wfdb\nimport json\n\n# 1. Load the saved model\nmodel = tf.keras.models.load_model('path/to/final_model.keras')\n\n# 2. Load the label mapping\nwith open('path/to/label_mapping.json', 'r') as f:\n    label_mapping = json.load(f)\nidx_to_disease = {int(k): v for k, v in label_mapping['idx_to_disease'].items()}\n\n# 3. Load and preprocess a new ECG signal\n# (Use the same preprocess_ecg_signal function defined above)\necg_record = wfdb.rdsamp('path/to/ecg/file')[0]\necg_processed = preprocess_ecg_signal(ecg_record, target_length=5000)\n\n# 4. Make prediction\necg_input = np.expand_dims(ecg_processed, axis=0)  # Add batch dimension\npredictions = model.predict(ecg_input)[0]\n\n# 5. Interpret results (using 0.5 as threshold)\nthreshold = 0.5\npredicted_diseases = []\nfor idx, prob in enumerate(predictions):\n    if prob > threshold:\n        predicted_diseases.append({\n            'disease': idx_to_disease[idx],\n            'probability': float(prob)\n        })\n\n# Sort by probability\npredicted_diseases.sort(key=lambda x: x['probability'], reverse=True)\n\nprint(\"Predicted Diseases:\")\nfor pred in predicted_diseases:\n    print(f\"  {pred['disease']}: {pred['probability']:.4f}\")\n```\n\n### Key Points:\n- The model expects input shape: (batch_size, 5000, num_leads)\n- All signals must be preprocessed using the same method as training\n- Use sigmoid threshold (typically 0.5) for multi-label classification\n- The model outputs probabilities for each of the 18 target diseases",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ===== Save Model and Create Summary Report =====\n\n# Save the final model\nmodel.save(os.path.join(ARTIFACT_DIR, 'final_model.keras'))\nprint(f\"Model saved to: {os.path.join(ARTIFACT_DIR, 'final_model.keras')}\")\n\n# Save label mapping\nimport json\nlabel_mapping = {\n    'disease_to_idx': disease_to_idx,\n    'idx_to_disease': {str(k): v for k, v in idx_to_disease.items()},\n    'target_diseases': TARGET_DISEASES\n}\n\nwith open(os.path.join(ARTIFACT_DIR, 'label_mapping.json'), 'w') as f:\n    json.dump(label_mapping, f, indent=2)\nprint(f\"Label mapping saved to: {os.path.join(ARTIFACT_DIR, 'label_mapping.json')}\")\n\n# Create a summary report\nsummary_report = {\n    'model_architecture': '1D CNN with Residual Connections',\n    'input_shape': input_shape,\n    'num_classes': num_classes,\n    'target_length': TARGET_LENGTH,\n    'sampling_rate': 500,\n    'batch_size': BATCH_SIZE,\n    'epochs_trained': len(history.history['loss']),\n    'dataset_splits': {\n        'train': len(X_train),\n        'val': len(X_val),\n        'test': len(X_test),\n        'total': len(X_processed)\n    },\n    'test_metrics': {\n        'hamming_loss': float(hamming),\n        'subset_accuracy': float(subset_accuracy),\n        'f1_micro': float(f1_micro),\n        'f1_macro': float(f1_macro),\n        'f1_weighted': float(f1_weighted)\n    }\n}\n\nwith open(os.path.join(ARTIFACT_DIR, 'model_summary.json'), 'w') as f:\n    json.dump(summary_report, f, indent=2)\nprint(f\"Summary report saved to: {os.path.join(ARTIFACT_DIR, 'model_summary.json')}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"TRAINING AND EVALUATION COMPLETE!\")\nprint(\"=\"*70)\nprint(f\"\\nAll artifacts saved to: {ARTIFACT_DIR}\")\nprint(\"\\nGenerated files:\")\nprint(\"  - best_model.keras (best model during training)\")\nprint(\"  - final_model.keras (final model)\")\nprint(\"  - training_log.csv (training history)\")\nprint(\"  - training_history.png (training curves)\")\nprint(\"  - roc_curves.png (ROC curves for each disease)\")\nprint(\"  - ecg_prediction_sample_*.png (sample predictions)\")\nprint(\"  - label_mapping.json (disease label mapping)\")\nprint(\"  - model_summary.json (model and performance summary)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ===== Visualize ECG Signals with Predictions =====\n\ndef plot_ecg_with_predictions(idx, X_data, y_true, y_pred_proba, num_leads_to_show=3):\n    \"\"\"Plot ECG signal with true and predicted labels\"\"\"\n    \n    ecg_signal = X_data[idx]\n    true_labels = [TARGET_DISEASES[j] for j in range(len(TARGET_DISEASES)) if y_true[idx, j] == 1]\n    pred_proba = y_pred_proba[idx]\n    pred_labels = [(TARGET_DISEASES[j], pred_proba[j]) \n                  for j in range(len(TARGET_DISEASES)) if pred_proba[j] > 0.5]\n    \n    # Plot first few leads\n    fig, axes = plt.subplots(num_leads_to_show, 1, figsize=(15, 8))\n    \n    lead_names = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n    \n    for i in range(min(num_leads_to_show, ecg_signal.shape[1])):\n        if num_leads_to_show == 1:\n            ax = axes\n        else:\n            ax = axes[i]\n        \n        ax.plot(ecg_signal[:, i], linewidth=0.5)\n        lead_name = lead_names[i] if i < len(lead_names) else f'Lead {i+1}'\n        ax.set_ylabel(f'{lead_name}', fontsize=10)\n        ax.grid(True, alpha=0.3)\n        ax.set_xlim(0, len(ecg_signal))\n        \n        if i == 0:\n            title = f'Sample {idx} - ECG Signal\\n'\n            title += f'True: {\", \".join(true_labels) if true_labels else \"No disease\"}\\n'\n            if pred_labels:\n                title += f'Predicted: {\", \".join([f\"{l} ({p:.2f})\" for l, p in pred_labels])}'\n            else:\n                title += 'Predicted: No disease'\n            ax.set_title(title, fontsize=11, pad=10)\n        \n        if i == num_leads_to_show - 1:\n            ax.set_xlabel('Time (samples @ 500 Hz)', fontsize=10)\n    \n    plt.tight_layout()\n    return fig\n\n# Plot a few interesting examples\ninteresting_indices = [0, 5, 10]  # You can change these\nfor idx in interesting_indices:\n    if idx < len(X_test):\n        fig = plot_ecg_with_predictions(idx, X_test, y_test, y_pred_proba, num_leads_to_show=3)\n        plt.savefig(os.path.join(ARTIFACT_DIR, f'ecg_prediction_sample_{idx}.png'), dpi=150, bbox_inches='tight')\n        plt.show()\n        plt.close()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ===== Example Predictions =====\n\n# Find some interesting examples from the test set\ndef show_predictions(indices, X_data, y_true, y_pred_proba, threshold=0.5):\n    \"\"\"Show predictions for specific samples\"\"\"\n    \n    for i, idx in enumerate(indices):\n        print(f\"\\n{'='*70}\")\n        print(f\"Sample {idx + 1}\")\n        print(f\"{'='*70}\")\n        \n        true_labels = [TARGET_DISEASES[j] for j in range(len(TARGET_DISEASES)) if y_true[idx, j] == 1]\n        pred_proba = y_pred_proba[idx]\n        pred_labels = [(TARGET_DISEASES[j], pred_proba[j]) \n                      for j in range(len(TARGET_DISEASES)) if pred_proba[j] > threshold]\n        \n        print(f\"\\nTrue Labels ({len(true_labels)}):\")\n        if true_labels:\n            for label in true_labels:\n                print(f\"  - {label}\")\n        else:\n            print(\"  - No diseases\")\n        \n        print(f\"\\nPredicted Labels (threshold={threshold}):\")\n        if pred_labels:\n            for label, prob in sorted(pred_labels, key=lambda x: x[1], reverse=True):\n                print(f\"  - {label}: {prob:.4f}\")\n        else:\n            print(\"  - No diseases predicted\")\n        \n        # Show top predictions even if below threshold\n        print(f\"\\nTop 5 Predictions (all):\")\n        top_predictions = sorted(enumerate(pred_proba), key=lambda x: x[1], reverse=True)[:5]\n        for disease_idx, prob in top_predictions:\n            marker = \"âœ“\" if y_true[idx, disease_idx] == 1 else \" \"\n            print(f\"  {marker} {TARGET_DISEASES[disease_idx]}: {prob:.4f}\")\n\n# Show some examples with multiple diseases\nmulti_disease_indices = np.where(y_test.sum(axis=1) >= 2)[0][:3]\nprint(\"Examples with Multiple Diseases:\")\nshow_predictions(multi_disease_indices, X_test, y_test, y_pred_proba)\n\n# Show some examples with single disease\nsingle_disease_indices = np.where(y_test.sum(axis=1) == 1)[0][:3]\nprint(\"\\n\\nExamples with Single Disease:\")\nshow_predictions(single_disease_indices, X_test, y_test, y_pred_proba)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ===== Plot ROC Curves for Each Disease =====\nfrom sklearn.metrics import roc_curve, auc\n\nfig, axes = plt.subplots(3, 6, figsize=(20, 12))\naxes = axes.ravel()\n\nfor idx, disease in enumerate(TARGET_DISEASES):\n    y_true_class = y_test[:, idx]\n    y_score_class = y_pred_proba[:, idx]\n    \n    if y_true_class.sum() > 0 and len(np.unique(y_true_class)) > 1:\n        fpr, tpr, _ = roc_curve(y_true_class, y_score_class)\n        roc_auc = auc(fpr, tpr)\n        \n        axes[idx].plot(fpr, tpr, label=f'AUC = {roc_auc:.3f}')\n        axes[idx].plot([0, 1], [0, 1], 'k--', label='Random')\n        axes[idx].set_xlabel('False Positive Rate')\n        axes[idx].set_ylabel('True Positive Rate')\n        axes[idx].set_title(f'{disease}\\n(n={int(y_true_class.sum())})', fontsize=9)\n        axes[idx].legend(fontsize=8)\n        axes[idx].grid(True, alpha=0.3)\n    else:\n        axes[idx].text(0.5, 0.5, 'No positive\\nsamples', \n                      ha='center', va='center', fontsize=10)\n        axes[idx].set_title(disease, fontsize=9)\n        axes[idx].set_xticks([])\n        axes[idx].set_yticks([])\n\nplt.tight_layout()\nplt.savefig(os.path.join(ARTIFACT_DIR, 'roc_curves.png'), dpi=150)\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ===== Evaluate on Test Set =====\nfrom sklearn.metrics import classification_report, roc_auc_score, f1_score, hamming_loss\n\n# Get predictions\ny_pred_proba = model.predict(X_test, batch_size=BATCH_SIZE)\ny_pred = (y_pred_proba > 0.5).astype(int)\n\n# Overall metrics\nprint(\"=\"*50)\nprint(\"OVERALL TEST SET METRICS\")\nprint(\"=\"*50)\n\n# Hamming Loss (fraction of wrong labels)\nhamming = hamming_loss(y_test, y_pred)\nprint(f\"Hamming Loss: {hamming:.4f}\")\n\n# Subset Accuracy (exact match)\nsubset_accuracy = np.mean(np.all(y_test == y_pred, axis=1))\nprint(f\"Subset Accuracy (Exact Match): {subset_accuracy:.4f}\")\n\n# F1 Scores\nf1_micro = f1_score(y_test, y_pred, average='micro', zero_division=0)\nf1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)\nf1_weighted = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n\nprint(f\"\\nF1 Score (Micro): {f1_micro:.4f}\")\nprint(f\"F1 Score (Macro): {f1_macro:.4f}\")\nprint(f\"F1 Score (Weighted): {f1_weighted:.4f}\")\n\n# ROC-AUC per class\ntry:\n    roc_auc_micro = roc_auc_score(y_test, y_pred_proba, average='micro')\n    roc_auc_macro = roc_auc_score(y_test, y_pred_proba, average='macro')\n    print(f\"\\nROC-AUC (Micro): {roc_auc_micro:.4f}\")\n    print(f\"ROC-AUC (Macro): {roc_auc_macro:.4f}\")\nexcept ValueError as e:\n    print(f\"\\nROC-AUC calculation failed: {e}\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"PER-CLASS METRICS\")\nprint(\"=\"*50)\n\n# Per-class performance\nfrom sklearn.metrics import precision_score, recall_score\n\nfor idx, disease in enumerate(TARGET_DISEASES):\n    y_true_class = y_test[:, idx]\n    y_pred_class = y_pred[:, idx]\n    \n    # Only calculate if there are positive samples\n    if y_true_class.sum() > 0:\n        precision = precision_score(y_true_class, y_pred_class, zero_division=0)\n        recall = recall_score(y_true_class, y_pred_class, zero_division=0)\n        f1 = f1_score(y_true_class, y_pred_class, zero_division=0)\n        \n        try:\n            roc_auc = roc_auc_score(y_true_class, y_pred_proba[:, idx])\n        except ValueError:\n            roc_auc = 0.0\n        \n        support = int(y_true_class.sum())\n        \n        print(f\"\\n{disease}:\")\n        print(f\"  Support: {support}\")\n        print(f\"  Precision: {precision:.4f}\")\n        print(f\"  Recall: {recall:.4f}\")\n        print(f\"  F1-Score: {f1:.4f}\")\n        print(f\"  ROC-AUC: {roc_auc:.4f}\")\n    else:\n        print(f\"\\n{disease}: No samples in test set\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ===== Plot Training History =====\n\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Loss\naxes[0, 0].plot(history.history['loss'], label='Train Loss')\naxes[0, 0].plot(history.history['val_loss'], label='Val Loss')\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Loss')\naxes[0, 0].set_title('Training and Validation Loss')\naxes[0, 0].legend()\naxes[0, 0].grid(True)\n\n# AUC\naxes[0, 1].plot(history.history['auc'], label='Train AUC')\naxes[0, 1].plot(history.history['val_auc'], label='Val AUC')\naxes[0, 1].set_xlabel('Epoch')\naxes[0, 1].set_ylabel('AUC')\naxes[0, 1].set_title('Training and Validation AUC')\naxes[0, 1].legend()\naxes[0, 1].grid(True)\n\n# Precision\naxes[1, 0].plot(history.history['precision'], label='Train Precision')\naxes[1, 0].plot(history.history['val_precision'], label='Val Precision')\naxes[1, 0].set_xlabel('Epoch')\naxes[1, 0].set_ylabel('Precision')\naxes[1, 0].set_title('Training and Validation Precision')\naxes[1, 0].legend()\naxes[1, 0].grid(True)\n\n# Recall\naxes[1, 1].plot(history.history['recall'], label='Train Recall')\naxes[1, 1].plot(history.history['val_recall'], label='Val Recall')\naxes[1, 1].set_xlabel('Epoch')\naxes[1, 1].set_ylabel('Recall')\naxes[1, 1].set_title('Training and Validation Recall')\naxes[1, 1].legend()\naxes[1, 1].grid(True)\n\nplt.tight_layout()\nplt.savefig(os.path.join(ARTIFACT_DIR, 'training_history.png'), dpi=150)\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ===== Train the Model =====\n\n# Define callbacks\ncallbacks = [\n    keras.callbacks.EarlyStopping(\n        monitor='val_auc',\n        patience=15,\n        mode='max',\n        restore_best_weights=True,\n        verbose=1\n    ),\n    keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=5,\n        min_lr=1e-7,\n        verbose=1\n    ),\n    keras.callbacks.ModelCheckpoint(\n        filepath=os.path.join(ARTIFACT_DIR, 'best_model.keras'),\n        monitor='val_auc',\n        mode='max',\n        save_best_only=True,\n        verbose=1\n    ),\n    keras.callbacks.CSVLogger(\n        filename=os.path.join(ARTIFACT_DIR, 'training_log.csv')\n    )\n]\n\n# Train the model\nBATCH_SIZE = 32\nEPOCHS = 100\n\nhistory = model.fit(\n    X_train, y_train,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_data=(X_val, y_val),\n    callbacks=callbacks,\n    verbose=1\n)\n\nprint(\"\\nTraining completed!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ===== Compile Model with Multi-Label Metrics =====\n\n# For multi-label classification, we use:\n# - Binary crossentropy loss (each label is treated independently)\n# - AUC, Precision, Recall metrics\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n    loss='binary_crossentropy',\n    metrics=[\n        keras.metrics.BinaryAccuracy(name='accuracy'),\n        keras.metrics.AUC(name='auc', multi_label=True),\n        keras.metrics.Precision(name='precision'),\n        keras.metrics.Recall(name='recall'),\n    ]\n)\n\n# Calculate class weights for imbalanced data\nclass_weights = {}\nfor idx in range(num_classes):\n    pos_count = y_train[:, idx].sum()\n    neg_count = len(y_train) - pos_count\n    if pos_count > 0:\n        # Weight inversely proportional to frequency\n        class_weights[idx] = neg_count / pos_count\n    else:\n        class_weights[idx] = 1.0\n\nprint(\"Class weights:\")\nfor idx, weight in class_weights.items():\n    print(f\"  {TARGET_DISEASES[idx]}: {weight:.2f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ===== Build 1D CNN Model for Multi-Label Classification =====\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models\n\ndef build_1d_cnn_model(input_shape, num_classes):\n    \"\"\"\n    Build a 1D CNN model for multi-label ECG classification\n    \n    Architecture based on recent ECG classification papers:\n    - Multiple convolutional blocks with residual connections\n    - Batch normalization and dropout for regularization\n    - Global pooling to handle variable-length signals\n    - Sigmoid activation for multi-label output\n    \"\"\"\n    inputs = layers.Input(shape=input_shape, name='ecg_input')\n    \n    # Initial convolution\n    x = layers.Conv1D(64, kernel_size=7, strides=1, padding='same')(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    x = layers.MaxPooling1D(pool_size=2)(x)\n    x = layers.Dropout(0.2)(x)\n    \n    # Convolutional Block 1\n    conv1 = x\n    x = layers.Conv1D(128, kernel_size=5, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    x = layers.Conv1D(128, kernel_size=5, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    \n    # Residual connection\n    conv1 = layers.Conv1D(128, kernel_size=1, padding='same')(conv1)\n    x = layers.Add()([x, conv1])\n    x = layers.Activation('relu')(x)\n    x = layers.MaxPooling1D(pool_size=2)(x)\n    x = layers.Dropout(0.3)(x)\n    \n    # Convolutional Block 2\n    conv2 = x\n    x = layers.Conv1D(256, kernel_size=3, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    x = layers.Conv1D(256, kernel_size=3, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    \n    # Residual connection\n    conv2 = layers.Conv1D(256, kernel_size=1, padding='same')(conv2)\n    x = layers.Add()([x, conv2])\n    x = layers.Activation('relu')(x)\n    x = layers.MaxPooling1D(pool_size=2)(x)\n    x = layers.Dropout(0.3)(x)\n    \n    # Convolutional Block 3\n    conv3 = x\n    x = layers.Conv1D(512, kernel_size=3, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    x = layers.Conv1D(512, kernel_size=3, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    \n    # Residual connection\n    conv3 = layers.Conv1D(512, kernel_size=1, padding='same')(conv3)\n    x = layers.Add()([x, conv3])\n    x = layers.Activation('relu')(x)\n    x = layers.MaxPooling1D(pool_size=2)(x)\n    x = layers.Dropout(0.4)(x)\n    \n    # Global pooling - captures features across all time steps\n    x_avg = layers.GlobalAveragePooling1D()(x)\n    x_max = layers.GlobalMaxPooling1D()(x)\n    x = layers.Concatenate()([x_avg, x_max])\n    \n    # Dense layers\n    x = layers.Dense(512, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.5)(x)\n    \n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.5)(x)\n    \n    # Output layer - sigmoid for multi-label classification\n    outputs = layers.Dense(num_classes, activation='sigmoid', name='disease_output')(x)\n    \n    model = models.Model(inputs=inputs, outputs=outputs, name='1D_CNN_MultiLabel')\n    \n    return model\n\n# Build the model\ninput_shape = (X_train.shape[1], X_train.shape[2])  # (time_steps, num_leads)\nnum_classes = len(TARGET_DISEASES)\n\nmodel = build_1d_cnn_model(input_shape, num_classes)\nmodel.summary()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ===== Train/Test Split =====\nfrom sklearn.model_selection import train_test_split\n\n# Split data - stratify is tricky for multi-label, so we'll use a simple random split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_processed, y_multilabel, \n    test_size=0.2, \n    random_state=42\n)\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train, y_train,\n    test_size=0.2,  # 0.2 of 0.8 = 0.16 of total\n    random_state=42\n)\n\nprint(f\"Training set: {X_train.shape}, {y_train.shape}\")\nprint(f\"Validation set: {X_val.shape}, {y_val.shape}\")\nprint(f\"Test set: {X_test.shape}, {y_test.shape}\")\n\n# Check class distribution\nprint(\"\\nTraining set label distribution:\")\nfor idx, disease in enumerate(TARGET_DISEASES):\n    count = int(y_train[:, idx].sum())\n    print(f\"  {disease}: {count}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ===== Signal Preprocessing =====\nfrom scipy import signal\nfrom scipy.interpolate import interp1d\n\ndef preprocess_ecg_signal(ecg_signal, target_length=5000, target_fs=500):\n    \"\"\"\n    Preprocess ECG signal:\n    - Normalize each lead\n    - Resample/pad to target length\n    - Apply baseline wander removal (high-pass filter)\n    \"\"\"\n    # Normalize each lead independently (z-score normalization)\n    ecg_normalized = (ecg_signal - np.mean(ecg_signal, axis=0)) / (np.std(ecg_signal, axis=0) + 1e-8)\n    \n    # Handle variable length - resample or pad to target_length\n    current_length = ecg_normalized.shape[0]\n    num_leads = ecg_normalized.shape[1]\n    \n    if current_length > target_length:\n        # Resample to target length\n        indices = np.linspace(0, current_length - 1, target_length)\n        ecg_resampled = np.zeros((target_length, num_leads))\n        for lead_idx in range(num_leads):\n            f = interp1d(np.arange(current_length), ecg_normalized[:, lead_idx], kind='linear')\n            ecg_resampled[:, lead_idx] = f(indices)\n    elif current_length < target_length:\n        # Pad with zeros\n        ecg_resampled = np.zeros((target_length, num_leads))\n        ecg_resampled[:current_length, :] = ecg_normalized\n    else:\n        ecg_resampled = ecg_normalized\n    \n    # Apply high-pass filter to remove baseline wander (< 0.5 Hz)\n    sos = signal.butter(4, 0.5, 'high', fs=target_fs, output='sos')\n    ecg_filtered = np.zeros_like(ecg_resampled)\n    for lead_idx in range(num_leads):\n        ecg_filtered[:, lead_idx] = signal.sosfilt(sos, ecg_resampled[:, lead_idx])\n    \n    return ecg_filtered.astype(np.float32)\n\n# Process all ECG signals\nprint(\"Preprocessing ECG signals...\")\nTARGET_LENGTH = 5000  # 10 seconds at 500 Hz\nX_processed = []\n\nfor i, ecg in enumerate(X):\n    if i % 1000 == 0:\n        print(f\"  Processing {i}/{len(X)}...\")\n    X_processed.append(preprocess_ecg_signal(ecg, target_length=TARGET_LENGTH))\n\nX_processed = np.array(X_processed, dtype=np.float32)\nprint(f\"Processed shape: {X_processed.shape}\")  # (num_samples, time_steps, num_leads)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "authorship_tag": "ABX9TyOpPBvY/mYxbOeehOEDF7c/",
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1656e05929c247e9ae3465c15ed6d93c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4504f1b5682d42acaffb528953b4e139": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4a4a813d17b641a39c7485172ee26c2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5caf253c7dd748f085ca441d88fb6835",
      "max": 21,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4504f1b5682d42acaffb528953b4e139",
      "value": 21
     }
    },
    "5caf253c7dd748f085ca441d88fb6835": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8294925094d8402987a6e2a47c89519d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a125fba600f426cbb5b022aacef32cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d5a0972c3b4d4e7782ca2dcb4476a723",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c2c6973c8d2a4f3b846c712ca1efd9d7",
      "value": "â€‡21/21â€‡[00:00&lt;00:00,â€‡1076.87it/s]"
     }
    },
    "c2c6973c8d2a4f3b846c712ca1efd9d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c8c9b8b3177841b0b902629f85afaef5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d604c7d0ed2445249185387394f203cb",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_1656e05929c247e9ae3465c15ed6d93c",
      "value": "Fetchingâ€‡21â€‡files:â€‡100%"
     }
    },
    "cb9c3bceac6a4f1a82c45307fe5608d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c8c9b8b3177841b0b902629f85afaef5",
       "IPY_MODEL_4a4a813d17b641a39c7485172ee26c2d",
       "IPY_MODEL_8a125fba600f426cbb5b022aacef32cf"
      ],
      "layout": "IPY_MODEL_8294925094d8402987a6e2a47c89519d"
     }
    },
    "d5a0972c3b4d4e7782ca2dcb4476a723": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d604c7d0ed2445249185387394f203cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}