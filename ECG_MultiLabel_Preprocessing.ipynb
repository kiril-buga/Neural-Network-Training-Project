{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECG Multi-Label Preprocessing (Self-Contained)\n",
    "Fully independent notebook: loads raw data, processes, creates 5-class one-hot labels, uploads to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wfdb neurokit2 scikit-learn scipy matplotlib pandas numpy huggingface-hub -q\n",
    "!apt-get update && apt-get install -y p7zip-full\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wfdb\n",
    "from scipy.signal import butter, filtfilt, welch, resample\n",
    "import neurokit2 as nk\n",
    "from datetime import datetime\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to True to download from Huggingface else use Google Drive\n",
    "USE_HF = True\n",
    "\n",
    "if USE_HF:\n",
    "  from huggingface_hub import snapshot_download\n",
    "  local_dir = snapshot_download(\n",
    "      repo_id=\"kiril-buga/ECG-database\",\n",
    "      repo_type=\"dataset\",\n",
    "      local_dir=\"/content/ECG-database/\" # Specify the desired download directory\n",
    "  )\n",
    "  print(\"Downloaded to:\", local_dir)\n",
    "\n",
    "    DATA_PATH = f\"{local_dir}/data/\"\n",
    "    ARTIFACT_DIR = f\"{local_dir}/artifacts/\"\n",
    "\n",
    "else:\n",
    "    # Detect environment and mount drive if Colab\n",
    "    IN_COLAB = False\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        IN_COLAB = True\n",
    "        drive.mount('/content/drive/')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Set paths\n",
    "    if IN_COLAB:\n",
    "        DATA_PATH = \"/content/drive/MyDrive/DeepLearningECG/data/\"\n",
    "        ARTIFACT_DIR = \"/content/drive/MyDrive/DeepLearningECG/artifacts/\"\n",
    "    else:\n",
    "        DATA_PATH = \"../DeepLearningECG/data/\"\n",
    "        ARTIFACT_DIR = \"../DeepLearningECG/artifacts/\"\n",
    "\n",
    "ECG_DIR = os.path.join(DATA_PATH, \"Child_ecg/\")\n",
    "OUT_DIR = os.path.join(ARTIFACT_DIR, \"multilabel_v2\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Colab: {IN_COLAB}\")\n",
    "print(f\"DATA: {DATA_PATH}\")\n",
    "print(f\"OUTPUT: {OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd DATA_PATH && 7z x Child_ecg.zip\n",
    "print(\"✓ Extraction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = os.path.join(DATA_PATH, 'AttributesDictionary.csv')\n",
    "\n",
    "if os.path.exists(csv_path):\n",
    "    df_attr = pd.read_csv(csv_path)\n",
    "    print(f\"Loaded CSV: {df_attr.shape}\")\n",
    "else:\n",
    "    from huggingface_hub import hf_hub_download\n",
    "    print(\"Downloading CSV from Hugging Face...\")\n",
    "    csv_file = hf_hub_download(\n",
    "        repo_id=\"kiril-buga/ECG-database\",\n",
    "        filename=\"AttributesDictionary.csv\",\n",
    "        repo_type=\"dataset\"\n",
    "    )\n",
    "    df_attr = pd.read_csv(csv_file)\n",
    "    print(f\"Loaded CSV: {df_attr.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_bandpass(x, fs, lowcut=0.5, highcut=40.0):\n",
    "    if x.ndim == 1:\n",
    "        x = x[:, None]\n",
    "    nyq = 0.5 * fs\n",
    "    b, a = butter(4, [lowcut/nyq, highcut/nyq], btype=\"band\")\n",
    "    return np.column_stack([filtfilt(b, a, x[:, i]) for i in range(x.shape[1])])\n",
    "\n",
    "def band_power(f, Pxx, fmin, fmax):\n",
    "    mask = (f >= fmin) & (f <= fmax)\n",
    "    return np.trapz(Pxx[mask], f[mask]) if np.any(mask) else 0.0\n",
    "\n",
    "HAS_NK = True\n",
    "try:\n",
    "    import neurokit2\n",
    "except:\n",
    "    HAS_NK = False\n",
    "\n",
    "def compute_qc(sig, meta, pSQI_mean, bSQI_mean):\n",
    "    \"\"\"Compute QC metrics.\"\"\"\n",
    "    qc = {\"pSQI_mean\": pSQI_mean, \"bSQI_mean\": bSQI_mean}\n",
    "    \n",
    "    fs = meta.get(\"fs\", None)\n",
    "    if fs is None:\n",
    "        fs = getattr(meta, \"fs\", None)\n",
    "    if fs is None:\n",
    "        raise ValueError(\"Missing fs\")\n",
    "    \n",
    "    if sig.ndim == 1:\n",
    "        sig = sig[:, None]\n",
    "    \n",
    "    n_samples, n_leads = sig.shape\n",
    "    qc[\"n_samples\"] = int(n_samples)\n",
    "    qc[\"n_leads\"] = int(n_leads)\n",
    "    qc[\"duration_sec\"] = n_samples / fs\n",
    "    \n",
    "    lead = sig[:, 0]\n",
    "    n_nans = np.isnan(lead).sum()\n",
    "    qc[\"nan_fraction\"] = float(n_nans / len(lead))\n",
    "    \n",
    "    lead_clean = lead.copy()\n",
    "    if n_nans > 0:\n",
    "        not_nan = ~np.isnan(lead_clean)\n",
    "        if not np.any(not_nan):\n",
    "            return {**qc, \"qc_pass\": False, \"fail_reason\": \"all_nan\"}\n",
    "        lead_clean[~not_nan] = np.interp(np.flatnonzero(~not_nan), \n",
    "                                          np.flatnonzero(not_nan), lead_clean[not_nan])\n",
    "    \n",
    "    amp = lead_clean\n",
    "    qc[\"amp_mean\"] = float(np.mean(amp))\n",
    "    qc[\"amp_std\"] = float(np.std(amp))\n",
    "    q1, q99 = np.percentile(amp, [1, 99])\n",
    "    qc[\"amp_robust_range\"] = float(q99 - q1)\n",
    "    \n",
    "    f, Pxx = welch(amp, fs=fs, nperseg=min(4096, len(amp)))\n",
    "    qc[\"baseline_wander_ratio\"] = band_power(f, Pxx, 0.0, 0.5) / (band_power(f, Pxx, 0.5, 40.0) + 1e-8)\n",
    "    qc[\"powerline_ratio\"] = band_power(f, Pxx, 48.0, 52.0) / (band_power(f, Pxx, 40.0, 60.0) + 1e-8)\n",
    "    \n",
    "    reasons = []\n",
    "    if qc[\"duration_sec\"] < 8.0: reasons.append(\"too_short\")\n",
    "    if qc[\"nan_fraction\"] > 0.01: reasons.append(\"too_many_nans\")\n",
    "    if not (0.05 < qc[\"amp_robust_range\"] < 10.0): reasons.append(\"amp_out_of_range\")\n",
    "    if qc[\"baseline_wander_ratio\"] > 0.5: reasons.append(\"baseline_wander\")\n",
    "    if qc[\"powerline_ratio\"] > 0.5: reasons.append(\"powerline_noise\")\n",
    "    if pSQI_mean < 0.2: reasons.append(\"low_pSQI\")\n",
    "    if bSQI_mean < 0.8: reasons.append(\"low_bSQI\")\n",
    "    \n",
    "    qc[\"qc_pass\"] = len(reasons) == 0\n",
    "    qc[\"fail_reason\"] = \";\".join(reasons) if reasons else \"\"\n",
    "    \n",
    "    return qc\n",
    "\n",
    "print(\"✓ Processing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing & Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_record(sig, meta, target_fs=500.0):\n",
    "    fs = meta.get(\"fs\", None) or getattr(meta, \"fs\", None)\n",
    "    if sig.ndim == 1:\n",
    "        sig = sig[:, None]\n",
    "    \n",
    "    sig_bp = apply_bandpass(sig, fs=fs)\n",
    "    if fs == target_fs:\n",
    "        return sig_bp, fs\n",
    "    \n",
    "    n_samples = sig_bp.shape[0]\n",
    "    n_new = int(round(n_samples / fs * target_fs))\n",
    "    sig_res = np.column_stack([resample(sig_bp[:, i], n_new) for i in range(sig_bp.shape[1])])\n",
    "    return sig_res, target_fs\n",
    "\n",
    "def window_record(sig, fs, window_sec=10.0, step_sec=5.0):\n",
    "    if sig.ndim == 1:\n",
    "        sig = sig[:, None]\n",
    "    \n",
    "    n_samples = sig.shape[0]\n",
    "    win_len = int(window_sec * fs)\n",
    "    step_len = int(step_sec * fs)\n",
    "    \n",
    "    windows = []\n",
    "    start = 0\n",
    "    while start + win_len <= n_samples:\n",
    "        segment = sig[start:start + win_len, :]\n",
    "        if np.isnan(segment).mean() > 0.05:\n",
    "            start += step_len\n",
    "            continue\n",
    "        \n",
    "        seg_norm = segment.copy()\n",
    "        for ch in range(seg_norm.shape[1]):\n",
    "            x = seg_norm[:, ch]\n",
    "            m, s = np.nanmean(x), np.nanstd(x)\n",
    "            seg_norm[:, ch] = (x - m) / (s if s > 1e-6 else 1.0)\n",
    "        \n",
    "        windows.append(seg_norm.astype(np.float32))\n",
    "        start += step_len\n",
    "    \n",
    "    return windows\n",
    "\n",
    "print(\"✓ Preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICD Code Parsing & Disease Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICD_TO_DISEASE = {\n",
    "    'I40.0': 'Myocarditis', 'I40.9': 'Myocarditis', 'I41.4': 'Myocarditis',\n",
    "    'I42.0': 'Cardiomyopathy', 'I42.2': 'Cardiomyopathy', 'I42.9': 'Cardiomyopathy', 'Q28.4': 'Cardiomyopathy',\n",
    "    'M30.3': 'Kawasaki',\n",
    "    'Q21.1': 'CHD', 'Q21.2': 'CHD', 'Q21.3': 'CHD', 'Q22.1': 'CHD', 'Q25.0': 'CHD', 'Q25.6': 'CHD', 'I27.9': 'CHD',\n",
    "}\n",
    "\n",
    "DISEASE_CLASSES = ['Myocarditis', 'Cardiomyopathy', 'Kawasaki', 'CHD', 'Healthy']\n",
    "CLASS_IDX = {c: i for i, c in enumerate(DISEASE_CLASSES)}\n",
    "\n",
    "def parse_icd(s):\n",
    "    if pd.isna(s):\n",
    "        return []\n",
    "    return [p.strip().replace(\"'\", \"\") for p in str(s).split(\";\") if p.strip()]\n",
    "\n",
    "def clean_icd(code):\n",
    "    if pd.isna(code):\n",
    "        return None\n",
    "    code_str = str(code).strip()\n",
    "    if ')' in code_str:\n",
    "        code_str = code_str.split(')')[-1].strip()\n",
    "    return code_str or None\n",
    "\n",
    "def parse_sqi(s):\n",
    "    if pd.isna(s):\n",
    "        return {}\n",
    "    out = {}\n",
    "    for item in str(s).split(\";\"):\n",
    "        if \":\" in item:\n",
    "            k, v = item.split(\":\")\n",
    "            try:\n",
    "                out[k.replace(\"'\", \"\").strip()] = float(v)\n",
    "            except:\n",
    "                pass\n",
    "    return out\n",
    "\n",
    "# Parse ICD codes\n",
    "df_attr[\"ICD_list\"] = df_attr[\"ICD-10 code\"].apply(parse_icd)\n",
    "df_attr[\"ICD_primary\"] = df_attr[\"ICD_list\"].apply(lambda x: x[0] if x else None)\n",
    "df_attr[\"ICD_primary_clean\"] = df_attr[\"ICD_primary\"].apply(clean_icd)\n",
    "df_attr[\"disease\"] = df_attr[\"ICD_primary_clean\"].apply(lambda x: ICD_TO_DISEASE.get(x, 'Healthy') if x else 'Healthy')\n",
    "\n",
    "# Parse SQI\n",
    "for col in [\"pSQI\", \"basSQI\", \"bSQI\"]:\n",
    "    df_attr[f\"{col}_dict\"] = df_attr[col].apply(parse_sqi)\n",
    "    df_attr[f\"{col}_mean\"] = df_attr[f\"{col}_dict\"].apply(lambda d: np.mean(list(d.values())) if d else np.nan)\n",
    "\n",
    "print(\"Disease distribution:\")\n",
    "print(df_attr['disease'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all(df, ecg_dir, max_records=None):\n",
    "    qc_list = []\n",
    "    all_windows = []\n",
    "    all_diseases = []\n",
    "    \n",
    "    iterator = df.iloc[:max_records].iterrows() if max_records else df.iterrows()\n",
    "    \n",
    "    for idx, row in iterator:\n",
    "        fname = row[\"Filename\"]\n",
    "        disease = row[\"disease\"]\n",
    "        path = os.path.join(ecg_dir, fname)\n",
    "        \n",
    "        try:\n",
    "            sig, meta = wfdb.rdsamp(path)\n",
    "        except Exception as e:\n",
    "            qc_list.append({\"Filename\": fname, \"disease\": disease, \"qc_pass\": False, \"fail_reason\": str(e)})\n",
    "            continue\n",
    "        \n",
    "        meta_dict = meta if isinstance(meta, dict) else meta.__dict__\n",
    "        \n",
    "        qc = compute_qc(np.asarray(sig), meta_dict, float(row[\"pSQI_mean\"]), float(row[\"bSQI_mean\"]))\n",
    "        qc[\"Filename\"] = fname\n",
    "        qc[\"disease\"] = disease\n",
    "        \n",
    "        if not qc[\"qc_pass\"]:\n",
    "            qc_list.append(qc)\n",
    "            continue\n",
    "        \n",
    "        sig_proc, fs = preprocess_record(np.asarray(sig), meta_dict)\n",
    "        windows = window_record(sig_proc, fs)\n",
    "        \n",
    "        qc[\"n_windows\"] = len(windows)\n",
    "        qc_list.append(qc)\n",
    "        all_windows.extend(windows)\n",
    "        all_diseases.extend([disease] * len(windows))\n",
    "    \n",
    "    if not all_windows:\n",
    "        raise RuntimeError(\"No windows created\")\n",
    "    \n",
    "    X = np.stack(all_windows)\n",
    "    y = np.zeros((len(all_windows), len(DISEASE_CLASSES)), dtype=np.int32)\n",
    "    for i, disease in enumerate(all_diseases):\n",
    "        y[i, CLASS_IDX[disease]] = 1\n",
    "    \n",
    "    return X, y, np.array(all_diseases), pd.DataFrame(qc_list)\n",
    "\n",
    "print(\"Processing ECG records...\")\n",
    "X, y, diseases, df_qc = process_all(df_attr, ECG_DIR, max_records=None)\n",
    "print(f\"\\n✓ Complete! X: {X.shape}, y: {y.shape}\")\n",
    "print(f\"\\nDisease distribution:\")\n",
    "for i, cls in enumerate(DISEASE_CLASSES):\n",
    "    count = y[:, i].sum()\n",
    "    print(f\"  {cls}: {count} ({100*count/len(y):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to local\n",
    "np.save(os.path.join(OUT_DIR, \"X_windows.npy\"), X)\n",
    "np.save(os.path.join(OUT_DIR, \"y_labels_onehot.npy\"), y)\n",
    "np.save(os.path.join(OUT_DIR, \"window_diseases.npy\"), diseases)\n",
    "\n",
    "with open(os.path.join(OUT_DIR, \"disease_classes.json\"), \"w\") as f:\n",
    "    json.dump({\"classes\": DISEASE_CLASSES, \"class_idx\": CLASS_IDX, \"icd_map\": ICD_TO_DISEASE}, f, indent=2)\n",
    "\n",
    "df_qc[['Filename', 'disease', 'qc_pass', 'n_windows']].to_csv(os.path.join(OUT_DIR, \"qc_summary.csv\"), index=False)\n",
    "\n",
    "print(f\"✓ Saved to {OUT_DIR}\")\n",
    "print(f\"  - X_windows.npy: {X.shape}\")\n",
    "print(f\"  - y_labels_onehot.npy: {y.shape}\")\n",
    "print(f\"  - window_diseases.npy: {diseases.shape}\")\n",
    "print(f\"  - disease_classes.json\")\n",
    "print(f\"  - qc_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Hugging Face (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPLOAD_TO_HF = False  # Set to True to upload\n",
    "\n",
    "if UPLOAD_TO_HF:\n",
    "    from huggingface_hub import HfApi, login\n",
    "    \n",
    "    print(\"Logging into Hugging Face...\")\n",
    "    login()\n",
    "    \n",
    "    api = HfApi()\n",
    "    print(\"Uploading to HF...\")\n",
    "    api.upload_folder(\n",
    "        folder_path=OUT_DIR,\n",
    "        repo_id=\"kiril-buga/ECG-database\",\n",
    "        repo_type=\"dataset\",\n",
    "        path_in_repo=\"multilabel_v2\",\n",
    "        commit_message=\"Multi-label preprocessed data\"\n",
    "    )\n",
    "    print(\"✓ Uploaded to HF\")\n",
    "else:\n",
    "    print(\"To upload: set UPLOAD_TO_HF=True and have HF write token\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
