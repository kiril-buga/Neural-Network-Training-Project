{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced 1D CNN for 19-Class Multi-Label ECG Classification (v03)\n",
    "\n",
    "**Key Features:**\n",
    "- 19-class multi-label classification (18 cardiac diseases + Healthy)\n",
    "- **Full-length variable recordings** (5-120 seconds, no windowing)\n",
    "- Enhanced architecture: Squeeze-Excitation blocks + Temporal Attention\n",
    "- Focal loss for class imbalance\n",
    "- **Outputs show disease names only** (not ICD codes)\n",
    "\n",
    "**Architecture:** 64→128→256→512 filters + SE + Attention + Adaptive Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install wfdb neurokit2 h5py tensorflow scikit-learn matplotlib seaborn pandas numpy scipy -q\n",
    "\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import wfdb\n",
    "from scipy.signal import butter, filtfilt, resample\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, hamming_loss, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Num GPUs Available: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "NOTEBOOK_VERSION = \"v03\"\n",
    "DATA_DIR = \"../.data/\"\n",
    "ECG_DIR = os.path.join(DATA_DIR, \"data/Child_ecg/\")\n",
    "CSV_PATH = os.path.join(DATA_DIR, \"data/AttributesDictionary.csv\")\n",
    "OUTPUT_DIR = os.path.join(DATA_DIR, \"artifacts/multilabel_v3_full_length/\")\n",
    "HDF5_FILE = os.path.join(OUTPUT_DIR, \"ecg_full_length_19classes.h5\")\n",
    "MODEL_NAME = \"Enhanced_1D_CNN_19Classes_v3\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 16  # Smaller for variable lengths\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-3\n",
    "TARGET_FS = 500  # Target sampling frequency\n",
    "TARGET_CHANNELS = 12\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Data directory: {DATA_DIR}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 1.5 Extract Split Archive (if needed)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Extract split archives if needed (local environment)\nimport os\nimport zipfile\n\n# Check if Child_ecg directory exists and has files\ndef count_files_recursive(directory):\n    \"\"\"Count all files recursively in a directory\"\"\"\n    if not os.path.exists(directory):\n        return 0\n    count = 0\n    for root, dirs, files in os.walk(directory):\n        count += len(files)\n    return count\n\necg_file_count = count_files_recursive(ECG_DIR)\n\nprint(f\"Checking ECG data extraction...\")\nprint(f\"  ECG directory: {ECG_DIR}\")\nprint(f\"  Files found: {ecg_file_count}\")\n\n# If directory doesn't exist or has very few files, extract the archives\nif ecg_file_count < 1000:  # Expect tens of thousands of files\n    print(f\"\\n⚠️  Insufficient ECG files detected. Checking for archives to extract...\")\n    \n    # Look for split archive files\n    archive_dir = os.path.join(DATA_DIR, \"data\")\n    zip_file = os.path.join(archive_dir, \"Child_ecg.zip\")\n    z01_file = os.path.join(archive_dir, \"Child_ecg.z01\")\n    \n    if os.path.exists(zip_file):\n        print(f\"\\n✓ Found archive files:\")\n        print(f\"  - {zip_file}\")\n        if os.path.exists(z01_file):\n            print(f\"  - {z01_file}\")\n        \n        print(f\"\\nExtracting archives...\")\n        print(f\"This may take several minutes...\")\n        \n        try:\n            # For split archives (.z01 + .zip), we need to use 7zip or similar\n            if os.path.exists(z01_file):\n                # Try using 7zip if available\n                try:\n                    import subprocess\n                    # Try 7z command on Windows (usually installed with 7-Zip)\n                    result = subprocess.run(\n                        ['7z', 'x', zip_file, f'-o{archive_dir}', '-y'],\n                        capture_output=True,\n                        text=True\n                    )\n                    if result.returncode == 0:\n                        print(f\"✓ Successfully extracted using 7zip\")\n                    else:\n                        raise Exception(\"7zip extraction failed\")\n                except:\n                    # Fallback: Try PowerShell Expand-Archive (may not work with split archives)\n                    print(\"7zip not available, trying PowerShell...\")\n                    result = subprocess.run(\n                        ['powershell', '-Command', \n                         f'Expand-Archive -Path \"{zip_file}\" -DestinationPath \"{archive_dir}\" -Force'],\n                        capture_output=True,\n                        text=True\n                    )\n                    if result.returncode == 0:\n                        print(f\"✓ Extraction attempted with PowerShell\")\n                    else:\n                        print(f\"PowerShell extraction failed, trying Python zipfile...\")\n                        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n                            zip_ref.extractall(archive_dir)\n                        print(f\"✓ Extraction attempted with zipfile\")\n            else:\n                # Single zip file, use Python's zipfile\n                with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n                    zip_ref.extractall(archive_dir)\n                print(f\"✓ Successfully extracted zip file\")\n            \n            # Verify extraction\n            new_file_count = count_files_recursive(ECG_DIR)\n            print(f\"\\n✓ Extraction complete!\")\n            print(f\"  Files extracted: {new_file_count}\")\n            \n            if new_file_count < 1000:\n                print(f\"\\n⚠️  Warning: Expected more files. Archive may not have extracted completely.\")\n                print(f\"  Please manually extract {zip_file} using 7-Zip if needed.\")\n        \n        except Exception as e:\n            print(f\"\\n❌ Error during extraction: {e}\")\n            print(f\"\\nPlease manually extract the following file:\")\n            print(f\"  {zip_file}\")\n            print(f\"To directory: {archive_dir}\")\n            print(f\"\\nYou can use 7-Zip (https://www.7-zip.org/) to extract split archives.\")\n            raise\n    else:\n        print(f\"\\n❌ Archive files not found!\")\n        print(f\"  Expected: {zip_file}\")\n        print(f\"\\nPlease ensure the archive files are present in {archive_dir}\")\n        raise FileNotFoundError(f\"Archive not found: {zip_file}\")\nelse:\n    print(f\"✓ ECG data already extracted ({ecg_file_count} files found)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Disease Mapping & Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICD codes to Disease Names mapping\n",
    "ICD_TO_DISEASE_NAME = {\n",
    "    'I40.0': 'Fulminant/Viral Myocarditis',\n",
    "    'I40.9': 'Acute Myocarditis',\n",
    "    'I51.4': 'Myocarditis Unspecified',\n",
    "    'I42.0': 'Dilated Cardiomyopathy',\n",
    "    'I42.2': 'Hypertrophic Cardiomyopathy',\n",
    "    'I42.9': 'Cardiomyopathy Unspecified',\n",
    "    'Q24.8': 'Noncompaction Ventricular Myocardium',\n",
    "    'M30.3': 'Kawasaki Disease',\n",
    "    'Q21.0': 'Ventricular Septal Defect',\n",
    "    'Q21.1': 'Atrial Septal Defect',\n",
    "    'Q21.2': 'Atrioventricular Septal Defect',\n",
    "    'Q21.3': 'Tetralogy of Fallot',\n",
    "    'Q22.1': 'Pulmonary Valve Stenosis',\n",
    "    'Q25.0': 'Patent Ductus Arteriosus',\n",
    "    'Q25.6': 'Pulmonary Artery Stenosis',\n",
    "    'I37.0': 'Pulmonary Valve Regurgitation',\n",
    "    'I34.0': 'Mitral Valve Insufficiency',\n",
    "    'Q24.9': 'Congenital Heart Malformation',\n",
    "    'Healthy': 'Healthy'  # Changed from \"No Disease (Healthy)\" to just \"Healthy\"\n",
    "}\n",
    "\n",
    "# Disease names list (for model outputs - NO ICD codes in outputs)\n",
    "DISEASE_NAMES = list(ICD_TO_DISEASE_NAME.values())\n",
    "ICD_CODES = list(ICD_TO_DISEASE_NAME.keys())\n",
    "NUM_CLASSES = len(DISEASE_NAMES)\n",
    "\n",
    "# Disease grouping for interpretability\n",
    "DISEASE_GROUPS = {\n",
    "    'Myocarditis': ['I40.0', 'I40.9', 'I51.4'],\n",
    "    'Cardiomyopathy': ['I42.0', 'I42.2', 'I42.9', 'Q24.8'],\n",
    "    'Kawasaki': ['M30.3'],\n",
    "    'CHD': ['Q21.0', 'Q21.1', 'Q21.2', 'Q21.3', 'Q22.1', 'Q25.0', 'Q25.6', 'I37.0', 'I34.0', 'Q24.9']\n",
    "}\n",
    "\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"\\nDisease Names (outputs will show these names ONLY):\")\n",
    "for i, name in enumerate(DISEASE_NAMES, 1):\n",
    "    print(f\"  {i:2d}. {name}\")\n",
    "\n",
    "print(f\"\\nDisease Groups:\")\n",
    "for group, codes in DISEASE_GROUPS.items():\n",
    "    names = [ICD_TO_DISEASE_NAME[code] for code in codes]\n",
    "    print(f\"  {group}: {len(codes)} diseases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_bandpass(x, fs, lowcut=0.5, highcut=40.0):\n",
    "    \"\"\"Apply bandpass filter to entire ECG signal.\"\"\"\n",
    "    if x.ndim == 1:\n",
    "        x = x[:, None]\n",
    "    nyq = 0.5 * fs\n",
    "    b, a = butter(4, [lowcut/nyq, highcut/nyq], btype=\"band\")\n",
    "    return np.column_stack([filtfilt(b, a, x[:, i]) for i in range(x.shape[1])])\n",
    "\n",
    "\n",
    "def process_full_recording(sig, meta, target_fs=500.0, target_channels=12):\n",
    "    \"\"\"\n",
    "    Process full ECG recording without windowing.\n",
    "    Handles variable lengths (5-120 seconds).\n",
    "    \n",
    "    Returns:\n",
    "        sig_processed: (n_samples, 12) - variable length\n",
    "        length: actual sample count\n",
    "    \"\"\"\n",
    "    # Get sampling frequency\n",
    "    fs = meta.get(\"fs\", None)\n",
    "    if fs is None:\n",
    "        fs = getattr(meta, \"fs\", None)\n",
    "    if fs is None:\n",
    "        raise ValueError(\"Missing sampling frequency\")\n",
    "    \n",
    "    if sig.ndim == 1:\n",
    "        sig = sig[:, None]\n",
    "    \n",
    "    # 1. Bandpass filter entire signal (0.5-40 Hz)\n",
    "    sig_bp = apply_bandpass(sig, fs=fs)\n",
    "    \n",
    "    # 2. Resample to target_fs if needed\n",
    "    if fs != target_fs:\n",
    "        n_samples = sig_bp.shape[0]\n",
    "        n_new = int(round(n_samples / fs * target_fs))\n",
    "        sig_res = np.column_stack([resample(sig_bp[:, i], n_new) \n",
    "                                   for i in range(sig_bp.shape[1])])\n",
    "    else:\n",
    "        sig_res = sig_bp\n",
    "    \n",
    "    # 3. Standardize to 12 leads (pad with zeros if fewer)\n",
    "    if sig_res.shape[1] < target_channels:\n",
    "        padded = np.zeros((sig_res.shape[0], target_channels), dtype=np.float32)\n",
    "        padded[:, :sig_res.shape[1]] = sig_res\n",
    "        sig_res = padded\n",
    "    elif sig_res.shape[1] > target_channels:\n",
    "        sig_res = sig_res[:, :target_channels]\n",
    "    \n",
    "    # 4. Z-score normalization per lead\n",
    "    sig_norm = sig_res.copy()\n",
    "    for ch in range(target_channels):\n",
    "        x = sig_norm[:, ch]\n",
    "        m, s = np.nanmean(x), np.nanstd(x)\n",
    "        sig_norm[:, ch] = (x - m) / (s if s > 1e-6 else 1.0)\n",
    "    \n",
    "    actual_length = sig_norm.shape[0]\n",
    "    \n",
    "    return sig_norm.astype(np.float16), actual_length\n",
    "\n",
    "\n",
    "def parse_icd_codes(s):\n",
    "    \"\"\"Parse semicolon-separated ICD codes from CSV.\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return []\n",
    "    return [p.strip().replace(\"'\", \"\").replace(\")\", \"\").split(\")\")[-1] \n",
    "            for p in str(s).split(\";\") if p.strip()]\n",
    "\n",
    "\n",
    "print(\"✓ Preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV metadata\n",
    "print(\"Loading metadata...\")\n",
    "df_attr = pd.read_csv(CSV_PATH)\n",
    "print(f\"Loaded {len(df_attr)} ECG records\")\n",
    "\n",
    "# Parse ICD codes\n",
    "df_attr[\"ICD_list\"] = df_attr[\"ICD-10 code\"].apply(parse_icd_codes)\n",
    "print(f\"Sample ICD codes: {df_attr['ICD_list'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if preprocessed HDF5 exists\n",
    "if os.path.exists(HDF5_FILE):\n",
    "    print(f\"✓ Found existing HDF5 file: {HDF5_FILE}\")\n",
    "    print(\"Loading preprocessed data...\")\n",
    "    \n",
    "    with h5py.File(HDF5_FILE, 'r') as h5f:\n",
    "        print(f\"\\nHDF5 Contents:\")\n",
    "        print(f\"  Bins: {list(h5f.keys())}\")\n",
    "        \n",
    "        total_samples = 0\n",
    "        for bin_name in h5f.keys():\n",
    "            if bin_name.startswith('bin_'):\n",
    "                grp = h5f[bin_name]\n",
    "                bin_samples = grp['X'].shape[0]\n",
    "                total_samples += bin_samples\n",
    "                print(f\"  {bin_name}: {bin_samples} samples\")\n",
    "        \n",
    "        print(f\"\\n  Total samples: {total_samples}\")\n",
    "        print(f\"  Number of classes: {h5f.attrs['num_classes']}\")\n",
    "        print(f\"  Sampling rate: {h5f.attrs['sampling_rate']} Hz\")\n",
    "\nelse:\n",
    "    print(f\"HDF5 file not found. Creating preprocessed dataset...\")\n",
    "    print(f\"This may take 30-60 minutes...\\n\")\n",
    "    \n",
    "    # Process all recordings\n",
    "    recordings = []\n",
    "    labels = []\n",
    "    lengths = []\n",
    "    filenames = []\n",
    "    \n",
    "    print(\"Processing ECG recordings...\")\n",
    "    for idx, row in df_attr.iterrows():\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print(f\"  Processed {idx + 1}/{len(df_attr)} records...\")\n",
    "        \n",
    "        fname = row[\"Filename\"]\n",
    "        path = os.path.join(ECG_DIR, fname)\n",
    "        \n",
    "        try:\n",
    "            sig, meta = wfdb.rdsamp(path)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        meta_dict = meta if isinstance(meta, dict) else meta.__dict__\n",
    "        sig = np.asarray(sig)\n",
    "        \n",
    "        # Process full recording\n",
    "        try:\n",
    "            sig_proc, length = process_full_recording(sig, meta_dict)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        # Create multi-label vector\n",
    "        label_vec = np.zeros(NUM_CLASSES, dtype=np.int32)\n",
    "        icd_list = row[\"ICD_list\"]\n",
    "        \n",
    "        if not icd_list or all(pd.isna(icd_list)):\n",
    "            # Healthy case\n",
    "            label_vec[ICD_CODES.index('Healthy')] = 1\n",
    "        else:\n",
    "            # Disease case\n",
    "            found_disease = False\n",
    "            for icd in icd_list:\n",
    "                if icd in ICD_CODES:\n",
    "                    label_vec[ICD_CODES.index(icd)] = 1\n",
    "                    found_disease = True\n",
    "            \n",
    "            # If no matching disease found, mark as healthy\n",
    "            if not found_disease:\n",
    "                label_vec[ICD_CODES.index('Healthy')] = 1\n",
    "        \n",
    "        recordings.append(sig_proc)\n",
    "        labels.append(label_vec)\n",
    "        lengths.append(length)\n",
    "        filenames.append(fname)\n",
    "    \n",
    "    print(f\"\\n✓ Processed {len(recordings)} valid recordings\")\n",
    "    \n",
    "    # Save to HDF5 with length binning\n",
    "    print(\"\\nSaving to HDF5 with length binning...\")\n",
    "    \n",
    "    labels_array = np.array(labels)\n",
    "    lengths_array = np.array(lengths)\n",
    "    \n",
    "    # Define bins: <10s, 10-30s, 30-60s, 60-120s (in samples @ 500Hz)\n",
    "    length_bins = [(0, 5000), (5000, 15000), (15000, 30000), (30000, 60000)]\n",
    "    \n",
    "    with h5py.File(HDF5_FILE, 'w') as h5f:\n",
    "        for bin_idx, (min_len, max_len) in enumerate(length_bins):\n",
    "            mask = (lengths_array >= min_len) & (lengths_array < max_len)\n",
    "            if not np.any(mask):\n",
    "                continue\n",
    "            \n",
    "            bin_recordings = [recordings[i] for i in np.where(mask)[0]]\n",
    "            bin_labels = labels_array[mask]\n",
    "            bin_lengths = lengths_array[mask]\n",
    "            bin_filenames = [filenames[i] for i in np.where(mask)[0]]\n",
    "            \n",
    "            # Pad to max length in this bin\n",
    "            max_bin_len = bin_lengths.max()\n",
    "            padded_sigs = np.zeros((len(bin_recordings), max_bin_len, 12), dtype=np.float16)\n",
    "            for i, rec in enumerate(bin_recordings):\n",
    "                padded_sigs[i, :len(rec), :] = rec\n",
    "            \n",
    "            # Create group\n",
    "            grp = h5f.create_group(f'bin_{bin_idx}')\n",
    "            grp.create_dataset('X', data=padded_sigs, compression='gzip', compression_opts=4)\n",
    "            grp.create_dataset('y', data=bin_labels, compression='gzip', compression_opts=4)\n",
    "            grp.create_dataset('lengths', data=bin_lengths, compression='gzip', compression_opts=4)\n",
    "            grp.attrs['min_len'] = min_len\n",
    "            grp.attrs['max_len'] = max_len\n",
    "            grp.attrs['count'] = len(bin_recordings)\n",
    "            \n",
    "            print(f\"  bin_{bin_idx} ({min_len/500:.1f}-{max_len/500:.1f}s): {len(bin_recordings)} samples\")\n",
    "        \n",
    "        # Global metadata\n",
    "        h5f.attrs['num_classes'] = NUM_CLASSES\n",
    "        h5f.attrs['disease_names'] = DISEASE_NAMES  # Store disease names\n",
    "        h5f.attrs['icd_codes'] = ICD_CODES\n",
    "        h5f.attrs['sampling_rate'] = TARGET_FS\n",
    "        h5f.attrs['target_channels'] = TARGET_CHANNELS\n",
    "        h5f.attrs['total_samples'] = len(recordings)\n",
    "    \n",
    "    print(f\"\\n✓ Saved to {HDF5_FILE}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'disease_names': DISEASE_NAMES,\n",
    "        'icd_codes': ICD_CODES,\n",
    "        'icd_to_disease_name': ICD_TO_DISEASE_NAME,\n",
    "        'disease_groups': DISEASE_GROUPS,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'sampling_rate': TARGET_FS,\n",
    "        'target_channels': TARGET_CHANNELS,\n",
    "        'created': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(OUTPUT_DIR, 'metadata.json'), 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Saved metadata to {OUTPUT_DIR}/metadata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Enhanced Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def se_block(input_tensor, ratio=16):\n",
    "    \"\"\"\n",
    "    Squeeze-Excitation block for channel attention.\n",
    "    Recalibrates channel-wise feature responses.\n",
    "    \"\"\"\n",
    "    channels = input_tensor.shape[-1]\n",
    "    \n",
    "    # Squeeze: Global pooling\n",
    "    se = layers.GlobalAveragePooling1D()(input_tensor)\n",
    "    \n",
    "    # Excitation: FC → ReLU → FC → Sigmoid\n",
    "    se = layers.Dense(channels // ratio, activation='relu')(se)\n",
    "    se = layers.Dense(channels, activation='sigmoid')(se)\n",
    "    \n",
    "    # Scale: Multiply original features by learned weights\n",
    "    se = layers.Reshape((1, channels))(se)\n",
    "    return layers.Multiply()([input_tensor, se])\n",
    "\n",
    "\n",
    "def residual_conv_block(x, filters, kernel_size=3, pool_size=2, dropout=0.2):\n",
    "    \"\"\"\n",
    "    Convolutional block with residual connection and SE attention.\n",
    "    \"\"\"\n",
    "    shortcut = x\n",
    "    \n",
    "    # Conv layers\n",
    "    x = layers.Conv1D(filters, kernel_size, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    x = layers.Conv1D(filters, kernel_size, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # SE block\n",
    "    x = se_block(x, ratio=16)\n",
    "    \n",
    "    # Residual connection (match dimensions if needed)\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv1D(filters, 1, padding='same')(shortcut)\n",
    "    \n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling1D(pool_size)(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def temporal_attention_layer(x):\n",
    "    \"\"\"\n",
    "    Self-attention mechanism over temporal dimension.\n",
    "    Helps model focus on important time segments (P wave, QRS, T wave).\n",
    "    \"\"\"\n",
    "    filters = x.shape[-1]\n",
    "    \n",
    "    # Query, Key, Value projections\n",
    "    query = layers.Conv1D(filters, 1, padding='same')(x)\n",
    "    key = layers.Conv1D(filters, 1, padding='same')(x)\n",
    "    value = layers.Conv1D(filters, 1, padding='same')(x)\n",
    "    \n",
    "    # Attention scores: Q·K^T / sqrt(d_k)\n",
    "    attention_scores = layers.Dot(axes=[2, 2])([query, key])\n",
    "    attention_scores = layers.Lambda(lambda z: z / np.sqrt(float(filters)))(attention_scores)\n",
    "    attention_weights = layers.Softmax(axis=-1)(attention_scores)\n",
    "    \n",
    "    # Weighted sum: Attention·V\n",
    "    attended = layers.Dot(axes=[2, 1])([attention_weights, value])\n",
    "    \n",
    "    # Residual connection\n",
    "    output = layers.Add()([x, attended])\n",
    "    output = layers.LayerNormalization()(output)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def adaptive_pooling_layer(x):\n",
    "    \"\"\"\n",
    "    Combines multiple pooling strategies to get fixed-size representation.\n",
    "    Works with any input length.\n",
    "    \"\"\"\n",
    "    # Global Average Pooling\n",
    "    gap = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Global Max Pooling\n",
    "    gmp = layers.GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    # Concatenate both\n",
    "    return layers.Concatenate()([gap, gmp])  # Output: (2 * filters,)\n",
    "\n",
    "\n",
    "print(\"✓ Model building blocks defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_enhanced_1d_cnn(num_classes=19):\n",
    "    \"\"\"\n",
    "    Build enhanced 1D CNN for variable-length ECG classification.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input: (None, 12) - variable time steps\n",
    "    - 4 residual blocks with SE attention: 64→128→256→512\n",
    "    - Temporal attention layer\n",
    "    - Adaptive pooling (variable → fixed)\n",
    "    - Dense layers with dropout\n",
    "    - Multi-label sigmoid output\n",
    "    \"\"\"\n",
    "    # Input: (None, 12) where None = variable time steps\n",
    "    inputs = layers.Input(shape=(None, 12), name='ecg_input')\n",
    "    \n",
    "    # Initial conv\n",
    "    x = layers.Conv1D(64, 7, padding='same', activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Block 1: 128 filters\n",
    "    x = residual_conv_block(x, 128, kernel_size=5, pool_size=2, dropout=0.2)\n",
    "    \n",
    "    # Block 2: 256 filters\n",
    "    x = residual_conv_block(x, 256, kernel_size=3, pool_size=2, dropout=0.3)\n",
    "    \n",
    "    # Block 3: 512 filters\n",
    "    x = residual_conv_block(x, 512, kernel_size=3, pool_size=2, dropout=0.3)\n",
    "    \n",
    "    # Block 4: 512 filters (no pooling)\n",
    "    shortcut = x\n",
    "    x = layers.Conv1D(512, 3, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = se_block(x, ratio=16)\n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Temporal attention\n",
    "    x = temporal_attention_layer(x)\n",
    "    \n",
    "    # Adaptive pooling (variable → fixed size)\n",
    "    x = adaptive_pooling_layer(x)  # Output: (1024,) = 2*512\n",
    "    \n",
    "    # Dense layers\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # Multi-label output (19 classes, sigmoid)\n",
    "    outputs = layers.Dense(num_classes, activation='sigmoid', name='disease_output')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name='Enhanced_1D_CNN_v3')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Build model\n",
    "print(\"Building model...\")\n",
    "model = build_enhanced_1d_cnn(num_classes=NUM_CLASSES)\n",
    "\n",
    "print(\"\\nModel Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Test variable-length inputs\n",
    "print(\"\\n Testing variable-length inputs:\")\n",
    "test_lengths = [2500, 5000, 15000, 30000]  # 5s, 10s, 30s, 60s\n",
    "for length in test_lengths:\n",
    "    dummy_input = np.random.randn(1, length, 12).astype(np.float32)\n",
    "    output = model(dummy_input, training=False)\n",
    "    print(f\"  Input: {dummy_input.shape} → Output: {output.shape}\")\n",
    "\n",
    "print(\"\\n✓ Model handles variable lengths correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Focal Loss & Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"\n",
    "    Focal loss for handling class imbalance.\n",
    "    FL(p_t) = -alpha_t * (1 - p_t)^gamma * log(p_t)\n",
    "    \n",
    "    Focuses on hard-to-classify examples.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        # Clip predictions to prevent log(0)\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Binary cross entropy\n",
    "        bce = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n",
    "        \n",
    "        # Focal term: (1 - p_t)^gamma\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        focal_weight = tf.pow(1 - p_t, self.gamma)\n",
    "        \n",
    "        # Apply alpha weighting\n",
    "        alpha_weight = y_true * self.alpha + (1 - y_true) * (1 - self.alpha)\n",
    "        \n",
    "        # Combine\n",
    "        focal_loss = alpha_weight * focal_weight * bce\n",
    "        \n",
    "        return tf.reduce_mean(tf.reduce_sum(focal_loss, axis=-1))\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"alpha\": self.alpha, \"gamma\": self.gamma})\n",
    "        return config\n",
    "\n",
    "\n",
    "print(\"✓ Focal loss implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading function\n",
    "def load_data_from_hdf5(h5_file):\n",
    "    \"\"\"\n",
    "    Load all data from HDF5 file.\n",
    "    Returns X (list of variable-length arrays), y, lengths\n",
    "    \"\"\"\n",
    "    X_all = []\n",
    "    y_all = []\n",
    "    lengths_all = []\n",
    "    \n",
    "    with h5py.File(h5_file, 'r') as h5f:\n",
    "        for bin_name in sorted(h5f.keys()):\n",
    "            if not bin_name.startswith('bin_'):\n",
    "                continue\n",
    "            \n",
    "            grp = h5f[bin_name]\n",
    "            X_bin = grp['X'][:]\n",
    "            y_bin = grp['y'][:]\n",
    "            lengths_bin = grp['lengths'][:]\n",
    "            \n",
    "            # Trim each recording to actual length\n",
    "            for i in range(len(X_bin)):\n",
    "                X_all.append(X_bin[i, :lengths_bin[i], :])\n",
    "                y_all.append(y_bin[i])\n",
    "                lengths_all.append(lengths_bin[i])\n",
    "    \n",
    "    return X_all, np.array(y_all), np.array(lengths_all)\n",
    "\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data from HDF5...\")\n",
    "X_data, y_data, lengths_data = load_data_from_hdf5(HDF5_FILE)\n",
    "\n",
    "print(f\"Loaded {len(X_data)} samples\")\n",
    "print(f\"Length range: {lengths_data.min()/TARGET_FS:.1f}s - {lengths_data.max()/TARGET_FS:.1f}s\")\n",
    "print(f\"Mean length: {lengths_data.mean()/TARGET_FS:.1f}s\")\n",
    "\n",
    "# Split data\n",
    "indices = np.arange(len(X_data))\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.3, random_state=42)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"\\nData split:\")\n",
    "print(f\"  Train: {len(train_idx)} samples\")\n",
    "print(f\"  Val:   {len(val_idx)} samples\")\n",
    "print(f\"  Test:  {len(test_idx)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tf.data.Dataset for training\n",
    "def create_dataset(X_list, y_array, indices, batch_size=16, shuffle=True):\n",
    "    \"\"\"\n",
    "    Create tf.data.Dataset for variable-length recordings.\n",
    "    \"\"\"\n",
    "    def generator():\n",
    "        idx_list = indices.copy()\n",
    "        if shuffle:\n",
    "            np.random.shuffle(idx_list)\n",
    "        \n",
    "        for idx in idx_list:\n",
    "            yield X_list[idx].astype(np.float32), y_array[idx].astype(np.float32)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, 12), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(NUM_CLASSES,), dtype=tf.float32)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Padded batch (pads to max length in batch)\n",
    "    dataset = dataset.padded_batch(\n",
    "        batch_size,\n",
    "        padded_shapes=([None, 12], [NUM_CLASSES]),\n",
    "        padding_values=(0.0, 0.0)\n",
    "    )\n",
    "    \n",
    "    return dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "print(\"Creating TensorFlow datasets...\")\n",
    "train_dataset = create_dataset(X_data, y_data, train_idx, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataset = create_dataset(X_data, y_data, val_idx, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataset = create_dataset(X_data, y_data, test_idx, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"✓ Datasets created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "print(\"Compiling model...\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss=FocalLoss(alpha=0.25, gamma=2.0),\n",
    "    metrics=[\n",
    "        keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        keras.metrics.AUC(name='auc', multi_label=True, num_labels=NUM_CLASSES),\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_auc',\n",
    "        patience=15,\n",
    "        mode='max',\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(OUTPUT_DIR, 'best_model_v3.keras'),\n",
    "        monitor='val_auc',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.CSVLogger(\n",
    "        os.path.join(OUTPUT_DIR, 'training_history.csv')\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"✓ Model compiled with Focal Loss\")\n",
    "print(f\"  Optimizer: Adam (lr={LEARNING_RATE})\")\n",
    "print(f\"  Loss: Focal Loss (alpha=0.25, gamma=2.0)\")\n",
    "print(f\"  Metrics: Accuracy, AUC, Precision, Recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(f\"Starting training for {EPOCHS} epochs...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history.history['loss'], label='Train Loss')\n",
    "axes[0, 0].plot(history.history['val_loss'], label='Val Loss')\n",
    "axes[0, 0].set_title('Loss', fontsize=12)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# AUC\n",
    "axes[0, 1].plot(history.history['auc'], label='Train AUC')\n",
    "axes[0, 1].plot(history.history['val_auc'], label='Val AUC')\n",
    "axes[0, 1].set_title('AUC', fontsize=12)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('AUC')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Accuracy\n",
    "axes[1, 0].plot(history.history['accuracy'], label='Train Accuracy')\n",
    "axes[1, 0].plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "axes[1, 0].set_title('Accuracy', fontsize=12)\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Precision & Recall\n",
    "axes[1, 1].plot(history.history['precision'], label='Train Precision', linestyle='--')\n",
    "axes[1, 1].plot(history.history['recall'], label='Train Recall', linestyle=':')\n",
    "axes[1, 1].plot(history.history['val_precision'], label='Val Precision', linestyle='--')\n",
    "axes[1, 1].plot(history.history['val_recall'], label='Val Recall', linestyle=':')\n",
    "axes[1, 1].set_title('Precision & Recall', fontsize=12)\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'training_history.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Training curves saved to {OUTPUT_DIR}/training_history.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation with Disease Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on test set\n",
    "print(\"Generating predictions on test set...\")\n",
    "\n",
    "y_true_test = y_data[test_idx]\n",
    "y_pred_proba_test = model.predict(test_dataset, verbose=1)\n",
    "y_pred_test = (y_pred_proba_test > 0.5).astype(int)\n",
    "\n",
    "print(\"✓ Predictions complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall metrics\n",
    "print(\"=\"*80)\n",
    "print(\"OVERALL TEST METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Subset accuracy (exact match)\n",
    "subset_acc = np.mean(np.all(y_true_test == y_pred_test, axis=1))\n",
    "print(f\"Subset Accuracy (Exact Match): {subset_acc:.4f}\")\n",
    "\n",
    "# Hamming loss\n",
    "hamming = hamming_loss(y_true_test, y_pred_test)\n",
    "print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "\n",
    "# F1 scores\n",
    "f1_micro = f1_score(y_true_test, y_pred_test, average='micro', zero_division=0)\n",
    "f1_macro = f1_score(y_true_test, y_pred_test, average='macro', zero_division=0)\n",
    "f1_weighted = f1_score(y_true_test, y_pred_test, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"F1-Score (Micro):    {f1_micro:.4f}\")\n",
    "print(f\"F1-Score (Macro):    {f1_macro:.4f}\")\n",
    "print(f\"F1-Score (Weighted): {f1_weighted:.4f}\")\n",
    "\n",
    "# Precision & Recall\n",
    "prec_micro = precision_score(y_true_test, y_pred_test, average='micro', zero_division=0)\n",
    "rec_micro = recall_score(y_true_test, y_pred_test, average='micro', zero_division=0)\n",
    "\n",
    "print(f\"Precision (Micro):   {prec_micro:.4f}\")\n",
    "print(f\"Recall (Micro):      {rec_micro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class metrics with DISEASE NAMES ONLY (no ICD codes)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PER-CLASS METRICS (Grouped by Disease Category)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "per_class_results = {}\n",
    "\n",
    "# Group by disease category\n",
    "for group_name, group_icds in DISEASE_GROUPS.items():\n",
    "    print(f\"\\n{group_name.upper()}:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for icd in group_icds:\n",
    "        if icd not in ICD_CODES:\n",
    "            continue\n",
    "        \n",
    "        idx = ICD_CODES.index(icd)\n",
    "        disease_name = DISEASE_NAMES[idx]  # Get disease name\n",
    "        \n",
    "        y_true_class = y_true_test[:, idx]\n",
    "        y_pred_class = y_pred_test[:, idx]\n",
    "        y_pred_proba_class = y_pred_proba_test[:, idx]\n",
    "        \n",
    "        support = int(y_true_class.sum())\n",
    "        \n",
    "        if support > 0:\n",
    "            precision = precision_score(y_true_class, y_pred_class, zero_division=0)\n",
    "            recall = recall_score(y_true_class, y_pred_class, zero_division=0)\n",
    "            f1 = f1_score(y_true_class, y_pred_class, zero_division=0)\n",
    "            \n",
    "            try:\n",
    "                roc_auc = roc_auc_score(y_true_class, y_pred_proba_class)\n",
    "            except:\n",
    "                roc_auc = 0.0\n",
    "            \n",
    "            per_class_results[disease_name] = {\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'roc_auc': roc_auc,\n",
    "                'support': support\n",
    "            }\n",
    "            \n",
    "            # Print ONLY disease name (no ICD code)\n",
    "            print(f\"\\n{disease_name}:\")  # Changed from showing ICD code\n",
    "            print(f\"  Support:   {support:4d}\")\n",
    "            print(f\"  Precision: {precision:.4f}\")\n",
    "            print(f\"  Recall:    {recall:.4f}\")\n",
    "            print(f\"  F1-Score:  {f1:.4f}\")\n",
    "            print(f\"  ROC-AUC:   {roc_auc:.4f}\")\n",
    "\n",
    "# Healthy class\n",
    "print(f\"\\nHEALTHY:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "idx = ICD_CODES.index('Healthy')\n",
    "disease_name = DISEASE_NAMES[idx]  # \"Healthy\"\n",
    "\n",
    "y_true_class = y_true_test[:, idx]\n",
    "y_pred_class = y_pred_test[:, idx]\n",
    "y_pred_proba_class = y_pred_proba_test[:, idx]\n",
    "\n",
    "support = int(y_true_class.sum())\n",
    "\n",
    "if support > 0:\n",
    "    precision = precision_score(y_true_class, y_pred_class, zero_division=0)\n",
    "    recall = recall_score(y_true_class, y_pred_class, zero_division=0)\n",
    "    f1 = f1_score(y_true_class, y_pred_class, zero_division=0)\n",
    "    \n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_true_class, y_pred_proba_class)\n",
    "    except:\n",
    "        roc_auc = 0.0\n",
    "    \n",
    "    per_class_results[disease_name] = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'support': support\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{disease_name}:\")  # Just \"Healthy\"\n",
    "    print(f\"  Support:   {support:4d}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "    print(f\"  ROC-AUC:   {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_prediction_with_names(sample_idx, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Display prediction with DISEASE NAMES ONLY (no ICD codes).\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Sample {sample_idx + 1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get data\n",
    "    test_sample_idx = test_idx[sample_idx]\n",
    "    y_true = y_true_test[sample_idx]\n",
    "    y_pred_proba = y_pred_proba_test[sample_idx]\n",
    "    \n",
    "    # True labels - DISEASE NAMES ONLY\n",
    "    true_indices = np.where(y_true == 1)[0]\n",
    "    print(f\"\\nTrue Diagnoses ({len(true_indices)}):\")\n",
    "    if len(true_indices) > 0:\n",
    "        for idx in true_indices:\n",
    "            print(f\"  - {DISEASE_NAMES[idx]}\")  # ONLY disease name\n",
    "    else:\n",
    "        print(\"  - None\")\n",
    "    \n",
    "    # Predicted labels - DISEASE NAMES ONLY\n",
    "    pred_indices = np.where(y_pred_proba > threshold)[0]\n",
    "    print(f\"\\nPredicted Diagnoses (threshold={threshold}):\")\n",
    "    if len(pred_indices) > 0:\n",
    "        for idx in sorted(pred_indices, key=lambda i: -y_pred_proba[i]):\n",
    "            prob = y_pred_proba[idx]\n",
    "            marker = \"✓\" if y_true[idx] == 1 else \" \"\n",
    "            print(f\"  {marker} {DISEASE_NAMES[idx]}: {prob:.4f}\")  # ONLY disease name\n",
    "    else:\n",
    "        print(\"  - None\")\n",
    "    \n",
    "    # Top 5 predictions - DISEASE NAMES ONLY\n",
    "    print(f\"\\nTop 5 Predictions (all probabilities):\")\n",
    "    top_5 = np.argsort(y_pred_proba)[-5:][::-1]\n",
    "    for idx in top_5:\n",
    "        prob = y_pred_proba[idx]\n",
    "        marker = \"✓\" if y_true[idx] == 1 else \" \"\n",
    "        print(f\"  {marker} {DISEASE_NAMES[idx]}: {prob:.4f}\")  # ONLY disease name\n",
    "\n",
    "\n",
    "# Show predictions for random samples\n",
    "print(\"\\nSample Predictions:\")\n",
    "random_samples = np.random.choice(len(test_idx), size=min(5, len(test_idx)), replace=False)\n",
    "for i in random_samples:\n",
    "    show_prediction_with_names(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ECG signal with predictions\n",
    "def plot_ecg_with_predictions(sample_idx, num_leads=3):\n",
    "    \"\"\"\n",
    "    Plot ECG signal with true and predicted labels (DISEASE NAMES ONLY).\n",
    "    \"\"\"\n",
    "    test_sample_idx = test_idx[sample_idx]\n",
    "    ecg_signal = X_data[test_sample_idx]\n",
    "    y_true = y_true_test[sample_idx]\n",
    "    y_pred_proba = y_pred_proba_test[sample_idx]\n",
    "    \n",
    "    # Get disease names\n",
    "    true_labels = [DISEASE_NAMES[j] for j in range(NUM_CLASSES) if y_true[j] == 1]\n",
    "    pred_labels = [(DISEASE_NAMES[j], y_pred_proba[j]) \n",
    "                  for j in range(NUM_CLASSES) if y_pred_proba[j] > 0.5]\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(num_leads, 1, figsize=(15, 8))\n",
    "    if num_leads == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    time = np.arange(len(ecg_signal)) / TARGET_FS\n",
    "    lead_names = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "    \n",
    "    for i in range(num_leads):\n",
    "        axes[i].plot(time, ecg_signal[:, i], linewidth=0.8)\n",
    "        axes[i].set_ylabel(f'Lead {lead_names[i]}', fontsize=10)\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[-1].set_xlabel('Time (s)', fontsize=10)\n",
    "    \n",
    "    # Title with disease names\n",
    "    title = f'Sample {sample_idx + 1} - ECG Signal ({len(ecg_signal)/TARGET_FS:.1f}s)\\n'\n",
    "    title += f'True: {\", \".join(true_labels) if true_labels else \"None\"}\\n'\n",
    "    if pred_labels:\n",
    "        pred_str = \", \".join([f\"{l} ({p:.2f})\" for l, p in pred_labels])\n",
    "        title += f'Predicted: {pred_str}'\n",
    "    else:\n",
    "        title += 'Predicted: None'\n",
    "    \n",
    "    plt.suptitle(title, fontsize=11, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot a few examples\n",
    "print(\"\\nECG Signal Visualizations:\")\n",
    "for i in random_samples[:3]:\n",
    "    plot_ecg_with_predictions(i, num_leads=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Model & Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model_path = os.path.join(OUTPUT_DIR, f'{MODEL_NAME}_final.keras')\n",
    "model.save(final_model_path)\n",
    "print(f\"✓ Model saved to {final_model_path}\")\n",
    "\n",
    "# Save comprehensive metadata with DISEASE NAMES\n",
    "results_metadata = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'version': NOTEBOOK_VERSION,\n",
    "    'created': datetime.now().isoformat(),\n",
    "    \n",
    "    # Classes - DISEASE NAMES ONLY in main list\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'disease_names': DISEASE_NAMES,  # Main output list - names only\n",
    "    'icd_codes': ICD_CODES,  # Reference only\n",
    "    'icd_to_disease_name': ICD_TO_DISEASE_NAME,  # Mapping reference\n",
    "    'disease_groups': DISEASE_GROUPS,\n",
    "    \n",
    "    # Architecture\n",
    "    'architecture': {\n",
    "        'type': 'Enhanced 1D CNN',\n",
    "        'blocks': '64→128→256→512 filters',\n",
    "        'features': ['Squeeze-Excitation', 'Temporal Attention', 'Residual Connections'],\n",
    "        'pooling': 'Adaptive (Global Avg + Max)',\n",
    "        'total_params': int(model.count_params())\n",
    "    },\n",
    "    \n",
    "    # Training\n",
    "    'training': {\n",
    "        'epochs': len(history.history['loss']),\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'loss': 'Focal Loss (alpha=0.25, gamma=2.0)',\n",
    "        'optimizer': 'Adam'\n",
    "    },\n",
    "    \n",
    "    # Data\n",
    "    'data': {\n",
    "        'total_samples': len(X_data),\n",
    "        'train_samples': len(train_idx),\n",
    "        'val_samples': len(val_idx),\n",
    "        'test_samples': len(test_idx),\n",
    "        'sampling_rate': TARGET_FS,\n",
    "        'num_channels': TARGET_CHANNELS,\n",
    "        'variable_length': True,\n",
    "        'length_range': f\"{lengths_data.min()/TARGET_FS:.1f}s - {lengths_data.max()/TARGET_FS:.1f}s\"\n",
    "    },\n",
    "    \n",
    "    # Test metrics\n",
    "    'test_metrics': {\n",
    "        'subset_accuracy': float(subset_acc),\n",
    "        'hamming_loss': float(hamming),\n",
    "        'f1_micro': float(f1_micro),\n",
    "        'f1_macro': float(f1_macro),\n",
    "        'f1_weighted': float(f1_weighted),\n",
    "        'precision_micro': float(prec_micro),\n",
    "        'recall_micro': float(rec_micro)\n",
    "    },\n",
    "    \n",
    "    # Per-class results (DISEASE NAMES ONLY)\n",
    "    'per_class_results': per_class_results\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = os.path.join(OUTPUT_DIR, 'model_results.json')\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(results_metadata, f, indent=2)\n",
    "\n",
    "print(f\"✓ Metadata saved to {metadata_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: {final_model_path}\")\n",
    "print(f\"Metadata: {metadata_path}\")\n",
    "print(f\"Training history: {os.path.join(OUTPUT_DIR, 'training_history.csv')}\")\n",
    "print(f\"\\nTest F1-Score (Macro): {f1_macro:.4f}\")\n",
    "print(f\"Test AUC: {roc_auc:.4f}\" if 'roc_auc' in locals() else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements an enhanced 1D CNN for 19-class multi-label ECG classification:\n",
    "\n",
    "**Key Features:**\n",
    "- ✅ Processes full-length variable recordings (5-120 seconds, no windowing)\n",
    "- ✅ 19 classes (18 cardiac diseases + Healthy)\n",
    "- ✅ Enhanced architecture with SE blocks and temporal attention\n",
    "- ✅ Focal loss for class imbalance\n",
    "- ✅ **All outputs show disease names only** (not ICD codes)\n",
    "\n",
    "**Architecture:**\n",
    "- 64→128→256→512 filters with residual connections\n",
    "- Squeeze-Excitation blocks for channel attention\n",
    "- Temporal attention for time-series focus\n",
    "- Adaptive pooling for variable lengths\n",
    "\n",
    "**Next Steps:**\n",
    "- Fine-tune threshold per class\n",
    "- Analyze attention weights\n",
    "- Deploy for inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}