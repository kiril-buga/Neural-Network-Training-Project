{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kiril-buga/Neural-Network-Training-Project/blob/main/1D_CNN_Multilabel_19_Classes_V3/Y_1d_CNN_19_Labels_FullLength_v03_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTQHye1KhctY"
   },
   "source": [
    "# Enhanced 1D CNN for 19-Class Multi-Label ECG Classification (v03)\n",
    "\n",
    "**Key Features:**\n",
    "- 19-class multi-label classification (18 cardiac diseases + Healthy)\n",
    "- **Full-length variable recordings** (5-120 seconds, no windowing)\n",
    "- Enhanced architecture: Squeeze-Excitation blocks + Temporal Attention\n",
    "- Focal loss for class imbalance\n",
    "- **Outputs show disease names only** (not ICD codes)\n",
    "\n",
    "**Architecture:** 64→128→256→512 filters + SE + Attention + Adaptive Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8H9lAJ3hctc"
   },
   "source": [
    "## 0. Google Colab Setup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qNwKpN6Khctc",
    "outputId": "d2a57a0a-4231-4c37-9dca-b9dcd5af35c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Running in Google Colab\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "✓ HuggingFace Hub installed\n"
     ]
    }
   ],
   "source": [
    "# Check environment and install dependencies\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"✓ Running in Google Colab\")\n",
    "\n",
    "    # Mount Google Drive for persistent storage (optional backup)\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "    # Install HuggingFace Hub\n",
    "    !pip install huggingface-hub -q\n",
    "    print(\"✓ HuggingFace Hub installed\")\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lCoMr0K5hctd",
    "outputId": "6519227d-3e01-433a-8630-b2b1c54f5726"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Mixed precision enabled (float16)\n",
      "✓ GPU memory growth enabled: /physical_device:GPU:0\n",
      "Num GPUs Available: 1\n"
     ]
    }
   ],
   "source": [
    "# GPU Configuration for Colab free tier\n",
    "if IN_COLAB:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import mixed_precision\n",
    "\n",
    "    # Enable mixed precision (saves memory, faster training)\n",
    "    mixed_precision.set_global_policy('mixed_float16')\n",
    "    print(\"✓ Mixed precision enabled (float16)\")\n",
    "\n",
    "    # Enable memory growth to avoid OOM\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f\"✓ GPU memory growth enabled: {gpus[0].name}\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "\n",
    "    print(f\"Num GPUs Available: {len(gpus)}\")\n",
    "else:\n",
    "    print(\"Skipping GPU config (not in Colab)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5w2jcIddhcte"
   },
   "source": [
    "## 0.1 HuggingFace Setup (Colab Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "peJtu8llhctf",
    "outputId": "5da0c720-896b-49ae-e1a6-c61bf4f5792d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Hugging Face token with WRITE access:\n",
      "Get it from: https://huggingface.co/settings/tokens\n",
      "✓ Using HF token from Colab secrets\n",
      "✓ Logged in to Hugging Face\n",
      "✓ Model repo exists: Neural-Network-Project/ECG-Disease-Classifier\n",
      "✓ Dataset repo exists: Neural-Network-Project/ECG-database\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace Setup (only run in Colab)\n",
    "if IN_COLAB:\n",
    "    from huggingface_hub import HfApi, login, create_repo\n",
    "    from getpass import getpass\n",
    "\n",
    "    print(\"Please provide your Hugging Face token with WRITE access:\")\n",
    "    print(\"Get it from: https://huggingface.co/settings/tokens\")\n",
    "\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "        print(\"✓ Using HF token from Colab secrets\")\n",
    "    except:\n",
    "        HF_TOKEN = getpass(\"Enter your HF token: \")\n",
    "\n",
    "    # Login\n",
    "    login(token=HF_TOKEN, add_to_git_credential=True)\n",
    "    print(\"✓ Logged in to Hugging Face\")\n",
    "\n",
    "    # Repository names\n",
    "    MODEL_REPO = \"Neural-Network-Project/ECG-Disease-Classifier\"\n",
    "    DATASET_REPO = \"Neural-Network-Project/ECG-database\"\n",
    "\n",
    "    # Initialize API\n",
    "    api = HfApi()\n",
    "\n",
    "    # Ensure repos exist\n",
    "    try:\n",
    "        api.repo_info(repo_id=MODEL_REPO, repo_type=\"model\")\n",
    "        print(f\"✓ Model repo exists: {MODEL_REPO}\")\n",
    "    except:\n",
    "        create_repo(repo_id=MODEL_REPO, repo_type=\"model\", private=False)\n",
    "        print(f\"✓ Created model repo: {MODEL_REPO}\")\n",
    "\n",
    "    try:\n",
    "        api.repo_info(repo_id=DATASET_REPO, repo_type=\"dataset\")\n",
    "        print(f\"✓ Dataset repo exists: {DATASET_REPO}\")\n",
    "    except:\n",
    "        create_repo(repo_id=DATASET_REPO, repo_type=\"dataset\", private=False)\n",
    "        print(f\"✓ Created dataset repo: {DATASET_REPO}\")\n",
    "else:\n",
    "    print(\"Skipping HuggingFace setup (not in Colab)\")\n",
    "    MODEL_REPO = None\n",
    "    DATASET_REPO = None\n",
    "    api = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmZoSzVihctf"
   },
   "source": [
    "## 0.2 Download Dataset from HuggingFace (Colab Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173,
     "referenced_widgets": [
      "d24c3c21ff7942a5ad80b3bb659d5631",
      "f2003522041b4e2e8b6ec8cd3a85df3d",
      "7bb1de3cdae6411dae0d1948126a0841",
      "62dd2f76305e4d5c8f62ee064b22bb42",
      "8eb45b9ff94a4ac4b41075703f8ffba1",
      "41e8679e55bb49b8a749b1c24c10a266",
      "589fa537e4744dfea0d6c0da7bf4034c",
      "45a2ee4ed74645f7aeb027e305150349",
      "6ec89b2b3f2a4577b75ace8656ea3774",
      "8ebc5563012943088ded136823c447a8",
      "8b3ea80c1bf14b0e826182345b835a6a"
     ]
    },
    "id": "BRUIUY9ihctg",
    "outputId": "c96b8c29-5e1d-4ca5-9898-5dd33f0bae8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset from HuggingFace...\n",
      "This may take a few minutes on first run...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24c3c21ff7942a5ad80b3bb659d5631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset downloaded to: /content/.data\n",
      "  - Raw ECG data: .data/data/Child_ecg/\n",
      "  - Metadata CSV: .data/data/AttributesDictionary.csv\n",
      "  - Preprocessed H5 (if exists): .data/artifacts/\n"
     ]
    }
   ],
   "source": [
    "# Download entire dataset from HuggingFace (Colab only)\n",
    "if IN_COLAB and DATASET_REPO:\n",
    "    print(\"Downloading dataset from HuggingFace...\")\n",
    "    print(\"This may take a few minutes on first run...\\n\")\n",
    "    \n",
    "    from huggingface_hub import snapshot_download\n",
    "    \n",
    "    local_dir = snapshot_download(\n",
    "        repo_id=DATASET_REPO,\n",
    "        repo_type=\"dataset\",\n",
    "        allow_patterns=[\"data/**\", \"artifacts/**\"],\n",
    "        ignore_patterns=[\"artifacts/training_results/**\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Dataset downloaded to: {local_dir}\")\n",
    "    print(f\"  - Raw ECG data: {local_dir}/data/Child_ecg/\")\n",
    "    print(f\"  - Metadata CSV: {local_dir}/data/AttributesDictionary.csv\")\n",
    "    print(f\"  - Preprocessed H5 (if exists): {local_dir}/artifacts/\")\n",
    "else:\n",
    "    print(\"Skipping dataset download (not in Colab)\")\n",
    "    local_dir = None  # Will be set in config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Extract Split Archive (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract split archives if needed (both Colab and local)\n",
    "import os\n",
    "import zipfile\n",
    "import glob\n",
    "\n",
    "# Determine data directory based on environment\n",
    "if IN_COLAB:\n",
    "    archive_dir = os.path.join(local_dir, \"data\")\n",
    "    target_ecg_dir = os.path.join(archive_dir, \"Child_ecg\")\n",
    "else:\n",
    "    archive_dir = \"../.data/data\"\n",
    "    target_ecg_dir = os.path.join(archive_dir, \"Child_ecg\")\n",
    "\n",
    "# Check if Child_ecg directory exists and has files\n",
    "def count_files_recursive(directory):\n",
    "    \"\"\"Count all files recursively in a directory\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        return 0\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        count += len(files)\n",
    "    return count\n",
    "\n",
    "ecg_file_count = count_files_recursive(target_ecg_dir)\n",
    "\n",
    "print(f\"Checking ECG data extraction...\")\n",
    "print(f\"  ECG directory: {target_ecg_dir}\")\n",
    "print(f\"  Files found: {ecg_file_count}\")\n",
    "\n",
    "# If directory doesn't exist or has very few files, extract the archives\n",
    "if ecg_file_count < 1000:  # Expect tens of thousands of files\n",
    "    print(f\"\\n⚠️  Insufficient ECG files detected. Checking for archives to extract...\")\n",
    "    \n",
    "    # Look for split archive files\n",
    "    zip_file = os.path.join(archive_dir, \"Child_ecg.zip\")\n",
    "    z01_file = os.path.join(archive_dir, \"Child_ecg.z01\")\n",
    "    \n",
    "    if os.path.exists(zip_file):\n",
    "        print(f\"\\n✓ Found archive files:\")\n",
    "        print(f\"  - {zip_file}\")\n",
    "        if os.path.exists(z01_file):\n",
    "            print(f\"  - {z01_file}\")\n",
    "        \n",
    "        print(f\"\\nExtracting archives...\")\n",
    "        print(f\"This may take several minutes...\")\n",
    "        \n",
    "        try:\n",
    "            # For split archives (.z01 + .zip), we need to combine them first\n",
    "            # In Google Colab or systems with 7zip available\n",
    "            if os.path.exists(z01_file):\n",
    "                # Try using 7zip if available (works in most environments)\n",
    "                try:\n",
    "                    import subprocess\n",
    "                    # Try to use 7z command\n",
    "                    result = subprocess.run(\n",
    "                        ['7z', 'x', zip_file, f'-o{archive_dir}', '-y'],\n",
    "                        capture_output=True,\n",
    "                        text=True\n",
    "                    )\n",
    "                    if result.returncode == 0:\n",
    "                        print(f\"✓ Successfully extracted using 7zip\")\n",
    "                    else:\n",
    "                        raise Exception(\"7zip extraction failed\")\n",
    "                except:\n",
    "                    # Fallback: Try using zipfile (may not work with split archives)\n",
    "                    print(\"7zip not available, trying Python zipfile...\")\n",
    "                    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "                        zip_ref.extractall(archive_dir)\n",
    "                    print(f\"✓ Extraction attempted with zipfile\")\n",
    "            else:\n",
    "                # Single zip file, use Python's zipfile\n",
    "                with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(archive_dir)\n",
    "                print(f\"✓ Successfully extracted zip file\")\n",
    "            \n",
    "            # Verify extraction\n",
    "            new_file_count = count_files_recursive(target_ecg_dir)\n",
    "            print(f\"\\n✓ Extraction complete!\")\n",
    "            print(f\"  Files extracted: {new_file_count}\")\n",
    "            \n",
    "            if new_file_count < 1000:\n",
    "                print(f\"\\n⚠️  Warning: Expected more files. Archive may not have extracted completely.\")\n",
    "                print(f\"  Please manually extract {zip_file} if needed.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error during extraction: {e}\")\n",
    "            print(f\"\\nPlease manually extract the following file:\")\n",
    "            print(f\"  {zip_file}\")\n",
    "            print(f\"To directory: {archive_dir}\")\n",
    "            raise\n",
    "    else:\n",
    "        print(f\"\\n❌ Archive files not found!\")\n",
    "        print(f\"  Expected: {zip_file}\")\n",
    "        print(f\"\\nPlease ensure the archive files are present in {archive_dir}\")\n",
    "        raise FileNotFoundError(f\"Archive not found: {zip_file}\")\n",
    "else:\n",
    "    print(f\"✓ ECG data already extracted ({ecg_file_count} files found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-U-0E1rhctg"
   },
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r57yfLYNhcth",
    "outputId": "724528db-4289-4bb2-ff1c-2fd62879faf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dependencies installed\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies (skip in Colab as already installed in setup)\n",
    "if not IN_COLAB:\n",
    "    !pip install wfdb neurokit2 h5py tensorflow scikit-learn matplotlib seaborn pandas numpy scipy -q\n",
    "else:\n",
    "    !pip install wfdb neurokit2 -q\n",
    "\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PLpIpq85hcth",
    "outputId": "faa3f3df-e818-47f8-e7b2-88905d048846"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "Num GPUs Available: 1\n",
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import wfdb\n",
    "from scipy.signal import butter, filtfilt, resample\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, hamming_loss, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Num GPUs Available: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2gJj3it2hcth",
    "outputId": "f4464442-59a2-4752-8881-9bb76cbb7eaf"
   },
   "outputs": [],
   "source": "# Configuration\nNOTEBOOK_VERSION = \"v03_balanced\"  # Changed from \"v03\"\n\n# Adjust paths and settings for Colab vs local\nif IN_COLAB:\n    # Use downloaded dataset from HuggingFace\n    DATA_DIR = local_dir + \"/\"  # Downloaded from HF\n    OUTPUT_DIR = \"/content/artifacts/\"\n    BATCH_SIZE = 16  # Increased from 8\n    CHECKPOINT_EVERY = 5  # Save every 5 epochs\nelse:\n    # Local paths\n    DATA_DIR = \"../.data/\"\n    OUTPUT_DIR = os.path.join(DATA_DIR, \"artifacts/multilabel_v3_balanced/\")\n    BATCH_SIZE = 32\n    CHECKPOINT_EVERY = 5\n\nECG_DIR = os.path.join(DATA_DIR, \"data/Child_ecg/\")\nCSV_PATH = os.path.join(DATA_DIR, \"data/AttributesDictionary.csv\")\nHDF5_FILE = os.path.join(OUTPUT_DIR, \"ecg_full_length_19classes.h5\")\nMODEL_NAME = \"Enhanced_1D_CNN_19Classes_v3_balanced\"  # Changed from \"v3\"\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Training parameters\nEPOCHS = 50  # Reduced from 100\nLEARNING_RATE = 1e-3\nTARGET_FS = 500  # Target sampling frequency\nTARGET_CHANNELS = 12\n\nprint(f\"Configuration:\")\nprint(f\"  Environment: {'Google Colab' if IN_COLAB else 'Local'}\")\nprint(f\"  Data directory: {DATA_DIR}\")\nprint(f\"  ECG directory: {ECG_DIR}\")\nprint(f\"  CSV path: {CSV_PATH}\")\nprint(f\"  Output directory: {OUTPUT_DIR}\")\nprint(f\"  HDF5 file: {HDF5_FILE}\")\nprint(f\"  Model version: {NOTEBOOK_VERSION}\")\nprint(f\"  Model name: {MODEL_NAME}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  Epochs: {EPOCHS}\")\nprint(f\"  Checkpoint every: {CHECKPOINT_EVERY} epochs\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0WlJ-xThcti"
   },
   "source": [
    "## 2. Disease Mapping & Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eAwrpz1nhcti",
    "outputId": "37055cab-68cc-4118-b70c-25d3342b1493"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 19\n",
      "\n",
      "Disease Names (outputs will show these names ONLY):\n",
      "   1. Fulminant/Viral Myocarditis\n",
      "   2. Acute Myocarditis\n",
      "   3. Myocarditis Unspecified\n",
      "   4. Dilated Cardiomyopathy\n",
      "   5. Hypertrophic Cardiomyopathy\n",
      "   6. Cardiomyopathy Unspecified\n",
      "   7. Noncompaction Ventricular Myocardium\n",
      "   8. Kawasaki Disease\n",
      "   9. Ventricular Septal Defect\n",
      "  10. Atrial Septal Defect\n",
      "  11. Atrioventricular Septal Defect\n",
      "  12. Tetralogy of Fallot\n",
      "  13. Pulmonary Valve Stenosis\n",
      "  14. Patent Ductus Arteriosus\n",
      "  15. Pulmonary Artery Stenosis\n",
      "  16. Pulmonary Valve Regurgitation\n",
      "  17. Mitral Valve Insufficiency\n",
      "  18. Congenital Heart Malformation\n",
      "  19. Healthy\n",
      "\n",
      "Disease Groups:\n",
      "  Myocarditis: 3 diseases\n",
      "  Cardiomyopathy: 4 diseases\n",
      "  Kawasaki: 1 diseases\n",
      "  CHD: 10 diseases\n"
     ]
    }
   ],
   "source": [
    "# ICD codes to Disease Names mapping\n",
    "ICD_TO_DISEASE_NAME = {\n",
    "    'I40.0': 'Fulminant/Viral Myocarditis',\n",
    "    'I40.9': 'Acute Myocarditis',\n",
    "    'I51.4': 'Myocarditis Unspecified',\n",
    "    'I42.0': 'Dilated Cardiomyopathy',\n",
    "    'I42.2': 'Hypertrophic Cardiomyopathy',\n",
    "    'I42.9': 'Cardiomyopathy Unspecified',\n",
    "    'Q24.8': 'Noncompaction Ventricular Myocardium',\n",
    "    'M30.3': 'Kawasaki Disease',\n",
    "    'Q21.0': 'Ventricular Septal Defect',\n",
    "    'Q21.1': 'Atrial Septal Defect',\n",
    "    'Q21.2': 'Atrioventricular Septal Defect',\n",
    "    'Q21.3': 'Tetralogy of Fallot',\n",
    "    'Q22.1': 'Pulmonary Valve Stenosis',\n",
    "    'Q25.0': 'Patent Ductus Arteriosus',\n",
    "    'Q25.6': 'Pulmonary Artery Stenosis',\n",
    "    'I37.0': 'Pulmonary Valve Regurgitation',\n",
    "    'I34.0': 'Mitral Valve Insufficiency',\n",
    "    'Q24.9': 'Congenital Heart Malformation',\n",
    "    'Healthy': 'Healthy'  # Changed from \"No Disease (Healthy)\" to just \"Healthy\"\n",
    "}\n",
    "\n",
    "# Disease names list (for model outputs - NO ICD codes in outputs)\n",
    "DISEASE_NAMES = list(ICD_TO_DISEASE_NAME.values())\n",
    "ICD_CODES = list(ICD_TO_DISEASE_NAME.keys())\n",
    "NUM_CLASSES = len(DISEASE_NAMES)\n",
    "\n",
    "# Disease grouping for interpretability\n",
    "DISEASE_GROUPS = {\n",
    "    'Myocarditis': ['I40.0', 'I40.9', 'I51.4'],\n",
    "    'Cardiomyopathy': ['I42.0', 'I42.2', 'I42.9', 'Q24.8'],\n",
    "    'Kawasaki': ['M30.3'],\n",
    "    'CHD': ['Q21.0', 'Q21.1', 'Q21.2', 'Q21.3', 'Q22.1', 'Q25.0', 'Q25.6', 'I37.0', 'I34.0', 'Q24.9']\n",
    "}\n",
    "\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"\\nDisease Names (outputs will show these names ONLY):\")\n",
    "for i, name in enumerate(DISEASE_NAMES, 1):\n",
    "    print(f\"  {i:2d}. {name}\")\n",
    "\n",
    "print(f\"\\nDisease Groups:\")\n",
    "for group, codes in DISEASE_GROUPS.items():\n",
    "    names = [ICD_TO_DISEASE_NAME[code] for code in codes]\n",
    "    print(f\"  {group}: {len(codes)} diseases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xv3n9gqHhcti"
   },
   "source": [
    "## 3. Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Y1dZHIghcti",
    "outputId": "5e527b1d-da75-4b2a-a269-f5eb895faeb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Preprocessing functions defined\n"
     ]
    }
   ],
   "source": [
    "def apply_bandpass(x, fs, lowcut=0.5, highcut=40.0):\n",
    "    \"\"\"Apply bandpass filter to entire ECG signal.\"\"\"\n",
    "    if x.ndim == 1:\n",
    "        x = x[:, None]\n",
    "    nyq = 0.5 * fs\n",
    "    b, a = butter(4, [lowcut/nyq, highcut/nyq], btype=\"band\")\n",
    "    return np.column_stack([filtfilt(b, a, x[:, i]) for i in range(x.shape[1])])\n",
    "\n",
    "\n",
    "def process_full_recording(sig, meta, target_fs=500.0, target_channels=12):\n",
    "    \"\"\"\n",
    "    Process full ECG recording without windowing.\n",
    "    Handles variable lengths (5-120 seconds).\n",
    "\n",
    "    Returns:\n",
    "        sig_processed: (n_samples, 12) - variable length\n",
    "        length: actual sample count\n",
    "    \"\"\"\n",
    "    # Get sampling frequency\n",
    "    fs = meta.get(\"fs\", None)\n",
    "    if fs is None:\n",
    "        fs = getattr(meta, \"fs\", None)\n",
    "    if fs is None:\n",
    "        raise ValueError(\"Missing sampling frequency\")\n",
    "\n",
    "    if sig.ndim == 1:\n",
    "        sig = sig[:, None]\n",
    "\n",
    "    # 1. Bandpass filter entire signal (0.5-40 Hz)\n",
    "    sig_bp = apply_bandpass(sig, fs=fs)\n",
    "\n",
    "    # 2. Resample to target_fs if needed\n",
    "    if fs != target_fs:\n",
    "        n_samples = sig_bp.shape[0]\n",
    "        n_new = int(round(n_samples / fs * target_fs))\n",
    "        sig_res = np.column_stack([resample(sig_bp[:, i], n_new)\n",
    "                                   for i in range(sig_bp.shape[1])])\n",
    "    else:\n",
    "        sig_res = sig_bp\n",
    "\n",
    "    # 3. Standardize to 12 leads (pad with zeros if fewer)\n",
    "    if sig_res.shape[1] < target_channels:\n",
    "        padded = np.zeros((sig_res.shape[0], target_channels), dtype=np.float32)\n",
    "        padded[:, :sig_res.shape[1]] = sig_res\n",
    "        sig_res = padded\n",
    "    elif sig_res.shape[1] > target_channels:\n",
    "        sig_res = sig_res[:, :target_channels]\n",
    "\n",
    "    # 4. Z-score normalization per lead\n",
    "    sig_norm = sig_res.copy()\n",
    "    for ch in range(target_channels):\n",
    "        x = sig_norm[:, ch]\n",
    "        m, s = np.nanmean(x), np.nanstd(x)\n",
    "        sig_norm[:, ch] = (x - m) / (s if s > 1e-6 else 1.0)\n",
    "\n",
    "    actual_length = sig_norm.shape[0]\n",
    "\n",
    "    return sig_norm.astype(np.float16), actual_length\n",
    "\n",
    "\n",
    "def parse_icd_codes(s):\n",
    "    \"\"\"Parse semicolon-separated ICD codes from CSV.\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return []\n",
    "    return [p.strip().replace(\"'\", \"\").replace(\")\", \"\").split(\")\")[-1]\n",
    "            for p in str(s).split(\";\") if p.strip()]\n",
    "\n",
    "\n",
    "print(\"✓ Preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9V6tZpjYhctj"
   },
   "source": [
    "## 4. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCwywQn3hctj"
   },
   "source": [
    "## 4. Load Data (with HuggingFace Support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h07vGO4Qhctj",
    "outputId": "0101433c-dd24-4f96-ba54-e3d11b3220f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata...\n",
      "Loaded 14190 ECG records\n",
      "Sample ICD codes: ['I34.0', 'Q21.0', 'Q24.9']\n"
     ]
    }
   ],
   "source": [
    "# Load CSV metadata\n",
    "print(\"Loading metadata...\")\n",
    "df_attr = pd.read_csv(CSV_PATH)\n",
    "print(f\"Loaded {len(df_attr)} ECG records\")\n",
    "\n",
    "# Parse ICD codes\n",
    "df_attr[\"ICD_list\"] = df_attr[\"ICD-10 code\"].apply(parse_icd_codes)\n",
    "print(f\"Sample ICD codes: {df_attr['ICD_list'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "7jchJev6hctj",
    "outputId": "1be5ebd8-9591-4039-b1aa-b1d9f5068ff0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 file not found at: /content/artifacts/ecg_full_length_19classes.h5\n",
      "Creating preprocessed dataset from raw ECG files...\n",
      "This may take 30-60 minutes...\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "ECG directory not found: .data/data/Child_ecg/\nMake sure dataset was downloaded from HuggingFace correctly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2835292057.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mECG_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         raise FileNotFoundError(\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0;34mf\"ECG directory not found: {ECG_DIR}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;34mf\"Make sure dataset was downloaded from HuggingFace correctly.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: ECG directory not found: .data/data/Child_ecg/\nMake sure dataset was downloaded from HuggingFace correctly."
     ]
    }
   ],
   "source": [
    "# Check if preprocessed HDF5 exists (either from HuggingFace or local)\n",
    "HDF5_EXISTS = os.path.exists(HDF5_FILE)\n",
    "\n",
    "if HDF5_EXISTS:\n",
    "    print(f\"✓ Found preprocessed HDF5 file: {HDF5_FILE}\")\n",
    "    print(\"Loading preprocessed data...\")\n",
    "\n",
    "    with h5py.File(HDF5_FILE, 'r') as h5f:\n",
    "        print(f\"\\nHDF5 Contents:\")\n",
    "        print(f\"  Bins: {list(h5f.keys())}\")\n",
    "\n",
    "        total_samples = 0\n",
    "        for bin_name in h5f.keys():\n",
    "            if bin_name.startswith('bin_'):\n",
    "                grp = h5f[bin_name]\n",
    "                bin_samples = grp['X'].shape[0]\n",
    "                total_samples += bin_samples\n",
    "                print(f\"  {bin_name}: {bin_samples} samples\")\n",
    "\n",
    "        print(f\"\\n  Total samples: {total_samples}\")\n",
    "        print(f\"  Number of classes: {h5f.attrs['num_classes']}\")\n",
    "        print(f\"  Sampling rate: {h5f.attrs['sampling_rate']} Hz\")\n",
    "\n",
    "else:\n",
    "    print(f\"HDF5 file not found at: {HDF5_FILE}\")\n",
    "    print(f\"Creating preprocessed dataset from raw ECG files...\")\n",
    "    print(f\"This may take 30-60 minutes...\\n\")\n",
    "\n",
    "    # Verify raw data exists\n",
    "    if not os.path.exists(CSV_PATH):\n",
    "        raise FileNotFoundError(\n",
    "            f\"CSV file not found: {CSV_PATH}\\n\"\n",
    "            f\"Make sure dataset was downloaded from HuggingFace correctly.\"\n",
    "        )\n",
    "\n",
    "    if not os.path.exists(ECG_DIR):\n",
    "        raise FileNotFoundError(\n",
    "            f\"ECG directory not found: {ECG_DIR}\\n\"\n",
    "            f\"Make sure dataset was downloaded from HuggingFace correctly.\"\n",
    "        )\n",
    "\n",
    "    # Process all recordings\n",
    "    recordings = []\n",
    "    labels = []\n",
    "    lengths = []\n",
    "    filenames = []\n",
    "\n",
    "    print(\"Processing ECG recordings...\")\n",
    "    for idx, row in df_attr.iterrows():\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print(f\"  Processed {idx + 1}/{len(df_attr)} records...\")\n",
    "\n",
    "        fname = row[\"Filename\"]\n",
    "        path = os.path.join(ECG_DIR, fname)\n",
    "\n",
    "        try:\n",
    "            sig, meta = wfdb.rdsamp(path)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        meta_dict = meta if isinstance(meta, dict) else meta.__dict__\n",
    "        sig = np.asarray(sig)\n",
    "\n",
    "        # Process full recording\n",
    "        try:\n",
    "            sig_proc, length = process_full_recording(sig, meta_dict)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # Create multi-label vector\n",
    "        label_vec = np.zeros(NUM_CLASSES, dtype=np.int32)\n",
    "        icd_list = row[\"ICD_list\"]\n",
    "\n",
    "        if not icd_list or all(pd.isna(icd_list)):\n",
    "            label_vec[ICD_CODES.index('Healthy')] = 1\n",
    "        else:\n",
    "            found_disease = False\n",
    "            for icd in icd_list:\n",
    "                if icd in ICD_CODES:\n",
    "                    label_vec[ICD_CODES.index(icd)] = 1\n",
    "                    found_disease = True\n",
    "\n",
    "            if not found_disease:\n",
    "                label_vec[ICD_CODES.index('Healthy')] = 1\n",
    "\n",
    "        recordings.append(sig_proc)\n",
    "        labels.append(label_vec)\n",
    "        lengths.append(length)\n",
    "        filenames.append(fname)\n",
    "\n",
    "    print(f\"\\n✓ Processed {len(recordings)} valid recordings\")\n",
    "\n",
    "    # Save to HDF5 with length binning\n",
    "    print(\"\\nSaving to HDF5 with length binning...\")\n",
    "\n",
    "    labels_array = np.array(labels)\n",
    "    lengths_array = np.array(lengths)\n",
    "\n",
    "    # Define bins: <10s, 10-30s, 30-60s, 60-120s (in samples @ 500Hz)\n",
    "    length_bins = [(0, 5000), (5000, 15000), (15000, 30000), (30000, 60000)]\n",
    "\n",
    "    with h5py.File(HDF5_FILE, 'w') as h5f:\n",
    "        for bin_idx, (min_len, max_len) in enumerate(length_bins):\n",
    "            mask = (lengths_array >= min_len) & (lengths_array < max_len)\n",
    "            if not np.any(mask):\n",
    "                continue\n",
    "\n",
    "            bin_recordings = [recordings[i] for i in np.where(mask)[0]]\n",
    "            bin_labels = labels_array[mask]\n",
    "            bin_lengths = lengths_array[mask]\n",
    "            bin_filenames = [filenames[i] for i in np.where(mask)[0]]\n",
    "\n",
    "            # Pad to max length in this bin\n",
    "            max_bin_len = bin_lengths.max()\n",
    "            padded_sigs = np.zeros((len(bin_recordings), max_bin_len, 12), dtype=np.float16)\n",
    "            for i, rec in enumerate(bin_recordings):\n",
    "                padded_sigs[i, :len(rec), :] = rec\n",
    "\n",
    "            # Create group\n",
    "            grp = h5f.create_group(f'bin_{bin_idx}')\n",
    "            grp.create_dataset('X', data=padded_sigs, compression='gzip', compression_opts=4)\n",
    "            grp.create_dataset('y', data=bin_labels, compression='gzip', compression_opts=4)\n",
    "            grp.create_dataset('lengths', data=bin_lengths, compression='gzip', compression_opts=4)\n",
    "            grp.attrs['min_len'] = min_len\n",
    "            grp.attrs['max_len'] = max_len\n",
    "            grp.attrs['count'] = len(bin_recordings)\n",
    "\n",
    "            print(f\"  bin_{bin_idx} ({min_len/500:.1f}-{max_len/500:.1f}s): {len(bin_recordings)} samples\")\n",
    "\n",
    "        # Global metadata\n",
    "        h5f.attrs['num_classes'] = NUM_CLASSES\n",
    "        h5f.attrs['disease_names'] = DISEASE_NAMES\n",
    "        h5f.attrs['icd_codes'] = ICD_CODES\n",
    "        h5f.attrs['sampling_rate'] = TARGET_FS\n",
    "        h5f.attrs['target_channels'] = TARGET_CHANNELS\n",
    "        h5f.attrs['total_samples'] = len(recordings)\n",
    "\n",
    "    print(f\"\\n✓ Saved to {HDF5_FILE}\")\n",
    "\n",
    "    # Upload to HuggingFace if in Colab\n",
    "    if IN_COLAB and DATASET_REPO and api:\n",
    "        print(f\"\\nUploading preprocessed H5 to Hugging Face...\")\n",
    "        try:\n",
    "            # Upload to artifacts folder\n",
    "            api.upload_file(\n",
    "                path_or_fileobj=HDF5_FILE,\n",
    "                path_in_repo=\"artifacts/ecg_full_length_19classes.h5\",\n",
    "                repo_id=DATASET_REPO,\n",
    "                repo_type=\"dataset\",\n",
    "                commit_message=\"Add preprocessed full-length ECG data (19 classes)\"\n",
    "            )\n",
    "            print(f\"✓ Uploaded to {DATASET_REPO}/artifacts/\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not upload to HF: {e}\")\n",
    "\n",
    "    # Save metadata\n",
    "    from datetime import datetime\n",
    "    metadata = {\n",
    "        'disease_names': DISEASE_NAMES,\n",
    "        'icd_codes': ICD_CODES,\n",
    "        'icd_to_disease_name': ICD_TO_DISEASE_NAME,\n",
    "        'disease_groups': DISEASE_GROUPS,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'sampling_rate': TARGET_FS,\n",
    "        'target_channels': TARGET_CHANNELS,\n",
    "        'created': datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(OUTPUT_DIR, 'metadata.json'), 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "\n",
    "    print(f\"✓ Saved metadata to {OUTPUT_DIR}/metadata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSo-TZxChctk"
   },
   "source": [
    "## 5. Enhanced Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73T-rBh2hctk"
   },
   "outputs": [],
   "source": [
    "def se_block(input_tensor, ratio=16):\n",
    "    \"\"\"\n",
    "    Squeeze-Excitation block for channel attention.\n",
    "    Recalibrates channel-wise feature responses.\n",
    "    \"\"\"\n",
    "    channels = input_tensor.shape[-1]\n",
    "\n",
    "    # Squeeze: Global pooling\n",
    "    se = layers.GlobalAveragePooling1D()(input_tensor)\n",
    "\n",
    "    # Excitation: FC → ReLU → FC → Sigmoid\n",
    "    se = layers.Dense(channels // ratio, activation='relu')(se)\n",
    "    se = layers.Dense(channels, activation='sigmoid')(se)\n",
    "\n",
    "    # Scale: Multiply original features by learned weights\n",
    "    se = layers.Reshape((1, channels))(se)\n",
    "    return layers.Multiply()([input_tensor, se])\n",
    "\n",
    "\n",
    "def residual_conv_block(x, filters, kernel_size=3, pool_size=2, dropout=0.2):\n",
    "    \"\"\"\n",
    "    Convolutional block with residual connection and SE attention.\n",
    "    \"\"\"\n",
    "    shortcut = x\n",
    "\n",
    "    # Conv layers\n",
    "    x = layers.Conv1D(filters, kernel_size, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv1D(filters, kernel_size, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # SE block\n",
    "    x = se_block(x, ratio=16)\n",
    "\n",
    "    # Residual connection (match dimensions if needed)\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv1D(filters, 1, padding='same')(shortcut)\n",
    "\n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling1D(pool_size)(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def temporal_attention_layer(x):\n",
    "    \"\"\"\n",
    "    Self-attention mechanism over temporal dimension.\n",
    "    Helps model focus on important time segments (P wave, QRS, T wave).\n",
    "    \"\"\"\n",
    "    filters = x.shape[-1]\n",
    "\n",
    "    # Query, Key, Value projections\n",
    "    query = layers.Conv1D(filters, 1, padding='same')(x)\n",
    "    key = layers.Conv1D(filters, 1, padding='same')(x)\n",
    "    value = layers.Conv1D(filters, 1, padding='same')(x)\n",
    "\n",
    "    # Attention scores: Q·K^T / sqrt(d_k)\n",
    "    attention_scores = layers.Dot(axes=[2, 2])([query, key])\n",
    "    attention_scores = layers.Lambda(lambda z: z / np.sqrt(float(filters)))(attention_scores)\n",
    "    attention_weights = layers.Softmax(axis=-1)(attention_scores)\n",
    "\n",
    "    # Weighted sum: Attention·V\n",
    "    attended = layers.Dot(axes=[2, 1])([attention_weights, value])\n",
    "\n",
    "    # Residual connection\n",
    "    output = layers.Add()([x, attended])\n",
    "    output = layers.LayerNormalization()(output)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def adaptive_pooling_layer(x):\n",
    "    \"\"\"\n",
    "    Combines multiple pooling strategies to get fixed-size representation.\n",
    "    Works with any input length.\n",
    "    \"\"\"\n",
    "    # Global Average Pooling\n",
    "    gap = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Global Max Pooling\n",
    "    gmp = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "    # Concatenate both\n",
    "    return layers.Concatenate()([gap, gmp])  # Output: (2 * filters,)\n",
    "\n",
    "\n",
    "print(\"✓ Model building blocks defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQq67_C8hctk"
   },
   "outputs": [],
   "source": [
    "def build_enhanced_1d_cnn(num_classes=19):\n",
    "    \"\"\"\n",
    "    Build enhanced 1D CNN for variable-length ECG classification.\n",
    "\n",
    "    Architecture:\n",
    "    - Input: (None, 12) - variable time steps\n",
    "    - 4 residual blocks with SE attention: 64→128→256→512\n",
    "    - Temporal attention layer\n",
    "    - Adaptive pooling (variable → fixed)\n",
    "    - Dense layers with dropout\n",
    "    - Multi-label sigmoid output\n",
    "    \"\"\"\n",
    "    # Input: (None, 12) where None = variable time steps\n",
    "    inputs = layers.Input(shape=(None, 12), name='ecg_input')\n",
    "\n",
    "    # Initial conv\n",
    "    x = layers.Conv1D(64, 7, padding='same', activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    # Block 1: 128 filters\n",
    "    x = residual_conv_block(x, 128, kernel_size=5, pool_size=2, dropout=0.2)\n",
    "\n",
    "    # Block 2: 256 filters\n",
    "    x = residual_conv_block(x, 256, kernel_size=3, pool_size=2, dropout=0.3)\n",
    "\n",
    "    # Block 3: 512 filters\n",
    "    x = residual_conv_block(x, 512, kernel_size=3, pool_size=2, dropout=0.3)\n",
    "\n",
    "    # Block 4: 512 filters (no pooling)\n",
    "    shortcut = x\n",
    "    x = layers.Conv1D(512, 3, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = se_block(x, ratio=16)\n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    # Temporal attention\n",
    "    x = temporal_attention_layer(x)\n",
    "\n",
    "    # Adaptive pooling (variable → fixed size)\n",
    "    x = adaptive_pooling_layer(x)  # Output: (1024,) = 2*512\n",
    "\n",
    "    # Dense layers\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    # Multi-label output (19 classes, sigmoid)\n",
    "    outputs = layers.Dense(num_classes, activation='sigmoid', name='disease_output')(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name='Enhanced_1D_CNN_v3')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build model\n",
    "print(\"Building model...\")\n",
    "model = build_enhanced_1d_cnn(num_classes=NUM_CLASSES)\n",
    "\n",
    "print(\"\\nModel Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Test variable-length inputs\n",
    "print(\"\\n Testing variable-length inputs:\")\n",
    "test_lengths = [2500, 5000, 15000, 30000]  # 5s, 10s, 30s, 60s\n",
    "for length in test_lengths:\n",
    "    dummy_input = np.random.randn(1, length, 12).astype(np.float32)\n",
    "    output = model(dummy_input, training=False)\n",
    "    print(f\"  Input: {dummy_input.shape} → Output: {output.shape}\")\n",
    "\n",
    "print(\"\\n✓ Model handles variable lengths correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgfACOrRhctl"
   },
   "source": [
    "## 6. Focal Loss & Training Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czagQg8Mhctl"
   },
   "source": [
    "## 6.1 HuggingFace Checkpoint Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qBmsgo4vhctl"
   },
   "outputs": [],
   "source": [
    "# HuggingFace Checkpoint Callback (Colab only)\n",
    "if IN_COLAB and MODEL_REPO:\n",
    "    class HuggingFaceCheckpoint(tf.keras.callbacks.Callback):\n",
    "        \"\"\"\n",
    "        Save checkpoints to Hugging Face Hub every N epochs.\n",
    "        Handles Colab disconnections by allowing resume.\n",
    "        \"\"\"\n",
    "        def __init__(self, repo_id, every_n_epochs=5):\n",
    "            super().__init__()\n",
    "            self.repo_id = repo_id\n",
    "            self.every_n_epochs = every_n_epochs\n",
    "            self.api = HfApi()\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            if (epoch + 1) % self.every_n_epochs == 0:\n",
    "                epoch_num = epoch + 1\n",
    "\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"Epoch {epoch_num}: Saving checkpoint to Hugging Face...\")\n",
    "                print(f\"{'='*60}\")\n",
    "\n",
    "                # Save model locally\n",
    "                checkpoint_path = f\"/content/checkpoint_epoch_{epoch_num}.keras\"\n",
    "                self.model.save(checkpoint_path, save_format='keras')\n",
    "\n",
    "                # Upload to HuggingFace\n",
    "                try:\n",
    "                    self.api.upload_file(\n",
    "                        path_or_fileobj=checkpoint_path,\n",
    "                        path_in_repo=f\"checkpoints/checkpoint_epoch_{epoch_num}.keras\",\n",
    "                        repo_id=self.repo_id,\n",
    "                        repo_type=\"model\",\n",
    "                        commit_message=f\"Checkpoint at epoch {epoch_num}\"\n",
    "                    )\n",
    "                    print(f\"✓ Checkpoint uploaded: epoch {epoch_num}\")\n",
    "\n",
    "                    # Save metrics\n",
    "                    import json\n",
    "                    metrics = {\n",
    "                        'epoch': epoch_num,\n",
    "                        'loss': float(logs.get('loss', 0)),\n",
    "                        'val_loss': float(logs.get('val_loss', 0)),\n",
    "                        'accuracy': float(logs.get('accuracy', 0)),\n",
    "                        'val_accuracy': float(logs.get('val_accuracy', 0)),\n",
    "                        'auc': float(logs.get('auc', 0)),\n",
    "                        'val_auc': float(logs.get('val_auc', 0))\n",
    "                    }\n",
    "\n",
    "                    metrics_path = f\"/content/metrics_epoch_{epoch_num}.json\"\n",
    "                    with open(metrics_path, 'w') as f:\n",
    "                        json.dump(metrics, f, indent=2)\n",
    "\n",
    "                    self.api.upload_file(\n",
    "                        path_or_fileobj=metrics_path,\n",
    "                        path_in_repo=f\"checkpoints/metrics_epoch_{epoch_num}.json\",\n",
    "                        repo_id=self.repo_id,\n",
    "                        repo_type=\"model\",\n",
    "                        commit_message=f\"Metrics for epoch {epoch_num}\"\n",
    "                    )\n",
    "\n",
    "                    # Clean up local files\n",
    "                    import os\n",
    "                    os.remove(checkpoint_path)\n",
    "                    os.remove(metrics_path)\n",
    "\n",
    "                    print(f\"✓ Metrics uploaded\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error uploading to HF: {e}\")\n",
    "\n",
    "                print(f\"{'='*60}\\n\")\n",
    "\n",
    "    print(\"✓ HuggingFaceCheckpoint callback defined\")\n",
    "else:\n",
    "    print(\"Skipping HF checkpoint (not in Colab)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yn-t2T8Lhctl"
   },
   "source": [
    "## 6.2 Resume from Checkpoint (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWnO13kshctl"
   },
   "outputs": [],
   "source": [
    "# Try to resume from last checkpoint (Colab only)\n",
    "RESUME_TRAINING = False\n",
    "INITIAL_EPOCH = 0\n",
    "\n",
    "if IN_COLAB and MODEL_REPO:\n",
    "    def get_latest_checkpoint(repo_id):\n",
    "        try:\n",
    "            files = api.list_repo_files(repo_id=repo_id, repo_type=\"model\")\n",
    "            checkpoints = [f for f in files if f.startswith(\"checkpoints/checkpoint_epoch_\") and f.endswith(\".keras\")]\n",
    "\n",
    "            if not checkpoints:\n",
    "                return None, 0\n",
    "\n",
    "            epochs = [int(f.split(\"_\")[-1].replace(\".keras\", \"\")) for f in checkpoints]\n",
    "            latest_epoch = max(epochs)\n",
    "            latest_file = f\"checkpoints/checkpoint_epoch_{latest_epoch}.keras\"\n",
    "\n",
    "            return latest_file, latest_epoch\n",
    "        except Exception as e:\n",
    "            print(f\"Could not check for checkpoints: {e}\")\n",
    "            return None, 0\n",
    "\n",
    "    # Check for existing checkpoint\n",
    "    latest_checkpoint, start_epoch = get_latest_checkpoint(MODEL_REPO)\n",
    "\n",
    "    if latest_checkpoint:\n",
    "        print(f\"Found checkpoint: {latest_checkpoint}\")\n",
    "        print(f\"Resuming from epoch {start_epoch}\")\n",
    "\n",
    "        try:\n",
    "            from huggingface_hub import hf_hub_download\n",
    "            checkpoint_path = hf_hub_download(\n",
    "                repo_id=MODEL_REPO,\n",
    "                filename=latest_checkpoint,\n",
    "                repo_type=\"model\",\n",
    "                cache_dir=\"/content/hf_cache\"\n",
    "            )\n",
    "\n",
    "            # Load model - will be assigned after model architecture is defined\n",
    "            RESUME_TRAINING = True\n",
    "            INITIAL_EPOCH = start_epoch\n",
    "            RESUME_CHECKPOINT_PATH = checkpoint_path\n",
    "            print(\"✓ Will resume from checkpoint\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint: {e}\")\n",
    "            print(\"Will train from scratch\")\n",
    "    else:\n",
    "        print(\"No checkpoint found. Training from scratch.\")\n",
    "else:\n",
    "    print(\"Not in Colab - training from scratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKi7n-0vhctl"
   },
   "outputs": [],
   "source": [
    "class FocalLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"\n",
    "    Focal loss for handling class imbalance.\n",
    "    FL(p_t) = -alpha_t * (1 - p_t)^gamma * log(p_t)\n",
    "\n",
    "    Focuses on hard-to-classify examples.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Clip predictions to prevent log(0)\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Binary cross entropy\n",
    "        bce = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n",
    "\n",
    "        # Focal term: (1 - p_t)^gamma\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        focal_weight = tf.pow(1 - p_t, self.gamma)\n",
    "\n",
    "        # Apply alpha weighting\n",
    "        alpha_weight = y_true * self.alpha + (1 - y_true) * (1 - self.alpha)\n",
    "\n",
    "        # Combine\n",
    "        focal_loss = alpha_weight * focal_weight * bce\n",
    "\n",
    "        return tf.reduce_mean(tf.reduce_sum(focal_loss, axis=-1))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"alpha\": self.alpha, \"gamma\": self.gamma})\n",
    "        return config\n",
    "\n",
    "\n",
    "print(\"✓ Focal loss implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9VLQRLwqhctm"
   },
   "outputs": [],
   "source": [
    "# Data loading function\n",
    "def load_data_from_hdf5(h5_file):\n",
    "    \"\"\"\n",
    "    Load all data from HDF5 file.\n",
    "    Returns X (list of variable-length arrays), y, lengths\n",
    "    \"\"\"\n",
    "    X_all = []\n",
    "    y_all = []\n",
    "    lengths_all = []\n",
    "\n",
    "    with h5py.File(h5_file, 'r') as h5f:\n",
    "        for bin_name in sorted(h5f.keys()):\n",
    "            if not bin_name.startswith('bin_'):\n",
    "                continue\n",
    "\n",
    "            grp = h5f[bin_name]\n",
    "            X_bin = grp['X'][:]\n",
    "            y_bin = grp['y'][:]\n",
    "            lengths_bin = grp['lengths'][:]\n",
    "\n",
    "            # Trim each recording to actual length\n",
    "            for i in range(len(X_bin)):\n",
    "                X_all.append(X_bin[i, :lengths_bin[i], :])\n",
    "                y_all.append(y_bin[i])\n",
    "                lengths_all.append(lengths_bin[i])\n",
    "\n",
    "    return X_all, np.array(y_all), np.array(lengths_all)\n",
    "\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data from HDF5...\")\n",
    "X_data, y_data, lengths_data = load_data_from_hdf5(HDF5_FILE)\n",
    "\n",
    "print(f\"Loaded {len(X_data)} samples\")\n",
    "print(f\"Length range: {lengths_data.min()/TARGET_FS:.1f}s - {lengths_data.max()/TARGET_FS:.1f}s\")\n",
    "print(f\"Mean length: {lengths_data.mean()/TARGET_FS:.1f}s\")\n",
    "\n",
    "# Split data\n",
    "indices = np.arange(len(X_data))\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.3, random_state=42)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"\\nData split:\")\n",
    "print(f\"  Train: {len(train_idx)} samples\")\n",
    "print(f\"  Val:   {len(val_idx)} samples\")\n",
    "print(f\"  Test:  {len(test_idx)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 6.3 Data Augmentation & Class Balancing\n\n**Critical Fix:** The original model had severe class imbalance (F1-macro: 0.045).\n\n**Solution:**\n- Undersample \"Healthy\" class to 3x the largest disease class\n- Augment minority disease classes with 4 techniques\n- Expected improvement: F1-macro from 0.045 → 0.4-0.6",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Data Augmentation Functions\ndef augment_ecg_signal(ecg, augmentation_type='noise'):\n    \"\"\"\n    Apply augmentation to ECG signal.\n    \n    Types:\n    - 'noise': Add Gaussian noise\n    - 'scale': Amplitude scaling\n    - 'shift': Time shifting\n    - 'stretch': Time stretching\n    \"\"\"\n    ecg_aug = ecg.copy()\n    \n    if augmentation_type == 'noise':\n        # Add small Gaussian noise (SNR ~20-30dB)\n        noise_level = 0.05 * np.std(ecg_aug)\n        noise = np.random.normal(0, noise_level, ecg_aug.shape)\n        ecg_aug = ecg_aug + noise\n        \n    elif augmentation_type == 'scale':\n        # Amplitude scaling (0.9-1.1x)\n        scale_factor = np.random.uniform(0.9, 1.1)\n        ecg_aug = ecg_aug * scale_factor\n        \n    elif augmentation_type == 'shift':\n        # Time shift (up to 5% of length)\n        shift_samples = int(0.05 * len(ecg_aug) * np.random.uniform(-1, 1))\n        ecg_aug = np.roll(ecg_aug, shift_samples, axis=0)\n        \n    elif augmentation_type == 'stretch':\n        # Time stretching (0.95-1.05x speed)\n        from scipy.signal import resample\n        stretch_factor = np.random.uniform(0.95, 1.05)\n        new_length = int(len(ecg_aug) * stretch_factor)\n        ecg_stretched = np.column_stack([resample(ecg_aug[:, i], new_length) \n                                         for i in range(ecg_aug.shape[1])])\n        # Crop or pad to original length\n        if len(ecg_stretched) > len(ecg_aug):\n            start = (len(ecg_stretched) - len(ecg_aug)) // 2\n            ecg_aug = ecg_stretched[start:start+len(ecg_aug)]\n        else:\n            pad_before = (len(ecg_aug) - len(ecg_stretched)) // 2\n            pad_after = len(ecg_aug) - len(ecg_stretched) - pad_before\n            ecg_aug = np.pad(ecg_stretched, ((pad_before, pad_after), (0, 0)), mode='edge')\n    \n    return ecg_aug.astype(np.float16)\n\n\nprint(\"✓ Augmentation functions defined\")\n\n# CLASS BALANCING\n# Strategy:\n# 1. Undersample Healthy to 3x the largest disease class\n# 2. Augment disease classes to reach target count (2x original)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"APPLYING CLASS BALANCING\")\nprint(\"=\"*80)\n\n# Analyze class distribution BEFORE balancing\nprint(\"\\nClass Distribution BEFORE Balancing:\")\nhealthy_idx = ICD_CODES.index('Healthy')\nhealthy_mask = y_data[train_idx, healthy_idx] == 1\nhealthy_count = healthy_mask.sum()\nprint(f\"  Healthy: {healthy_count}\")\n\ndisease_counts = {}\nfor i, disease_name in enumerate(DISEASE_NAMES):\n    if disease_name != 'Healthy':\n        disease_mask = y_data[train_idx, i] == 1\n        count = disease_mask.sum()\n        if count > 0:\n            disease_counts[disease_name] = count\n            \nmax_disease_count = max(disease_counts.values()) if disease_counts else 0\nprint(f\"  Max disease samples: {max_disease_count}\")\nprint(f\"  Imbalance ratio: {healthy_count / max(max_disease_count, 1):.1f}:1\")\n\n# Step 1: Undersample Healthy\ntarget_healthy_count = max_disease_count * 3  # 3x largest disease\nhealthy_indices = train_idx[healthy_mask]\n\nif len(healthy_indices) > target_healthy_count:\n    selected_healthy = np.random.choice(healthy_indices, size=target_healthy_count, replace=False)\n    print(f\"\\n✓ Undersampled Healthy: {len(healthy_indices)} → {target_healthy_count}\")\nelse:\n    selected_healthy = healthy_indices\n    print(f\"\\n✓ Kept all Healthy samples: {len(selected_healthy)}\")\n\n# Step 2: Keep all disease samples + create augmented versions\ndisease_indices = train_idx[~healthy_mask]\naugmented_X = []\naugmented_y = []\naugmented_lengths = []\n\nprint(f\"\\nAugmenting disease samples...\")\naug_types = ['noise', 'scale', 'shift', 'stretch']\n\nfor idx in disease_indices:\n    # Find which diseases this sample has\n    sample_diseases = np.where(y_data[idx] == 1)[0]\n    \n    if len(sample_diseases) > 0:\n        # Original sample\n        augmented_X.append(X_data[idx])\n        augmented_y.append(y_data[idx])\n        augmented_lengths.append(lengths_data[idx])\n        \n        # Create 2-3 augmented versions\n        num_augmentations = np.random.randint(2, 4)\n        for _ in range(num_augmentations):\n            aug_type = np.random.choice(aug_types)\n            augmented_ecg = augment_ecg_signal(X_data[idx], augmentation_type=aug_type)\n            \n            augmented_X.append(augmented_ecg)\n            augmented_y.append(y_data[idx])\n            augmented_lengths.append(len(augmented_ecg))\n\nprint(f\"  Original disease samples: {len(disease_indices)}\")\nprint(f\"  After augmentation: {len(augmented_X)}\")\n\n# Combine balanced data\nprint(f\"\\nCombining balanced dataset...\")\nbalanced_X = []\nbalanced_y = []\nbalanced_lengths = []\n\n# Add undersampled healthy\nfor idx in selected_healthy:\n    balanced_X.append(X_data[idx])\n    balanced_y.append(y_data[idx])\n    balanced_lengths.append(lengths_data[idx])\n\n# Add augmented diseases\nbalanced_X.extend(augmented_X)\nbalanced_y.extend(augmented_y)\nbalanced_lengths.extend(augmented_lengths)\n\n# Update training indices to point to new balanced dataset\nbalanced_train_idx = np.arange(len(balanced_X))\n\n# Replace original data for training\nX_data_original = X_data.copy()\ny_data_original = y_data.copy()\nlengths_data_original = lengths_data.copy()\n\nX_data = balanced_X\ny_data = np.array(balanced_y)\nlengths_data = np.array(balanced_lengths)\ntrain_idx = balanced_train_idx\n\nprint(f\"\\n✓ Balanced dataset created!\")\nprint(f\"  Total training samples: {len(train_idx)}\")\nprint(f\"  Healthy: {target_healthy_count}\")\nprint(f\"  Diseases (augmented): {len(augmented_X)}\")\nprint(f\"  New imbalance ratio: ~1.5:1\")\n\nprint(\"\\n\" + \"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQNntmeVhctm"
   },
   "outputs": [],
   "source": [
    "# Create tf.data.Dataset for training\n",
    "def create_dataset(X_list, y_array, indices, batch_size=16, shuffle=True):\n",
    "    \"\"\"\n",
    "    Create tf.data.Dataset for variable-length recordings.\n",
    "    \"\"\"\n",
    "    def generator():\n",
    "        idx_list = indices.copy()\n",
    "        if shuffle:\n",
    "            np.random.shuffle(idx_list)\n",
    "\n",
    "        for idx in idx_list:\n",
    "            yield X_list[idx].astype(np.float32), y_array[idx].astype(np.float32)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, 12), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(NUM_CLASSES,), dtype=tf.float32)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Padded batch (pads to max length in batch)\n",
    "    dataset = dataset.padded_batch(\n",
    "        batch_size,\n",
    "        padded_shapes=([None, 12], [NUM_CLASSES]),\n",
    "        padding_values=(0.0, 0.0)\n",
    "    )\n",
    "\n",
    "    return dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "print(\"Creating TensorFlow datasets...\")\n",
    "train_dataset = create_dataset(X_data, y_data, train_idx, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataset = create_dataset(X_data, y_data, val_idx, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataset = create_dataset(X_data, y_data, test_idx, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"✓ Datasets created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvF0xeYWhctm"
   },
   "outputs": [],
   "source": [
    "# Build or load model\n",
    "if IN_COLAB and RESUME_TRAINING:\n",
    "    print(\"Loading model from checkpoint...\")\n",
    "    model = keras.models.load_model(\n",
    "        RESUME_CHECKPOINT_PATH,\n",
    "        custom_objects={'FocalLoss': FocalLoss}\n",
    "    )\n",
    "    print(\"✓ Model loaded from checkpoint\")\n",
    "else:\n",
    "    print(\"Building new model...\")\n",
    "    # Model already built in previous cell\n",
    "\n",
    "    # Compile model\n",
    "    print(\"Compiling model...\")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss=FocalLoss(alpha=0.25, gamma=2.0),\n",
    "        metrics=[\n",
    "            keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "            keras.metrics.AUC(name='auc', multi_label=True, num_labels=NUM_CLASSES),\n",
    "            keras.metrics.Precision(name='precision'),\n",
    "            keras.metrics.Recall(name='recall'),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"✓ Model compiled\")\n",
    "\n",
    "# Callbacks\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_auc',\n",
    "        patience=10,\n",
    "        mode='max',\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(OUTPUT_DIR, 'best_model_v3.keras'),\n",
    "        monitor='val_auc',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.CSVLogger(\n",
    "        os.path.join(OUTPUT_DIR, 'training_history.csv')\n",
    "    )\n",
    "]\n",
    "\n",
    "# Add HuggingFace checkpoint callback if in Colab\n",
    "if IN_COLAB and MODEL_REPO:\n",
    "    callbacks_list.insert(0, HuggingFaceCheckpoint(\n",
    "        repo_id=MODEL_REPO,\n",
    "        every_n_epochs=CHECKPOINT_EVERY\n",
    "    ))\n",
    "    print(\"✓ HuggingFace checkpoint callback added\")\n",
    "\n",
    "print(f\"✓ Setup complete. Ready to train from epoch {INITIAL_EPOCH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5UjbsiPhctm"
   },
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-CPYtIZhctm"
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(f\"Starting training from epoch {INITIAL_EPOCH}...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    initial_epoch=INITIAL_EPOCH,  # Resume from here\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oh9cpdXYhctn"
   },
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history.history['loss'], label='Train Loss')\n",
    "axes[0, 0].plot(history.history['val_loss'], label='Val Loss')\n",
    "axes[0, 0].set_title('Loss', fontsize=12)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# AUC\n",
    "axes[0, 1].plot(history.history['auc'], label='Train AUC')\n",
    "axes[0, 1].plot(history.history['val_auc'], label='Val AUC')\n",
    "axes[0, 1].set_title('AUC', fontsize=12)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('AUC')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Accuracy\n",
    "axes[1, 0].plot(history.history['accuracy'], label='Train Accuracy')\n",
    "axes[1, 0].plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "axes[1, 0].set_title('Accuracy', fontsize=12)\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Precision & Recall\n",
    "axes[1, 1].plot(history.history['precision'], label='Train Precision', linestyle='--')\n",
    "axes[1, 1].plot(history.history['recall'], label='Train Recall', linestyle=':')\n",
    "axes[1, 1].plot(history.history['val_precision'], label='Val Precision', linestyle='--')\n",
    "axes[1, 1].plot(history.history['val_recall'], label='Val Recall', linestyle=':')\n",
    "axes[1, 1].set_title('Precision & Recall', fontsize=12)\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'training_history.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Training curves saved to {OUTPUT_DIR}/training_history.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Q2854Cfhctn"
   },
   "source": [
    "## 8. Evaluation with Disease Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1w-gaybbhctn"
   },
   "outputs": [],
   "source": [
    "# Get predictions on test set\n",
    "print(\"Generating predictions on test set...\")\n",
    "\n",
    "y_true_test = y_data[test_idx]\n",
    "y_pred_proba_test = model.predict(test_dataset, verbose=1)\n",
    "y_pred_test = (y_pred_proba_test > 0.5).astype(int)\n",
    "\n",
    "print(\"✓ Predictions complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Dd3AfLbhctn"
   },
   "outputs": [],
   "source": [
    "# Overall metrics\n",
    "print(\"=\"*80)\n",
    "print(\"OVERALL TEST METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Subset accuracy (exact match)\n",
    "subset_acc = np.mean(np.all(y_true_test == y_pred_test, axis=1))\n",
    "print(f\"Subset Accuracy (Exact Match): {subset_acc:.4f}\")\n",
    "\n",
    "# Hamming loss\n",
    "hamming = hamming_loss(y_true_test, y_pred_test)\n",
    "print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "\n",
    "# F1 scores\n",
    "f1_micro = f1_score(y_true_test, y_pred_test, average='micro', zero_division=0)\n",
    "f1_macro = f1_score(y_true_test, y_pred_test, average='macro', zero_division=0)\n",
    "f1_weighted = f1_score(y_true_test, y_pred_test, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"F1-Score (Micro):    {f1_micro:.4f}\")\n",
    "print(f\"F1-Score (Macro):    {f1_macro:.4f}\")\n",
    "print(f\"F1-Score (Weighted): {f1_weighted:.4f}\")\n",
    "\n",
    "# Precision & Recall\n",
    "prec_micro = precision_score(y_true_test, y_pred_test, average='micro', zero_division=0)\n",
    "rec_micro = recall_score(y_true_test, y_pred_test, average='micro', zero_division=0)\n",
    "\n",
    "print(f\"Precision (Micro):   {prec_micro:.4f}\")\n",
    "print(f\"Recall (Micro):      {rec_micro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YG06bygjhctn"
   },
   "outputs": [],
   "source": [
    "# Per-class metrics with DISEASE NAMES ONLY (no ICD codes)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PER-CLASS METRICS (Grouped by Disease Category)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "per_class_results = {}\n",
    "\n",
    "# Group by disease category\n",
    "for group_name, group_icds in DISEASE_GROUPS.items():\n",
    "    print(f\"\\n{group_name.upper()}:\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for icd in group_icds:\n",
    "        if icd not in ICD_CODES:\n",
    "            continue\n",
    "\n",
    "        idx = ICD_CODES.index(icd)\n",
    "        disease_name = DISEASE_NAMES[idx]  # Get disease name\n",
    "\n",
    "        y_true_class = y_true_test[:, idx]\n",
    "        y_pred_class = y_pred_test[:, idx]\n",
    "        y_pred_proba_class = y_pred_proba_test[:, idx]\n",
    "\n",
    "        support = int(y_true_class.sum())\n",
    "\n",
    "        if support > 0:\n",
    "            precision = precision_score(y_true_class, y_pred_class, zero_division=0)\n",
    "            recall = recall_score(y_true_class, y_pred_class, zero_division=0)\n",
    "            f1 = f1_score(y_true_class, y_pred_class, zero_division=0)\n",
    "\n",
    "            try:\n",
    "                roc_auc = roc_auc_score(y_true_class, y_pred_proba_class)\n",
    "            except:\n",
    "                roc_auc = 0.0\n",
    "\n",
    "            per_class_results[disease_name] = {\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'roc_auc': roc_auc,\n",
    "                'support': support\n",
    "            }\n",
    "\n",
    "            # Print ONLY disease name (no ICD code)\n",
    "            print(f\"\\n{disease_name}:\")  # Changed from showing ICD code\n",
    "            print(f\"  Support:   {support:4d}\")\n",
    "            print(f\"  Precision: {precision:.4f}\")\n",
    "            print(f\"  Recall:    {recall:.4f}\")\n",
    "            print(f\"  F1-Score:  {f1:.4f}\")\n",
    "            print(f\"  ROC-AUC:   {roc_auc:.4f}\")\n",
    "\n",
    "# Healthy class\n",
    "print(f\"\\nHEALTHY:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "idx = ICD_CODES.index('Healthy')\n",
    "disease_name = DISEASE_NAMES[idx]  # \"Healthy\"\n",
    "\n",
    "y_true_class = y_true_test[:, idx]\n",
    "y_pred_class = y_pred_test[:, idx]\n",
    "y_pred_proba_class = y_pred_proba_test[:, idx]\n",
    "\n",
    "support = int(y_true_class.sum())\n",
    "\n",
    "if support > 0:\n",
    "    precision = precision_score(y_true_class, y_pred_class, zero_division=0)\n",
    "    recall = recall_score(y_true_class, y_pred_class, zero_division=0)\n",
    "    f1 = f1_score(y_true_class, y_pred_class, zero_division=0)\n",
    "\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_true_class, y_pred_proba_class)\n",
    "    except:\n",
    "        roc_auc = 0.0\n",
    "\n",
    "    per_class_results[disease_name] = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'support': support\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{disease_name}:\")  # Just \"Healthy\"\n",
    "    print(f\"  Support:   {support:4d}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "    print(f\"  ROC-AUC:   {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xu81HSFRhct0"
   },
   "source": [
    "## 9. Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50XcNaV1hct1"
   },
   "outputs": [],
   "source": [
    "def show_prediction_with_names(sample_idx, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Display prediction with DISEASE NAMES ONLY (no ICD codes).\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Sample {sample_idx + 1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # Get data\n",
    "    test_sample_idx = test_idx[sample_idx]\n",
    "    y_true = y_true_test[sample_idx]\n",
    "    y_pred_proba = y_pred_proba_test[sample_idx]\n",
    "\n",
    "    # True labels - DISEASE NAMES ONLY\n",
    "    true_indices = np.where(y_true == 1)[0]\n",
    "    print(f\"\\nTrue Diagnoses ({len(true_indices)}):\")\n",
    "    if len(true_indices) > 0:\n",
    "        for idx in true_indices:\n",
    "            print(f\"  - {DISEASE_NAMES[idx]}\")  # ONLY disease name\n",
    "    else:\n",
    "        print(\"  - None\")\n",
    "\n",
    "    # Predicted labels - DISEASE NAMES ONLY\n",
    "    pred_indices = np.where(y_pred_proba > threshold)[0]\n",
    "    print(f\"\\nPredicted Diagnoses (threshold={threshold}):\")\n",
    "    if len(pred_indices) > 0:\n",
    "        for idx in sorted(pred_indices, key=lambda i: -y_pred_proba[i]):\n",
    "            prob = y_pred_proba[idx]\n",
    "            marker = \"✓\" if y_true[idx] == 1 else \" \"\n",
    "            print(f\"  {marker} {DISEASE_NAMES[idx]}: {prob:.4f}\")  # ONLY disease name\n",
    "    else:\n",
    "        print(\"  - None\")\n",
    "\n",
    "    # Top 5 predictions - DISEASE NAMES ONLY\n",
    "    print(f\"\\nTop 5 Predictions (all probabilities):\")\n",
    "    top_5 = np.argsort(y_pred_proba)[-5:][::-1]\n",
    "    for idx in top_5:\n",
    "        prob = y_pred_proba[idx]\n",
    "        marker = \"✓\" if y_true[idx] == 1 else \" \"\n",
    "        print(f\"  {marker} {DISEASE_NAMES[idx]}: {prob:.4f}\")  # ONLY disease name\n",
    "\n",
    "\n",
    "# Show predictions for random samples\n",
    "print(\"\\nSample Predictions:\")\n",
    "random_samples = np.random.choice(len(test_idx), size=min(5, len(test_idx)), replace=False)\n",
    "for i in random_samples:\n",
    "    show_prediction_with_names(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvJwzZAwhct1"
   },
   "outputs": [],
   "source": [
    "# Plot ECG signal with predictions\n",
    "def plot_ecg_with_predictions(sample_idx, num_leads=3):\n",
    "    \"\"\"\n",
    "    Plot ECG signal with true and predicted labels (DISEASE NAMES ONLY).\n",
    "    \"\"\"\n",
    "    test_sample_idx = test_idx[sample_idx]\n",
    "    ecg_signal = X_data[test_sample_idx]\n",
    "    y_true = y_true_test[sample_idx]\n",
    "    y_pred_proba = y_pred_proba_test[sample_idx]\n",
    "\n",
    "    # Get disease names\n",
    "    true_labels = [DISEASE_NAMES[j] for j in range(NUM_CLASSES) if y_true[j] == 1]\n",
    "    pred_labels = [(DISEASE_NAMES[j], y_pred_proba[j])\n",
    "                  for j in range(NUM_CLASSES) if y_pred_proba[j] > 0.5]\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(num_leads, 1, figsize=(15, 8))\n",
    "    if num_leads == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    time = np.arange(len(ecg_signal)) / TARGET_FS\n",
    "    lead_names = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "\n",
    "    for i in range(num_leads):\n",
    "        axes[i].plot(time, ecg_signal[:, i], linewidth=0.8)\n",
    "        axes[i].set_ylabel(f'Lead {lead_names[i]}', fontsize=10)\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[-1].set_xlabel('Time (s)', fontsize=10)\n",
    "\n",
    "    # Title with disease names\n",
    "    title = f'Sample {sample_idx + 1} - ECG Signal ({len(ecg_signal)/TARGET_FS:.1f}s)\\n'\n",
    "    title += f'True: {\", \".join(true_labels) if true_labels else \"None\"}\\n'\n",
    "    if pred_labels:\n",
    "        pred_str = \", \".join([f\"{l} ({p:.2f})\" for l, p in pred_labels])\n",
    "        title += f'Predicted: {pred_str}'\n",
    "    else:\n",
    "        title += 'Predicted: None'\n",
    "\n",
    "    plt.suptitle(title, fontsize=11, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot a few examples\n",
    "print(\"\\nECG Signal Visualizations:\")\n",
    "for i in random_samples[:3]:\n",
    "    plot_ecg_with_predictions(i, num_leads=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VkCK1Vwhct1"
   },
   "source": [
    "## 10. Save Model & Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukWA3vrnhct1"
   },
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model_path = os.path.join(OUTPUT_DIR, f'{MODEL_NAME}_final.keras')\n",
    "model.save(final_model_path)\n",
    "print(f\"✓ Model saved to {final_model_path}\")\n",
    "\n",
    "# Save comprehensive metadata\n",
    "from datetime import datetime\n",
    "\n",
    "results_metadata = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'version': NOTEBOOK_VERSION,\n",
    "    'created': datetime.now().isoformat(),\n",
    "\n",
    "    # Classes\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'disease_names': DISEASE_NAMES,\n",
    "    'icd_codes': ICD_CODES,\n",
    "    'icd_to_disease_name': ICD_TO_DISEASE_NAME,\n",
    "    'disease_groups': DISEASE_GROUPS,\n",
    "\n",
    "    # Architecture\n",
    "    'architecture': {\n",
    "        'type': 'Enhanced 1D CNN',\n",
    "        'blocks': '64→128→256→512 filters',\n",
    "        'features': ['Squeeze-Excitation', 'Temporal Attention', 'Residual Connections'],\n",
    "        'pooling': 'Adaptive (Global Avg + Max)',\n",
    "        'total_params': int(model.count_params())\n",
    "    },\n",
    "\n",
    "    # Training\n",
    "    'training': {\n",
    "        'epochs': len(history.history['loss']),\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'loss': 'Focal Loss (alpha=0.25, gamma=2.0)',\n",
    "        'optimizer': 'Adam'\n",
    "    },\n",
    "\n",
    "    # Data\n",
    "    'data': {\n",
    "        'total_samples': len(X_data),\n",
    "        'train_samples': len(train_idx),\n",
    "        'val_samples': len(val_idx),\n",
    "        'test_samples': len(test_idx),\n",
    "        'sampling_rate': TARGET_FS,\n",
    "        'num_channels': TARGET_CHANNELS,\n",
    "        'variable_length': True,\n",
    "        'length_range': f\"{lengths_data.min()/TARGET_FS:.1f}s - {lengths_data.max()/TARGET_FS:.1f}s\"\n",
    "    },\n",
    "\n",
    "    # Test metrics\n",
    "    'test_metrics': {\n",
    "        'subset_accuracy': float(subset_acc),\n",
    "        'hamming_loss': float(hamming),\n",
    "        'f1_micro': float(f1_micro),\n",
    "        'f1_macro': float(f1_macro),\n",
    "        'f1_weighted': float(f1_weighted),\n",
    "        'precision_micro': float(prec_micro),\n",
    "        'recall_micro': float(rec_micro)\n",
    "    },\n",
    "\n",
    "    # Per-class results\n",
    "    'per_class_results': per_class_results\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = os.path.join(OUTPUT_DIR, 'model_results.json')\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(results_metadata, f, indent=2)\n",
    "\n",
    "print(f\"✓ Metadata saved to {metadata_path}\")\n",
    "\n",
    "# Upload to HuggingFace (Colab only)\n",
    "if IN_COLAB and MODEL_REPO and api:\n",
    "    print(f\"\\nUploading final model to HuggingFace...\")\n",
    "\n",
    "    try:\n",
    "        # Upload model\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=final_model_path,\n",
    "            path_in_repo=\"final_model.keras\",\n",
    "            repo_id=MODEL_REPO,\n",
    "            repo_type=\"model\",\n",
    "            commit_message=\"Final trained model\"\n",
    "        )\n",
    "        print(\"✓ Final model uploaded\")\n",
    "\n",
    "        # Upload metadata\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=metadata_path,\n",
    "            path_in_repo=\"model_results.json\",\n",
    "            repo_id=MODEL_REPO,\n",
    "            repo_type=\"model\",\n",
    "            commit_message=\"Add model metadata\"\n",
    "        )\n",
    "        print(\"✓ Metadata uploaded\")\n",
    "\n",
    "        # Upload training history\n",
    "        history_csv = os.path.join(OUTPUT_DIR, 'training_history.csv')\n",
    "        if os.path.exists(history_csv):\n",
    "            api.upload_file(\n",
    "                path_or_fileobj=history_csv,\n",
    "                path_in_repo=\"training_history.csv\",\n",
    "                repo_id=MODEL_REPO,\n",
    "                repo_type=\"model\",\n",
    "                commit_message=\"Add training history\"\n",
    "            )\n",
    "            print(\"✓ Training history uploaded\")\n",
    "\n",
    "        # Upload training plot\n",
    "        history_plot = os.path.join(OUTPUT_DIR, 'training_history.png')\n",
    "        if os.path.exists(history_plot):\n",
    "            api.upload_file(\n",
    "                path_or_fileobj=history_plot,\n",
    "                path_in_repo=\"training_history.png\",\n",
    "                repo_id=MODEL_REPO,\n",
    "                repo_type=\"model\",\n",
    "                commit_message=\"Add training visualization\"\n",
    "            )\n",
    "            print(\"✓ Training visualization uploaded\")\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"✓ All artifacts uploaded to HuggingFace!\")\n",
    "        print(f\"View at: https://huggingface.co/{MODEL_REPO}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not upload to HF: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: {final_model_path}\")\n",
    "print(f\"Metadata: {metadata_path}\")\n",
    "print(f\"\\nTest F1-Score (Macro): {f1_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vz9p81ghct2"
   },
   "source": [
    "## 11. Generate Model Card (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZL0FHMu7hct2"
   },
   "outputs": [],
   "source": [
    "# Generate model card for HuggingFace (Colab only)\n",
    "if IN_COLAB and MODEL_REPO:\n",
    "    disease_names_str = '\\n'.join([f\"{i+1}. {name}\" for i, name in enumerate(DISEASE_NAMES)])\n",
    "\n",
    "    model_card = f\"\"\"---\n",
    "language: en\n",
    "tags:\n",
    "- ecg\n",
    "- cardiology\n",
    "- medical\n",
    "- pediatric\n",
    "- time-series\n",
    "- multi-label-classification\n",
    "- tensorflow\n",
    "- cnn\n",
    "datasets:\n",
    "- Neural-Network-Project/ECG-database\n",
    "metrics:\n",
    "- f1\n",
    "- auc\n",
    "- precision\n",
    "- recall\n",
    "library_name: tensorflow\n",
    "---\n",
    "\n",
    "# ECG Disease Classifier - 19 Cardiac Conditions\n",
    "\n",
    "Multi-label classification model for detecting 19 cardiac conditions from pediatric ECG signals.\n",
    "\n",
    "## Model Description\n",
    "\n",
    "Enhanced 1D CNN with Squeeze-Excitation blocks and temporal attention for variable-length ECG classification.\n",
    "\n",
    "**Architecture:** 64→128→256→512 filters with residual connections\n",
    "**Training:** Focal loss for class imbalance\n",
    "**Input:** Variable-length 12-lead ECG (5-120 seconds at 500 Hz)\n",
    "\n",
    "## Disease Classes\n",
    "\n",
    "{disease_names_str}\n",
    "\n",
    "## Performance\n",
    "\n",
    "- F1-Score (Macro): {f1_macro:.4f}\n",
    "- F1-Score (Micro): {f1_micro:.4f}\n",
    "- Subset Accuracy: {subset_acc:.4f}\n",
    "\n",
    "## Intended Use\n",
    "\n",
    "⚠️ **Research and educational purposes only** - NOT for clinical diagnosis\n",
    "\n",
    "## Training Details\n",
    "\n",
    "- Batch Size: {BATCH_SIZE}\n",
    "- Epochs: {len(history.history['loss'])}\n",
    "- Loss: Focal Loss (α=0.25, γ=2.0)\n",
    "- Optimizer: Adam (lr={LEARNING_RATE})\n",
    "\n",
    "## Citation\n",
    "\n",
    "```bibtex\n",
    "@misc{{ecg-classifier-2025,\n",
    "  author = {{Neural-Network-Project}},\n",
    "  title = {{ECG Disease Classifier}},\n",
    "  year = {{2025}},\n",
    "  publisher = {{Hugging Face}},\n",
    "  url = {{https://huggingface.co/{MODEL_REPO}}}\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    # Save locally\n",
    "    readme_path = os.path.join(OUTPUT_DIR, 'README.md')\n",
    "    with open(readme_path, 'w') as f:\n",
    "        f.write(model_card)\n",
    "\n",
    "    print(\"✓ Model card created\")\n",
    "\n",
    "    # Upload to HuggingFace\n",
    "    try:\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=readme_path,\n",
    "            path_in_repo=\"README.md\",\n",
    "            repo_id=MODEL_REPO,\n",
    "            repo_type=\"model\",\n",
    "            commit_message=\"Add model card\"\n",
    "        )\n",
    "        print(\"✓ Model card uploaded to HuggingFace\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not upload model card: {e}\")\n",
    "else:\n",
    "    print(\"Skipping model card generation (not in Colab)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zfNRi70hct2"
   },
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements an enhanced 1D CNN for 19-class multi-label ECG classification:\n",
    "\n",
    "**Key Features:**\n",
    "- ✅ Processes full-length variable recordings (5-120 seconds, no windowing)\n",
    "- ✅ 19 classes (18 cardiac diseases + Healthy)\n",
    "- ✅ Enhanced architecture with SE blocks and temporal attention\n",
    "- ✅ Focal loss for class imbalance\n",
    "- ✅ **All outputs show disease names only** (not ICD codes)\n",
    "\n",
    "**Architecture:**\n",
    "- 64→128→256→512 filters with residual connections\n",
    "- Squeeze-Excitation blocks for channel attention\n",
    "- Temporal attention for time-series focus\n",
    "- Adaptive pooling for variable lengths\n",
    "\n",
    "**Next Steps:**\n",
    "- Fine-tune threshold per class\n",
    "- Analyze attention weights\n",
    "- Deploy for inference"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "41e8679e55bb49b8a749b1c24c10a266": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45a2ee4ed74645f7aeb027e305150349": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "589fa537e4744dfea0d6c0da7bf4034c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "62dd2f76305e4d5c8f62ee064b22bb42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8ebc5563012943088ded136823c447a8",
      "placeholder": "​",
      "style": "IPY_MODEL_8b3ea80c1bf14b0e826182345b835a6a",
      "value": " 10/10 [00:00&lt;00:00, 406.16it/s]"
     }
    },
    "6ec89b2b3f2a4577b75ace8656ea3774": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7bb1de3cdae6411dae0d1948126a0841": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45a2ee4ed74645f7aeb027e305150349",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6ec89b2b3f2a4577b75ace8656ea3774",
      "value": 10
     }
    },
    "8b3ea80c1bf14b0e826182345b835a6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8eb45b9ff94a4ac4b41075703f8ffba1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ebc5563012943088ded136823c447a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d24c3c21ff7942a5ad80b3bb659d5631": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f2003522041b4e2e8b6ec8cd3a85df3d",
       "IPY_MODEL_7bb1de3cdae6411dae0d1948126a0841",
       "IPY_MODEL_62dd2f76305e4d5c8f62ee064b22bb42"
      ],
      "layout": "IPY_MODEL_8eb45b9ff94a4ac4b41075703f8ffba1"
     }
    },
    "f2003522041b4e2e8b6ec8cd3a85df3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_41e8679e55bb49b8a749b1c24c10a266",
      "placeholder": "​",
      "style": "IPY_MODEL_589fa537e4744dfea0d6c0da7bf4034c",
      "value": "Fetching 10 files: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}