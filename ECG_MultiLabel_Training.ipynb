{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECG Multi-Label Model Training (Self-Contained)\n",
    "Fully independent notebook: loads preprocessed data, trains multi-label model, saves results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow scikit-learn matplotlib seaborn huggingface-hub -q\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hamming_loss, precision_score, recall_score, f1_score\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"GPU: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect environment\n",
    "IN_COLAB = False\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    drive.mount('/content/drive/')\nexcept:\n",
    "    pass\n",
    "\n# Set paths\nif IN_COLAB:\n",
    "    DATA_DIR = \"/content/drive/MyDrive/DeepLearningECG/artifacts/multilabel_v2\"\n",
    "    RESULTS_DIR = \"/content/drive/MyDrive/DeepLearningECG/results/\"\nelse:\n",
    "    DATA_DIR = \"../DeepLearningECG/artifacts/multilabel_v2\"\n",
    "    RESULTS_DIR = \"../DeepLearningECG/results/\"\n\nos.makedirs(RESULTS_DIR, exist_ok=True)\n\nprint(f\"Colab: {IN_COLAB}\")\nprint(f\"DATA_DIR: {DATA_DIR}\")\nprint(f\"RESULTS_DIR: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from local or Hugging Face\nif os.path.exists(DATA_DIR):\n",
    "    print(f\"Loading from local: {DATA_DIR}\")\n",
    "    X = np.load(os.path.join(DATA_DIR, \"X_windows.npy\"))\n",
    "    y = np.load(os.path.join(DATA_DIR, \"y_labels_onehot.npy\"))\n",
    "    with open(os.path.join(DATA_DIR, \"disease_classes.json\")) as f:\n",
    "        class_info = json.load(f)\nelse:\n",
    "    print(\"Downloading from Hugging Face...\")\n",
    "    from huggingface_hub import hf_hub_download\n",
    "    \n",
    "    X_file = hf_hub_download(\"kiril-buga/ECG-database\", \"multilabel_v2/X_windows.npy\", repo_type=\"dataset\")\n",
    "    y_file = hf_hub_download(\"kiril-buga/ECG-database\", \"multilabel_v2/y_labels_onehot.npy\", repo_type=\"dataset\")\n",
    "    class_file = hf_hub_download(\"kiril-buga/ECG-database\", \"multilabel_v2/disease_classes.json\", repo_type=\"dataset\")\n",
    "    \n",
    "    X = np.load(X_file)\n",
    "    y = np.load(y_file)\n",
    "    with open(class_file) as f:\n",
    "        class_info = json.load(f)\n",
    "\nDISEASE_CLASSES = class_info[\"classes\"]\n\nprint(f\"✓ Loaded data\")\nprint(f\"  X: {X.shape}\")\nprint(f\"  y: {y.shape}\")\nprint(f\"  Classes: {DISEASE_CLASSES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation & Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For patient-level splitting (assumes qc_summary available)\ntry:\n",
    "    qc_file = os.path.join(DATA_DIR, \"qc_summary.csv\") if os.path.exists(DATA_DIR) else None\n",
    "    if qc_file and os.path.exists(qc_file):\n",
    "        df_qc = pd.read_csv(qc_file)\n",
    "        df_qc['Patient_ID'] = df_qc['Filename'].str.split('/').str[1]\n",
    "        \n",
    "        # Map windows to patients\n",
    "        window_patients = []\n",
    "        for _, row in df_qc.iterrows():\n",
    "            n_windows = int(row['n_windows']) if pd.notna(row['n_windows']) else 0\n",
    "            window_patients.extend([row['Patient_ID']] * n_windows)\n",
    "        window_patients = np.array(window_patients)\n",
    "        \n",
    "        # Patient-level split\n",
    "        unique_patients = np.unique(window_patients)\n",
    "        train_pat, test_pat = train_test_split(unique_patients, test_size=0.2, random_state=42)\n",
    "        train_pat, val_pat = train_test_split(train_pat, test_size=0.25, random_state=42)\n",
    "        \n",
    "        train_idx = np.where(np.isin(window_patients, train_pat))[0]\n",
    "        val_idx = np.where(np.isin(window_patients, val_pat))[0]\n",
    "        test_idx = np.where(np.isin(window_patients, test_pat))[0]\n",
    "    else:\n",
    "        raise FileNotFoundError()\nexcept:\n",
    "    print(\"No patient mapping available, using random split\")\n",
    "    all_idx = np.arange(len(X))\n",
    "    train_idx, test_idx = train_test_split(all_idx, test_size=0.2, random_state=42)\n",
    "    train_idx, val_idx = train_test_split(train_idx, test_size=0.25, random_state=42)\n",
    "\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\n\nprint(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\nprint(f\"\\nClass distribution (test):\")\nfor i, cls in enumerate(DISEASE_CLASSES):\n",
    "    count = y_test[:, i].sum()\n",
    "    print(f\"  {cls}: {count} ({100*count/len(y_test):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build & Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, num_classes):\n",
    "    return keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Conv1D(32, 3, padding='same', activation='relu'),\n",
    "        layers.Conv1D(32, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Conv1D(64, 3, padding='same', activation='relu'),\n",
    "        layers.Conv1D(64, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Conv1D(128, 3, padding='same', activation='relu'),\n",
    "        layers.Conv1D(128, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Conv1D(256, 3, padding='same', activation='relu'),\n",
    "        layers.Conv1D(256, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='sigmoid')\n",
    "    ])\n",
    "\nmodel = build_model((X_train.shape[1], X_train.shape[2]), len(DISEASE_CLASSES))\nmodel.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n\nprint(\"Model:\")\nmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nmodel_path = os.path.join(RESULTS_DIR, f\"model_{timestamp}.keras\")\n\ncallbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6),\n",
    "    keras.callbacks.ModelCheckpoint(model_path, monitor='val_binary_accuracy', save_best_only=True)\n",
    "]\n",
    "\nprint(\"Training...\")\nhistory = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\nprint(f\"✓ Model saved: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs = model.predict(X_test, verbose=0)\ny_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\nprint(\"\\n\" + \"=\"*70)\nprint(\"TEST SET METRICS\")\nprint(\"=\"*70)\n\nprint(f\"\\nHamming Loss: {hamming_loss(y_test, y_pred):.4f}\")\nprint(f\"Exact Match Accuracy: {np.mean(np.all(y_test == y_pred, axis=1)):.4f}\")\n\nprint(f\"\\nPer-Class Metrics:\")\nprint(f\"{'Class':<20} {'Precision':>12} {'Recall':>12} {'F1-Score':>12}\")\nprint(\"-\" * 70)\n\nfor i, cls in enumerate(DISEASE_CLASSES):\n",
    "    p = precision_score(y_test[:, i], y_pred[:, i], zero_division=0)\n",
    "    r = recall_score(y_test[:, i], y_pred[:, i], zero_division=0)\n",
    "    f = f1_score(y_test[:, i], y_pred[:, i], zero_division=0)\n",
    "    print(f\"{cls:<20} {p:>12.4f} {r:>12.4f} {f:>12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(history.history['loss'], label='Train', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Val', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss'); axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history.history['binary_accuracy'], label='Train', linewidth=2)\n",
    "axes[1].plot(history.history['val_binary_accuracy'], label='Val', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy'); axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
    "\nplt.tight_layout()\nplt.savefig(os.path.join(RESULTS_DIR, f'training_{timestamp}.png'), dpi=150, bbox_inches='tight')\nprint(f\"✓ Plot saved\")\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "np.savez(\n",
    "    os.path.join(RESULTS_DIR, f\"predictions_{timestamp}.npz\"),\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    y_probs=y_pred_probs\n",
    ")\n",
    "\n# Save results JSON\nresults = {\n",
    "    'timestamp': timestamp,\n",
    "    'disease_classes': DISEASE_CLASSES,\n",
    "    'epochs_trained': len(history.history['loss']),\n",
    "    'train_samples': X_train.shape[0],\n",
    "    'val_samples': X_val.shape[0],\n",
    "    'test_samples': X_test.shape[0],\n",
    "}\n",
    "\nwith open(os.path.join(RESULTS_DIR, f\"results_{timestamp}.json\"), \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\nprint(f\"✓ Results saved to {RESULTS_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
