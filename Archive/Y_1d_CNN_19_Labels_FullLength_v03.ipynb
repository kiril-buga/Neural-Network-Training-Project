{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced 1D CNN for 19-Class Multi-Label ECG Classification (v03)\n",
    "\n",
    "**Key Features:**\n",
    "- 19-class multi-label classification (18 cardiac diseases + Healthy)\n",
    "- **Full-length variable recordings** (5-120 seconds, no windowing)\n",
    "- Enhanced architecture: Squeeze-Excitation blocks + Temporal Attention\n",
    "- Focal loss for class imbalance\n",
    "- **Outputs show disease names only** (not ICD codes)\n",
    "\n",
    "**Architecture:** 64→128→256→512 filters + SE + Attention + Adaptive Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dependencies installed\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install wfdb neurokit2 h5py tensorflow scikit-learn matplotlib seaborn pandas numpy scipy -q\n",
    "\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "Num GPUs Available: 0\n",
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import wfdb\n",
    "from scipy.signal import butter, filtfilt, resample\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, hamming_loss, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Num GPUs Available: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 GPU Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using float32 (mixed precision disabled for compatibility)\n",
      "⚠️  WARNING: No GPU detected! Training will be VERY slow.\n",
      "   Current TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "# GPU Configuration - Optimized for GTX 1050 4GB\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "import os\n",
    "\n",
    "# Additional cuDNN configuration to prevent CUDNN_STATUS_INTERNAL_ERROR\n",
    "os.environ['TF_CUDNN_WORKSPACE_LIMIT_IN_MB'] = '512'  # Limit cuDNN workspace to 512MB\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices=false'  # Disable XLA (can cause issues on older GPUs)\n",
    "\n",
    "# Set cuDNN environment variables for GTX 1050 compatibility\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'  # Use deterministic algorithms\n",
    "os.environ['TF_CUDNN_USE_AUTOTUNE'] = '0'  # Disable autotuning (can cause issues)\n",
    "\n",
    "# DO NOT use mixed precision on GTX 1050 - causes cuDNN errors\n",
    "# mixed_precision.set_global_policy('mixed_float16')  # DISABLED\n",
    "print(\"✓ Using float32 (mixed precision disabled for compatibility)\")\n",
    "\n",
    "# Enable memory growth\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"✓ GPU memory growth enabled: {gpus[0].name}\")\n",
    "        print(f\"✓ Num GPUs Available: {len(gpus)}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU configuration error: {e}\")\n",
    "else:\n",
    "    print(\"⚠️  WARNING: No GPU detected! Training will be VERY slow.\")\n",
    "    print(f\"   Current TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️  GPU troubleshooting options available (see cell comments)\n"
     ]
    }
   ],
   "source": [
    "# ALTERNATIVE: If GPU still fails, set memory limit or force CPU\n",
    "\n",
    "# Option 1: Set explicit GPU memory limit (use only 2GB of your 4GB GPU)\n",
    "# Uncomment these lines if the error persists:\n",
    "\n",
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#     try:\n",
    "#         tf.config.set_logical_device_configuration(\n",
    "#             gpus[0],\n",
    "#             [tf.config.LogicalDeviceConfiguration(memory_limit=2048)])  # 2GB limit\n",
    "#         print(\"✓ GPU memory limited to 2048MB\")\n",
    "#     except RuntimeError as e:\n",
    "#         print(f\"Memory limit error: {e}\")\n",
    "\n",
    "# Option 2: Force CPU-only mode (FALLBACK if GPU continues to fail)\n",
    "# Uncomment this line to disable GPU entirely:\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "# print(\"⚠️  GPU DISABLED - Using CPU only (will be slower)\")\n",
    "\n",
    "print(\"ℹ️  GPU troubleshooting options available (see cell comments)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 HuggingFace Setup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kiril\\Documents\\VSCode Projects\\Neural-Network-Project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFace Setup\n",
      "============================================================\n",
      "Please provide your Hugging Face token with WRITE access:\n",
      "Get it from: https://huggingface.co/settings/tokens\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Logged in to Hugging Face\n",
      "✓ Model repo exists: Neural-Network-Project/ECG-Disease-Classifier\n",
      "✓ Dataset repo exists: Neural-Network-Project/ECG-database\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace Setup (optional - set ENABLE_HF_UPLOAD=True to enable)\n",
    "# Install huggingface_hub if not already installed\n",
    "try:\n",
    "    from huggingface_hub import HfApi, login, create_repo\n",
    "except ImportError:\n",
    "    print(\"Installing huggingface_hub...\")\n",
    "    import subprocess\n",
    "    subprocess.run(['.venv\\\\Scripts\\\\python.exe', '-m', 'pip', 'install', 'huggingface-hub', '-q'], check=True)\n",
    "    from huggingface_hub import HfApi, login, create_repo\n",
    "\n",
    "# Set this to True to enable HuggingFace uploads\n",
    "ENABLE_HF_UPLOAD = True  # Change to True to enable uploads\n",
    "\n",
    "if ENABLE_HF_UPLOAD:\n",
    "    from getpass import getpass\n",
    "    import os\n",
    "    \n",
    "    print(\"HuggingFace Setup\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Please provide your Hugging Face token with WRITE access:\")\n",
    "    print(\"Get it from: https://huggingface.co/settings/tokens\")\n",
    "    print()\n",
    "    \n",
    "    # Try to get token from environment variable first\n",
    "    HF_TOKEN = os.environ.get('HF_TOKEN')\n",
    "    \n",
    "    if not HF_TOKEN:\n",
    "        HF_TOKEN = getpass(\"Enter your HF token (hidden): \")\n",
    "    \n",
    "    # Login\n",
    "    try:\n",
    "        login(token=HF_TOKEN, add_to_git_credential=True)\n",
    "        print(\"✓ Logged in to Hugging Face\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Login failed: {e}\")\n",
    "        print(\"HuggingFace uploads will be disabled\")\n",
    "        ENABLE_HF_UPLOAD = False\n",
    "    \n",
    "    if ENABLE_HF_UPLOAD:\n",
    "        # Repository names\n",
    "        MODEL_REPO = \"Neural-Network-Project/ECG-Disease-Classifier\"\n",
    "        DATASET_REPO = \"Neural-Network-Project/ECG-database\"\n",
    "        \n",
    "        # Initialize API\n",
    "        api = HfApi()\n",
    "        \n",
    "        # Ensure repos exist\n",
    "        try:\n",
    "            api.repo_info(repo_id=MODEL_REPO, repo_type=\"model\")\n",
    "            print(f\"✓ Model repo exists: {MODEL_REPO}\")\n",
    "        except:\n",
    "            try:\n",
    "                create_repo(repo_id=MODEL_REPO, repo_type=\"model\", private=False)\n",
    "                print(f\"✓ Created model repo: {MODEL_REPO}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  Could not create model repo: {e}\")\n",
    "        \n",
    "        try:\n",
    "            api.repo_info(repo_id=DATASET_REPO, repo_type=\"dataset\")\n",
    "            print(f\"✓ Dataset repo exists: {DATASET_REPO}\")\n",
    "        except:\n",
    "            try:\n",
    "                create_repo(repo_id=DATASET_REPO, repo_type=\"dataset\", private=False)\n",
    "                print(f\"✓ Created dataset repo: {DATASET_REPO}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  Could not create dataset repo: {e}\")\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"HuggingFace uploads disabled (set ENABLE_HF_UPLOAD=True to enable)\")\n",
    "    MODEL_REPO = None\n",
    "    DATASET_REPO = None\n",
    "    api = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Data directory: ../.data/\n",
      "  Output directory: ../.data/artifacts/multilabel_v3_balanced/\n",
      "  Model name: Enhanced_1D_CNN_19Classes_v3_balanced\n",
      "  Max sequence length: 15000 samples (30.0s)\n",
      "  Batch size: 4\n",
      "  Epochs: 50\n",
      "  Checkpoint every: 5 epochs (HuggingFace)\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "NOTEBOOK_VERSION = \"v03_balanced\"  # Updated to distinguish from original\n",
    "DATA_DIR = \"../.data/\"\n",
    "ECG_DIR = os.path.join(DATA_DIR, \"data/Child_ecg/\")\n",
    "CSV_PATH = os.path.join(DATA_DIR, \"data/AttributesDictionary.csv\")\n",
    "OUTPUT_DIR = os.path.join(DATA_DIR, \"artifacts/multilabel_v3_balanced/\")  # New output directory\n",
    "HDF5_FILE = os.path.join(DATA_DIR, \"artifacts/multilabel_v3_full_length/ecg_full_length_19classes.h5\")  # Still use original HDF5\n",
    "MODEL_NAME = \"Enhanced_1D_CNN_19Classes_v3_balanced\"  # Updated model name\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Training parameters - OPTIMIZED FOR GTX 1050 Ti 4GB VRAM\n",
    "MAX_LENGTH = 15000  # Maximum sequence length (30 seconds at 500Hz) to prevent OOM\n",
    "BATCH_SIZE = 4  # Further reduced for 4GB VRAM with long sequences\n",
    "GRADIENT_ACCUMULATION_STEPS = 2  # Accumulate gradients to simulate batch_size=8\n",
    "# NOTE: Gradient accumulation requires custom training loop (not implemented yet)\n",
    "# For now, using batch_size=4 directly provides stable memory usage\n",
    "EPOCHS = 50  # Reduced from 100 (balanced data trains faster)\n",
    "LEARNING_RATE = 1e-3\n",
    "TARGET_FS = 500  # Target sampling frequency\n",
    "TARGET_CHANNELS = 12\n",
    "CHECKPOINT_EVERY = 5  # Save checkpoint every 5 epochs (faster than 10)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Data directory: {DATA_DIR}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Model name: {MODEL_NAME}\")\n",
    "print(f\"  Max sequence length: {MAX_LENGTH} samples ({MAX_LENGTH/TARGET_FS:.1f}s)\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "# print(f\"  Gradient accumulation steps: {GRADIENT_ACCUMULATION_STEPS} (effective batch_size={BATCH_SIZE*GRADIENT_ACCUMULATION_STEPS})\")  # Not yet implemented\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Checkpoint every: {CHECKPOINT_EVERY} epochs (HuggingFace)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Extract Split Archive (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking ECG data extraction...\n",
      "  ECG directory: ../.data/data/Child_ecg/\n",
      "  Files found: 56761\n",
      "✓ ECG data already extracted (56761 files found)\n"
     ]
    }
   ],
   "source": [
    "# Extract split archives if needed (local environment)\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Check if Child_ecg directory exists and has files\n",
    "def count_files_recursive(directory):\n",
    "    \"\"\"Count all files recursively in a directory\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        return 0\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        count += len(files)\n",
    "    return count\n",
    "\n",
    "ecg_file_count = count_files_recursive(ECG_DIR)\n",
    "\n",
    "print(f\"Checking ECG data extraction...\")\n",
    "print(f\"  ECG directory: {ECG_DIR}\")\n",
    "print(f\"  Files found: {ecg_file_count}\")\n",
    "\n",
    "# If directory doesn't exist or has very few files, extract the archives\n",
    "if ecg_file_count < 1000:  # Expect tens of thousands of files\n",
    "    print(f\"\\n⚠️  Insufficient ECG files detected. Checking for archives to extract...\")\n",
    "    \n",
    "    # Look for split archive files\n",
    "    archive_dir = os.path.join(DATA_DIR, \"data\")\n",
    "    zip_file = os.path.join(archive_dir, \"Child_ecg.zip\")\n",
    "    z01_file = os.path.join(archive_dir, \"Child_ecg.z01\")\n",
    "    \n",
    "    if os.path.exists(zip_file):\n",
    "        print(f\"\\n✓ Found archive files:\")\n",
    "        print(f\"  - {zip_file}\")\n",
    "        if os.path.exists(z01_file):\n",
    "            print(f\"  - {z01_file}\")\n",
    "        \n",
    "        print(f\"\\nExtracting archives...\")\n",
    "        print(f\"This may take several minutes...\")\n",
    "        \n",
    "        try:\n",
    "            # For split archives (.z01 + .zip), we need to use 7zip or similar\n",
    "            if os.path.exists(z01_file):\n",
    "                # Try using 7zip if available\n",
    "                try:\n",
    "                    import subprocess\n",
    "                    # Try 7z command on Windows (usually installed with 7-Zip)\n",
    "                    result = subprocess.run(\n",
    "                        ['7z', 'x', zip_file, f'-o{archive_dir}', '-y'],\n",
    "                        capture_output=True,\n",
    "                        text=True\n",
    "                    )\n",
    "                    if result.returncode == 0:\n",
    "                        print(f\"✓ Successfully extracted using 7zip\")\n",
    "                    else:\n",
    "                        raise Exception(\"7zip extraction failed\")\n",
    "                except:\n",
    "                    # Fallback: Try PowerShell Expand-Archive (may not work with split archives)\n",
    "                    print(\"7zip not available, trying PowerShell...\")\n",
    "                    result = subprocess.run(\n",
    "                        ['powershell', '-Command', \n",
    "                         f'Expand-Archive -Path \"{zip_file}\" -DestinationPath \"{archive_dir}\" -Force'],\n",
    "                        capture_output=True,\n",
    "                        text=True\n",
    "                    )\n",
    "                    if result.returncode == 0:\n",
    "                        print(f\"✓ Extraction attempted with PowerShell\")\n",
    "                    else:\n",
    "                        print(f\"PowerShell extraction failed, trying Python zipfile...\")\n",
    "                        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "                            zip_ref.extractall(archive_dir)\n",
    "                        print(f\"✓ Extraction attempted with zipfile\")\n",
    "            else:\n",
    "                # Single zip file, use Python's zipfile\n",
    "                with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(archive_dir)\n",
    "                print(f\"✓ Successfully extracted zip file\")\n",
    "            \n",
    "            # Verify extraction\n",
    "            new_file_count = count_files_recursive(ECG_DIR)\n",
    "            print(f\"\\n✓ Extraction complete!\")\n",
    "            print(f\"  Files extracted: {new_file_count}\")\n",
    "            \n",
    "            if new_file_count < 1000:\n",
    "                print(f\"\\n⚠️  Warning: Expected more files. Archive may not have extracted completely.\")\n",
    "                print(f\"  Please manually extract {zip_file} using 7-Zip if needed.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error during extraction: {e}\")\n",
    "            print(f\"\\nPlease manually extract the following file:\")\n",
    "            print(f\"  {zip_file}\")\n",
    "            print(f\"To directory: {archive_dir}\")\n",
    "            print(f\"\\nYou can use 7-Zip (https://www.7-zip.org/) to extract split archives.\")\n",
    "            raise\n",
    "    else:\n",
    "        print(f\"\\n❌ Archive files not found!\")\n",
    "        print(f\"  Expected: {zip_file}\")\n",
    "        print(f\"\\nPlease ensure the archive files are present in {archive_dir}\")\n",
    "        raise FileNotFoundError(f\"Archive not found: {zip_file}\")\n",
    "else:\n",
    "    print(f\"✓ ECG data already extracted ({ecg_file_count} files found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Disease Mapping & Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 19\n",
      "\n",
      "Disease Names (outputs will show these names ONLY):\n",
      "   1. Fulminant/Viral Myocarditis\n",
      "   2. Acute Myocarditis\n",
      "   3. Myocarditis Unspecified\n",
      "   4. Dilated Cardiomyopathy\n",
      "   5. Hypertrophic Cardiomyopathy\n",
      "   6. Cardiomyopathy Unspecified\n",
      "   7. Noncompaction Ventricular Myocardium\n",
      "   8. Kawasaki Disease\n",
      "   9. Ventricular Septal Defect\n",
      "  10. Atrial Septal Defect\n",
      "  11. Atrioventricular Septal Defect\n",
      "  12. Tetralogy of Fallot\n",
      "  13. Pulmonary Valve Stenosis\n",
      "  14. Patent Ductus Arteriosus\n",
      "  15. Pulmonary Artery Stenosis\n",
      "  16. Pulmonary Valve Regurgitation\n",
      "  17. Mitral Valve Insufficiency\n",
      "  18. Congenital Heart Malformation\n",
      "  19. Healthy\n",
      "\n",
      "Disease Groups:\n",
      "  Myocarditis: 3 diseases\n",
      "  Cardiomyopathy: 4 diseases\n",
      "  Kawasaki: 1 diseases\n",
      "  CHD: 10 diseases\n"
     ]
    }
   ],
   "source": [
    "# ICD codes to Disease Names mapping\n",
    "ICD_TO_DISEASE_NAME = {\n",
    "    'I40.0': 'Fulminant/Viral Myocarditis',\n",
    "    'I40.9': 'Acute Myocarditis',\n",
    "    'I51.4': 'Myocarditis Unspecified',\n",
    "    'I42.0': 'Dilated Cardiomyopathy',\n",
    "    'I42.2': 'Hypertrophic Cardiomyopathy',\n",
    "    'I42.9': 'Cardiomyopathy Unspecified',\n",
    "    'Q24.8': 'Noncompaction Ventricular Myocardium',\n",
    "    'M30.3': 'Kawasaki Disease',\n",
    "    'Q21.0': 'Ventricular Septal Defect',\n",
    "    'Q21.1': 'Atrial Septal Defect',\n",
    "    'Q21.2': 'Atrioventricular Septal Defect',\n",
    "    'Q21.3': 'Tetralogy of Fallot',\n",
    "    'Q22.1': 'Pulmonary Valve Stenosis',\n",
    "    'Q25.0': 'Patent Ductus Arteriosus',\n",
    "    'Q25.6': 'Pulmonary Artery Stenosis',\n",
    "    'I37.0': 'Pulmonary Valve Regurgitation',\n",
    "    'I34.0': 'Mitral Valve Insufficiency',\n",
    "    'Q24.9': 'Congenital Heart Malformation',\n",
    "    'Healthy': 'Healthy'  # Changed from \"No Disease (Healthy)\" to just \"Healthy\"\n",
    "}\n",
    "\n",
    "# Disease names list (for model outputs - NO ICD codes in outputs)\n",
    "DISEASE_NAMES = list(ICD_TO_DISEASE_NAME.values())\n",
    "ICD_CODES = list(ICD_TO_DISEASE_NAME.keys())\n",
    "NUM_CLASSES = len(DISEASE_NAMES)\n",
    "\n",
    "# Disease grouping for interpretability\n",
    "DISEASE_GROUPS = {\n",
    "    'Myocarditis': ['I40.0', 'I40.9', 'I51.4'],\n",
    "    'Cardiomyopathy': ['I42.0', 'I42.2', 'I42.9', 'Q24.8'],\n",
    "    'Kawasaki': ['M30.3'],\n",
    "    'CHD': ['Q21.0', 'Q21.1', 'Q21.2', 'Q21.3', 'Q22.1', 'Q25.0', 'Q25.6', 'I37.0', 'I34.0', 'Q24.9']\n",
    "}\n",
    "\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"\\nDisease Names (outputs will show these names ONLY):\")\n",
    "for i, name in enumerate(DISEASE_NAMES, 1):\n",
    "    print(f\"  {i:2d}. {name}\")\n",
    "\n",
    "print(f\"\\nDisease Groups:\")\n",
    "for group, codes in DISEASE_GROUPS.items():\n",
    "    names = [ICD_TO_DISEASE_NAME[code] for code in codes]\n",
    "    print(f\"  {group}: {len(codes)} diseases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Preprocessing functions defined\n"
     ]
    }
   ],
   "source": [
    "def apply_bandpass(x, fs, lowcut=0.5, highcut=40.0):\n",
    "    \"\"\"Apply bandpass filter to entire ECG signal.\"\"\"\n",
    "    if x.ndim == 1:\n",
    "        x = x[:, None]\n",
    "    nyq = 0.5 * fs\n",
    "    b, a = butter(4, [lowcut/nyq, highcut/nyq], btype=\"band\")\n",
    "    return np.column_stack([filtfilt(b, a, x[:, i]) for i in range(x.shape[1])])\n",
    "\n",
    "\n",
    "def process_full_recording(sig, meta, target_fs=500.0, target_channels=12, max_length=None):\n",
    "    \"\"\"\n",
    "    Process full ECG recording without windowing.\n",
    "    Handles variable lengths (5-120 seconds), with optional truncation.\n",
    "    \n",
    "    Args:\n",
    "        sig: ECG signal\n",
    "        meta: Metadata with sampling frequency\n",
    "        target_fs: Target sampling frequency (Hz)\n",
    "        target_channels: Target number of channels\n",
    "        max_length: Maximum length in samples (default: None = no truncation)\n",
    "    \n",
    "    Returns:\n",
    "        sig_processed: (n_samples, 12) - variable length\n",
    "        length: actual sample count\n",
    "    \"\"\"\n",
    "    # Get sampling frequency\n",
    "    fs = meta.get(\"fs\", None)\n",
    "    if fs is None:\n",
    "        fs = getattr(meta, \"fs\", None)\n",
    "    if fs is None:\n",
    "        raise ValueError(\"Missing sampling frequency\")\n",
    "    \n",
    "    if sig.ndim == 1:\n",
    "        sig = sig[:, None]\n",
    "    \n",
    "    # 1. Bandpass filter entire signal (0.5-40 Hz)\n",
    "    sig_bp = apply_bandpass(sig, fs=fs)\n",
    "    \n",
    "    # 2. Resample to target_fs if needed\n",
    "    if fs != target_fs:\n",
    "        n_samples = sig_bp.shape[0]\n",
    "        n_new = int(round(n_samples / fs * target_fs))\n",
    "        sig_res = np.column_stack([resample(sig_bp[:, i], n_new) \n",
    "                                   for i in range(sig_bp.shape[1])])\n",
    "    else:\n",
    "        sig_res = sig_bp\n",
    "    \n",
    "    # 3. Truncate if max_length specified (MEMORY OPTIMIZATION)\n",
    "    if max_length is not None and sig_res.shape[0] > max_length:\n",
    "        sig_res = sig_res[:max_length, :]\n",
    "    \n",
    "    # 4. Standardize to 12 leads (pad with zeros if fewer)\n",
    "    if sig_res.shape[1] < target_channels:\n",
    "        padded = np.zeros((sig_res.shape[0], target_channels), dtype=np.float32)\n",
    "        padded[:, :sig_res.shape[1]] = sig_res\n",
    "        sig_res = padded\n",
    "    elif sig_res.shape[1] > target_channels:\n",
    "        sig_res = sig_res[:, :target_channels]\n",
    "    \n",
    "    # 5. Z-score normalization per lead\n",
    "    sig_norm = sig_res.copy()\n",
    "    for ch in range(target_channels):\n",
    "        x = sig_norm[:, ch]\n",
    "        m, s = np.nanmean(x), np.nanstd(x)\n",
    "        sig_norm[:, ch] = (x - m) / (s if s > 1e-6 else 1.0)\n",
    "    \n",
    "    actual_length = sig_norm.shape[0]\n",
    "    \n",
    "    return sig_norm.astype(np.float16), actual_length\n",
    "\n",
    "\n",
    "def parse_icd_codes(s):\n",
    "    \"\"\"Parse semicolon-separated ICD codes from CSV.\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return []\n",
    "    return [p.strip().replace(\"'\", \"\").replace(\")\", \"\").split(\")\")[-1] \n",
    "            for p in str(s).split(\";\") if p.strip()]\n",
    "\n",
    "\n",
    "print(\"✓ Preprocessing functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata...\n",
      "Loaded 14190 ECG records\n",
      "Sample ICD codes: ['I34.0', 'Q21.0', 'Q24.9']\n"
     ]
    }
   ],
   "source": [
    "# Load CSV metadata\n",
    "print(\"Loading metadata...\")\n",
    "df_attr = pd.read_csv(CSV_PATH)\n",
    "print(f\"Loaded {len(df_attr)} ECG records\")\n",
    "\n",
    "# Parse ICD codes\n",
    "df_attr[\"ICD_list\"] = df_attr[\"ICD-10 code\"].apply(parse_icd_codes)\n",
    "print(f\"Sample ICD codes: {df_attr['ICD_list'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found existing HDF5 file: ../.data/artifacts/multilabel_v3_full_length/ecg_full_length_19classes.h5\n",
      "Loading preprocessed data...\n",
      "\n",
      "HDF5 Contents:\n",
      "  Bins: ['bin_0', 'bin_1', 'bin_2', 'bin_3']\n",
      "  bin_0: 7 samples\n",
      "  bin_1: 4663 samples\n",
      "  bin_2: 9479 samples\n",
      "  bin_3: 40 samples\n",
      "\n",
      "  Total samples: 14189\n",
      "  Number of classes: 19\n",
      "  Sampling rate: 500 Hz\n"
     ]
    }
   ],
   "source": [
    "# Check if preprocessed HDF5 exists\n",
    "if os.path.exists(HDF5_FILE):\n",
    "    print(f\"✓ Found existing HDF5 file: {HDF5_FILE}\")\n",
    "    print(\"Loading preprocessed data...\")\n",
    "    \n",
    "    with h5py.File(HDF5_FILE, 'r') as h5f:\n",
    "        print(f\"\\nHDF5 Contents:\")\n",
    "        print(f\"  Bins: {list(h5f.keys())}\")\n",
    "        \n",
    "        total_samples = 0\n",
    "        for bin_name in h5f.keys():\n",
    "            if bin_name.startswith('bin_'):\n",
    "                grp = h5f[bin_name]\n",
    "                bin_samples = grp['X'].shape[0]\n",
    "                total_samples += bin_samples\n",
    "                print(f\"  {bin_name}: {bin_samples} samples\")\n",
    "        \n",
    "        print(f\"\\n  Total samples: {total_samples}\")\n",
    "        print(f\"  Number of classes: {h5f.attrs['num_classes']}\")\n",
    "        print(f\"  Sampling rate: {h5f.attrs['sampling_rate']} Hz\")\n",
    "\n",
    "else:\n",
    "    print(f\"HDF5 file not found. Creating preprocessed dataset...\")\n",
    "    print(f\"This may take 30-60 minutes...\\n\")\n",
    "    \n",
    "    # Process all recordings\n",
    "    recordings = []\n",
    "    labels = []\n",
    "    lengths = []\n",
    "    filenames = []\n",
    "    \n",
    "    print(\"Processing ECG recordings...\")\n",
    "    for idx, row in df_attr.iterrows():\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print(f\"  Processed {idx + 1}/{len(df_attr)} records...\")\n",
    "        \n",
    "        fname = row[\"Filename\"]\n",
    "        path = os.path.join(ECG_DIR, fname)\n",
    "        \n",
    "        try:\n",
    "            sig, meta = wfdb.rdsamp(path)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        meta_dict = meta if isinstance(meta, dict) else meta.__dict__\n",
    "        sig = np.asarray(sig)\n",
    "        \n",
    "        # Process full recording\n",
    "        try:\n",
    "            sig_proc, length = process_full_recording(sig, meta_dict, max_length=MAX_LENGTH)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        # Create multi-label vector\n",
    "        label_vec = np.zeros(NUM_CLASSES, dtype=np.int32)\n",
    "        icd_list = row[\"ICD_list\"]\n",
    "        \n",
    "        if not icd_list or all(pd.isna(icd_list)):\n",
    "            # Healthy case\n",
    "            label_vec[ICD_CODES.index('Healthy')] = 1\n",
    "        else:\n",
    "            # Disease case\n",
    "            found_disease = False\n",
    "            for icd in icd_list:\n",
    "                if icd in ICD_CODES:\n",
    "                    label_vec[ICD_CODES.index(icd)] = 1\n",
    "                    found_disease = True\n",
    "            \n",
    "            # If no matching disease found, mark as healthy\n",
    "            if not found_disease:\n",
    "                label_vec[ICD_CODES.index('Healthy')] = 1\n",
    "        \n",
    "        recordings.append(sig_proc)\n",
    "        labels.append(label_vec)\n",
    "        lengths.append(length)\n",
    "        filenames.append(fname)\n",
    "    \n",
    "    print(f\"\\n✓ Processed {len(recordings)} valid recordings\")\n",
    "    \n",
    "    # Save to HDF5 with length binning\n",
    "    print(\"\\nSaving to HDF5 with length binning...\")\n",
    "    \n",
    "    labels_array = np.array(labels)\n",
    "    lengths_array = np.array(lengths)\n",
    "    \n",
    "    # Define bins: <10s, 10-30s, 30-60s, 60-120s (in samples @ 500Hz)\n",
    "    length_bins = [(0, 5000), (5000, 15000), (15000, 30000), (30000, 60000)]\n",
    "    \n",
    "    with h5py.File(HDF5_FILE, 'w') as h5f:\n",
    "        for bin_idx, (min_len, max_len) in enumerate(length_bins):\n",
    "            mask = (lengths_array >= min_len) & (lengths_array < max_len)\n",
    "            if not np.any(mask):\n",
    "                continue\n",
    "            \n",
    "            bin_recordings = [recordings[i] for i in np.where(mask)[0]]\n",
    "            bin_labels = labels_array[mask]\n",
    "            bin_lengths = lengths_array[mask]\n",
    "            bin_filenames = [filenames[i] for i in np.where(mask)[0]]\n",
    "            \n",
    "            # Pad to max length in this bin\n",
    "            max_bin_len = bin_lengths.max()\n",
    "            padded_sigs = np.zeros((len(bin_recordings), max_bin_len, 12), dtype=np.float16)\n",
    "            for i, rec in enumerate(bin_recordings):\n",
    "                padded_sigs[i, :len(rec), :] = rec\n",
    "            \n",
    "            # Create group\n",
    "            grp = h5f.create_group(f'bin_{bin_idx}')\n",
    "            grp.create_dataset('X', data=padded_sigs, compression='gzip', compression_opts=4)\n",
    "            grp.create_dataset('y', data=bin_labels, compression='gzip', compression_opts=4)\n",
    "            grp.create_dataset('lengths', data=bin_lengths, compression='gzip', compression_opts=4)\n",
    "            grp.attrs['min_len'] = min_len\n",
    "            grp.attrs['max_len'] = max_len\n",
    "            grp.attrs['count'] = len(bin_recordings)\n",
    "            \n",
    "            print(f\"  bin_{bin_idx} ({min_len/500:.1f}-{max_len/500:.1f}s): {len(bin_recordings)} samples\")\n",
    "        \n",
    "        # Global metadata\n",
    "        h5f.attrs['num_classes'] = NUM_CLASSES\n",
    "        h5f.attrs['disease_names'] = DISEASE_NAMES  # Store disease names\n",
    "        h5f.attrs['icd_codes'] = ICD_CODES\n",
    "        h5f.attrs['sampling_rate'] = TARGET_FS\n",
    "        h5f.attrs['target_channels'] = TARGET_CHANNELS\n",
    "        h5f.attrs['total_samples'] = len(recordings)\n",
    "    \n",
    "    print(f\"\\n✓ Saved to {HDF5_FILE}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'disease_names': DISEASE_NAMES,\n",
    "        'icd_codes': ICD_CODES,\n",
    "        'icd_to_disease_name': ICD_TO_DISEASE_NAME,\n",
    "        'disease_groups': DISEASE_GROUPS,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'sampling_rate': TARGET_FS,\n",
    "        'target_channels': TARGET_CHANNELS,\n",
    "        'created': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(OUTPUT_DIR, 'metadata.json'), 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Saved metadata to {OUTPUT_DIR}/metadata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Enhanced Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model building blocks defined\n"
     ]
    }
   ],
   "source": [
    "def se_block(input_tensor, ratio=16):\n",
    "    \"\"\"\n",
    "    Squeeze-Excitation block for channel attention.\n",
    "    Recalibrates channel-wise feature responses.\n",
    "    \"\"\"\n",
    "    channels = input_tensor.shape[-1]\n",
    "    \n",
    "    # Squeeze: Global pooling\n",
    "    se = layers.GlobalAveragePooling1D()(input_tensor)\n",
    "    \n",
    "    # Excitation: FC → ReLU → FC → Sigmoid\n",
    "    se = layers.Dense(channels // ratio, activation='relu')(se)\n",
    "    se = layers.Dense(channels, activation='sigmoid')(se)\n",
    "    \n",
    "    # Scale: Multiply original features by learned weights\n",
    "    se = layers.Reshape((1, channels))(se)\n",
    "    return layers.Multiply()([input_tensor, se])\n",
    "\n",
    "\n",
    "def residual_conv_block(x, filters, kernel_size=3, pool_size=2, dropout=0.2):\n",
    "    \"\"\"\n",
    "    Convolutional block with residual connection and SE attention.\n",
    "    \"\"\"\n",
    "    shortcut = x\n",
    "    \n",
    "    # Conv layers\n",
    "    x = layers.Conv1D(filters, kernel_size, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    x = layers.Conv1D(filters, kernel_size, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # SE block\n",
    "    x = se_block(x, ratio=16)\n",
    "    \n",
    "    # Residual connection (match dimensions if needed)\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv1D(filters, 1, padding='same')(shortcut)\n",
    "    \n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling1D(pool_size)(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def temporal_attention_layer(x):\n",
    "    \"\"\"\n",
    "    Self-attention mechanism over temporal dimension.\n",
    "    Helps model focus on important time segments (P wave, QRS, T wave).\n",
    "    \"\"\"\n",
    "    filters = x.shape[-1]\n",
    "    \n",
    "    # Query, Key, Value projections\n",
    "    query = layers.Conv1D(filters, 1, padding='same')(x)\n",
    "    key = layers.Conv1D(filters, 1, padding='same')(x)\n",
    "    value = layers.Conv1D(filters, 1, padding='same')(x)\n",
    "    \n",
    "    # Attention scores: Q·K^T / sqrt(d_k)\n",
    "    attention_scores = layers.Dot(axes=[2, 2])([query, key])\n",
    "    attention_scores = layers.Lambda(lambda z: z / np.sqrt(float(filters)), output_shape=lambda s: s)(attention_scores)\n",
    "    attention_weights = layers.Softmax(axis=-1)(attention_scores)\n",
    "    \n",
    "    # Weighted sum: Attention·V\n",
    "    attended = layers.Dot(axes=[2, 1])([attention_weights, value])\n",
    "    \n",
    "    # Residual connection\n",
    "    output = layers.Add()([x, attended])\n",
    "    output = layers.LayerNormalization()(output)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def adaptive_pooling_layer(x):\n",
    "    \"\"\"\n",
    "    Combines multiple pooling strategies to get fixed-size representation.\n",
    "    Works with any input length.\n",
    "    \"\"\"\n",
    "    # Global Average Pooling\n",
    "    gap = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Global Max Pooling\n",
    "    gmp = layers.GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    # Concatenate both\n",
    "    return layers.Concatenate()([gap, gmp])  # Output: (2 * filters,)\n",
    "\n",
    "\n",
    "print(\"✓ Model building blocks defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Model Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Enhanced_1D_CNN_v3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Enhanced_1D_CNN_v3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ ecg_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,440</span> │ ecg_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">41,088</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> │ activation[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,032</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,152</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multiply (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│                     │                   │            │ reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│                     │                   │            │ conv1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,560</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv1d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_2        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │ activation_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv1d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,112</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,352</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multiply_1          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)          │                   │            │ reshape_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ conv1d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_3        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">393,728</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ conv1d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_4        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">786,944</span> │ activation_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ conv1d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,416</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,896</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multiply_2          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)          │                   │            │ reshape_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ conv1d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_5        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">786,944</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ conv1d_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_6        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,416</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,896</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multiply_3          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)          │                   │            │ reshape_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dot (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │ <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)             │            │ conv1d_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dot[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "│                     │ <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ softmax (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Softmax</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│                     │ <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dot_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ softmax[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│                     │                   │            │ conv1d_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ dot_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dense_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ disease_output      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,883</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ ecg_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │      \u001b[38;5;34m5,440\u001b[0m │ ecg_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ max_pooling1d[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │     \u001b[38;5;34m41,088\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │        \u001b[38;5;34m512\u001b[0m │ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │     \u001b[38;5;34m82,048\u001b[0m │ activation[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │        \u001b[38;5;34m512\u001b[0m │ conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │      \u001b[38;5;34m1,032\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │      \u001b[38;5;34m1,152\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape (\u001b[38;5;33mReshape\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multiply (\u001b[38;5;33mMultiply\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│                     │                   │            │ reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │      \u001b[38;5;34m8,320\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ multiply[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│                     │                   │            │ conv1d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ activation_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ max_pooling1d_1[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │     \u001b[38;5;34m98,560\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │      \u001b[38;5;34m1,024\u001b[0m │ conv1d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_2        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │    \u001b[38;5;34m196,864\u001b[0m │ activation_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │      \u001b[38;5;34m1,024\u001b[0m │ conv1d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │      \u001b[38;5;34m4,112\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m4,352\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_1 (\u001b[38;5;33mReshape\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multiply_1          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiply\u001b[0m)          │                   │            │ reshape_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │     \u001b[38;5;34m33,024\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ multiply_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ conv1d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_3        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ activation_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ max_pooling1d_2[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │    \u001b[38;5;34m393,728\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │      \u001b[38;5;34m2,048\u001b[0m │ conv1d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_4        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_8 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │    \u001b[38;5;34m786,944\u001b[0m │ activation_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │      \u001b[38;5;34m2,048\u001b[0m │ conv1d_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │     \u001b[38;5;34m16,416\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │     \u001b[38;5;34m16,896\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_2 (\u001b[38;5;33mReshape\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multiply_2          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiply\u001b[0m)          │                   │            │ reshape_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_9 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │    \u001b[38;5;34m131,584\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_2 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ multiply_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ conv1d_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_5        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ activation_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ max_pooling1d_3[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_10 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │    \u001b[38;5;34m786,944\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │      \u001b[38;5;34m2,048\u001b[0m │ conv1d_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_6        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ activation_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │     \u001b[38;5;34m16,416\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │     \u001b[38;5;34m16,896\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_3 (\u001b[38;5;33mReshape\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multiply_3          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ activation_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mMultiply\u001b[0m)          │                   │            │ reshape_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_3 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ multiply_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_11 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │    \u001b[38;5;34m262,656\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_12 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │    \u001b[38;5;34m262,656\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dot (\u001b[38;5;33mDot\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ conv1d_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │ \u001b[38;5;45mNone\u001b[0m)             │            │ conv1d_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ dot[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "│                     │ \u001b[38;5;45mNone\u001b[0m)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ softmax (\u001b[38;5;33mSoftmax\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│                     │ \u001b[38;5;45mNone\u001b[0m)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_13 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │    \u001b[38;5;34m262,656\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dot_1 (\u001b[38;5;33mDot\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ softmax[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│                     │                   │            │ conv1d_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_4 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ dot_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │      \u001b[38;5;34m1,024\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │    \u001b[38;5;34m524,800\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │      \u001b[38;5;34m2,048\u001b[0m │ dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m131,328\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ dense_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ disease_output      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m)        │      \u001b[38;5;34m4,883\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,104,363</span> (15.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,104,363\u001b[0m (15.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,098,091</span> (15.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,098,091\u001b[0m (15.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,272</span> (24.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m6,272\u001b[0m (24.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DIAGNOSTIC TEST: Testing on CPU first...\n",
      "============================================================\n",
      "✓ CPU Test PASSED: (1, 2500, 12) → (1, 19)\n",
      "  Model architecture is correct!\n",
      "============================================================\n",
      "Clearing GPU memory...\n",
      "============================================================\n",
      "WARNING:tensorflow:From c:\\Users\\kiril\\Documents\\VSCode Projects\\Neural-Network-Project\\.venv\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "⚠ Could not reset GPU memory stats (this is normal)\n",
      "Rebuilding model after memory clear...\n",
      "============================================================\n",
      "Testing variable-length inputs on GPU:\n",
      "============================================================\n",
      "  ✓ Input: (1, 500, 12) → Output: (1, 19)\n",
      "  ✓ Input: (1, 1000, 12) → Output: (1, 19)\n",
      "  ✓ Input: (1, 2500, 12) → Output: (1, 19)\n",
      "  ✓ Input: (1, 5000, 12) → Output: (1, 19)\n",
      "✓ Diagnostic complete!\n"
     ]
    }
   ],
   "source": [
    "def build_enhanced_1d_cnn(num_classes=19):\n",
    "    \"\"\"\n",
    "    Build enhanced 1D CNN for variable-length ECG classification.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input: (None, 12) - variable time steps\n",
    "    - 4 residual blocks with SE attention: 64→128→256→512\n",
    "    - Temporal attention layer\n",
    "    - Adaptive pooling (variable → fixed)\n",
    "    - Dense layers with dropout\n",
    "    - Multi-label sigmoid output\n",
    "    \"\"\"\n",
    "    # Input: (None, 12) where None = variable time steps\n",
    "    inputs = layers.Input(shape=(None, 12), name='ecg_input')\n",
    "    \n",
    "    # Initial conv\n",
    "    x = layers.Conv1D(64, 7, padding='same', activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Block 1: 128 filters\n",
    "    x = residual_conv_block(x, 128, kernel_size=5, pool_size=2, dropout=0.2)\n",
    "    \n",
    "    # Block 2: 256 filters\n",
    "    x = residual_conv_block(x, 256, kernel_size=3, pool_size=2, dropout=0.3)\n",
    "    \n",
    "    # Block 3: 512 filters\n",
    "    x = residual_conv_block(x, 512, kernel_size=3, pool_size=2, dropout=0.3)\n",
    "    \n",
    "    # Block 4: 512 filters (no pooling)\n",
    "    shortcut = x\n",
    "    x = layers.Conv1D(512, 3, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = se_block(x, ratio=16)\n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Temporal attention\n",
    "    x = temporal_attention_layer(x)\n",
    "    \n",
    "    # Adaptive pooling (variable → fixed size)\n",
    "    x = adaptive_pooling_layer(x)  # Output: (1024,) = 2*512\n",
    "    \n",
    "    # Dense layers\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # Multi-label output (19 classes, sigmoid)\n",
    "    outputs = layers.Dense(num_classes, activation='sigmoid', name='disease_output')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name='Enhanced_1D_CNN_v3')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Build model\n",
    "print(\"Building model...\")\n",
    "model = build_enhanced_1d_cnn(num_classes=NUM_CLASSES)\n",
    "\n",
    "print(\"Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# === DIAGNOSTIC: Test on CPU first ===\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"DIAGNOSTIC TEST: Testing on CPU first...\")\n",
    "print(\"=\"*60)\n",
    "try:\n",
    "    with tf.device('/CPU:0'):\n",
    "        test_input = np.random.randn(1, 2500, 12).astype(np.float32)\n",
    "        output = model(test_input, training=False)\n",
    "        print(f\"✓ CPU Test PASSED: {test_input.shape} → {output.shape}\")\n",
    "        print(\"  Model architecture is correct!\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ CPU Test FAILED: {e}\")\n",
    "    print(\"  Issue is with model architecture, not GPU!\")\n",
    "\n",
    "# === Clear GPU memory before GPU test ===\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"Clearing GPU memory...\")\n",
    "print(\"=\"*60)\n",
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n",
    "try:\n",
    "    tf.config.experimental.reset_memory_stats('GPU:0')\n",
    "    print(\"✓ GPU memory cleared\")\n",
    "except:\n",
    "    print(\"⚠ Could not reset GPU memory stats (this is normal)\")\n",
    "\n",
    "# Rebuild model after clearing session\n",
    "print(\"Rebuilding model after memory clear...\")\n",
    "model = build_enhanced_1d_cnn(num_classes=NUM_CLASSES)\n",
    "\n",
    "# === Test GPU with progressively longer sequences ===\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"Testing variable-length inputs on GPU:\")\n",
    "print(\"=\"*60)\n",
    "test_lengths = [500, 1000, 2500, 5000]  # Start with shorter sequences\n",
    "for length in test_lengths:\n",
    "    try:\n",
    "        dummy_input = np.random.randn(1, length, 12).astype(np.float32)\n",
    "        output = model(dummy_input, training=False)\n",
    "        print(f\"  ✓ Input: {dummy_input.shape} → Output: {output.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Failed at length {length}: {str(e)[:100]}\")\n",
    "        print(f\"     GPU cannot handle sequences of length {length}+\")\n",
    "        break\n",
    "\n",
    "print(\"✓ Diagnostic complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Focal Loss & Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Focal loss implemented\n"
     ]
    }
   ],
   "source": [
    "class FocalLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"\n",
    "    Focal loss for handling class imbalance.\n",
    "    FL(p_t) = -alpha_t * (1 - p_t)^gamma * log(p_t)\n",
    "    \n",
    "    Focuses on hard-to-classify examples.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        # Clip predictions to prevent log(0)\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Binary cross entropy\n",
    "        bce = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n",
    "        \n",
    "        # Focal term: (1 - p_t)^gamma\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        focal_weight = tf.pow(1 - p_t, self.gamma)\n",
    "        \n",
    "        # Apply alpha weighting\n",
    "        alpha_weight = y_true * self.alpha + (1 - y_true) * (1 - self.alpha)\n",
    "        \n",
    "        # Combine\n",
    "        focal_loss = alpha_weight * focal_weight * bce\n",
    "        \n",
    "        return tf.reduce_mean(tf.reduce_sum(focal_loss, axis=-1))\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"alpha\": self.alpha, \"gamma\": self.gamma})\n",
    "        return config\n",
    "\n",
    "\n",
    "print(\"✓ Focal loss implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 HuggingFace Checkpoint Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ HuggingFaceCheckpoint callback defined\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace Checkpoint Callback (optional - requires ENABLE_HF_UPLOAD=True)\n",
    "if ENABLE_HF_UPLOAD and MODEL_REPO:\n",
    "    class HuggingFaceCheckpoint(tf.keras.callbacks.Callback):\n",
    "        \"\"\"\n",
    "        Save checkpoints to Hugging Face Hub every N epochs.\n",
    "        Handles training interruptions by allowing resume.\n",
    "        \"\"\"\n",
    "        def __init__(self, repo_id, every_n_epochs=10):\n",
    "            super().__init__()\n",
    "            self.repo_id = repo_id\n",
    "            self.every_n_epochs = every_n_epochs\n",
    "            self.api = HfApi()\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            if (epoch + 1) % self.every_n_epochs == 0:\n",
    "                epoch_num = epoch + 1\n",
    "\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"Epoch {epoch_num}: Saving checkpoint to Hugging Face...\")\n",
    "                print(f\"{'='*60}\")\n",
    "\n",
    "                # Save model locally\n",
    "                checkpoint_path = os.path.join(OUTPUT_DIR, f\"checkpoint_epoch_{epoch_num}.keras\")\n",
    "                self.model.save(checkpoint_path, save_format='keras')\n",
    "\n",
    "                # Upload to HuggingFace\n",
    "                try:\n",
    "                    self.api.upload_file(\n",
    "                        path_or_fileobj=checkpoint_path,\n",
    "                        path_in_repo=f\"checkpoints/checkpoint_epoch_{epoch_num}.keras\",\n",
    "                        repo_id=self.repo_id,\n",
    "                        repo_type=\"model\",\n",
    "                        commit_message=f\"Checkpoint at epoch {epoch_num}\"\n",
    "                    )\n",
    "                    print(f\"✓ Checkpoint uploaded: epoch {epoch_num}\")\n",
    "\n",
    "                    # Save metrics\n",
    "                    import json\n",
    "                    metrics = {\n",
    "                        'epoch': epoch_num,\n",
    "                        'loss': float(logs.get('loss', 0)),\n",
    "                        'val_loss': float(logs.get('val_loss', 0)),\n",
    "                        'accuracy': float(logs.get('accuracy', 0)),\n",
    "                        'val_accuracy': float(logs.get('val_accuracy', 0)),\n",
    "                        'auc': float(logs.get('auc', 0)),\n",
    "                        'val_auc': float(logs.get('val_auc', 0))\n",
    "                    }\n",
    "\n",
    "                    metrics_path = os.path.join(OUTPUT_DIR, f\"metrics_epoch_{epoch_num}.json\")\n",
    "                    with open(metrics_path, 'w') as f:\n",
    "                        json.dump(metrics, f, indent=2)\n",
    "\n",
    "                    self.api.upload_file(\n",
    "                        path_or_fileobj=metrics_path,\n",
    "                        path_in_repo=f\"checkpoints/metrics_epoch_{epoch_num}.json\",\n",
    "                        repo_id=self.repo_id,\n",
    "                        repo_type=\"model\",\n",
    "                        commit_message=f\"Metrics for epoch {epoch_num}\"\n",
    "                    )\n",
    "\n",
    "                    # Clean up local checkpoint (keep only metrics)\n",
    "                    os.remove(checkpoint_path)\n",
    "\n",
    "                    print(f\"✓ Metrics uploaded\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error uploading to HF: {e}\")\n",
    "                    # Keep local checkpoint if upload failed\n",
    "                    if os.path.exists(checkpoint_path):\n",
    "                        print(f\"✓ Local checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "                print(f\"{'='*60}\\n\")\n",
    "\n",
    "    print(\"✓ HuggingFaceCheckpoint callback defined\")\n",
    "else:\n",
    "    print(\"Skipping HF checkpoint (ENABLE_HF_UPLOAD=False or not configured)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Resume from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Training from scratch.\n"
     ]
    }
   ],
   "source": [
    "# Try to resume from last checkpoint (requires ENABLE_HF_UPLOAD=True)\n",
    "RESUME_TRAINING = False\n",
    "INITIAL_EPOCH = 0\n",
    "\n",
    "if ENABLE_HF_UPLOAD and MODEL_REPO:\n",
    "    def get_latest_checkpoint(repo_id):\n",
    "        try:\n",
    "            files = api.list_repo_files(repo_id=repo_id, repo_type=\"model\")\n",
    "            checkpoints = [f for f in files if f.startswith(\"checkpoints/checkpoint_epoch_\") and f.endswith(\".keras\")]\n",
    "\n",
    "            if not checkpoints:\n",
    "                return None, 0\n",
    "\n",
    "            epochs = [int(f.split(\"_\")[-1].replace(\".keras\", \"\")) for f in checkpoints]\n",
    "            latest_epoch = max(epochs)\n",
    "            latest_file = f\"checkpoints/checkpoint_epoch_{latest_epoch}.keras\"\n",
    "\n",
    "            return latest_file, latest_epoch\n",
    "        except Exception as e:\n",
    "            print(f\"Could not check for checkpoints: {e}\")\n",
    "            return None, 0\n",
    "\n",
    "    # Check for existing checkpoint\n",
    "    latest_checkpoint, start_epoch = get_latest_checkpoint(MODEL_REPO)\n",
    "\n",
    "    if latest_checkpoint:\n",
    "        print(f\"Found checkpoint: {latest_checkpoint}\")\n",
    "        print(f\"Resuming from epoch {start_epoch}\")\n",
    "\n",
    "        try:\n",
    "            from huggingface_hub import hf_hub_download\n",
    "            checkpoint_path = hf_hub_download(\n",
    "                repo_id=MODEL_REPO,\n",
    "                filename=latest_checkpoint,\n",
    "                repo_type=\"model\",\n",
    "                cache_dir=os.path.join(OUTPUT_DIR, \"hf_cache\")\n",
    "            )\n",
    "\n",
    "            # Load model - will be assigned after model architecture is defined\n",
    "            RESUME_TRAINING = True\n",
    "            INITIAL_EPOCH = start_epoch\n",
    "            RESUME_CHECKPOINT_PATH = checkpoint_path\n",
    "            print(\"✓ Will resume from checkpoint\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint: {e}\")\n",
    "            print(\"Will train from scratch\")\n",
    "    else:\n",
    "        print(\"No checkpoint found. Training from scratch.\")\n",
    "else:\n",
    "    print(\"Not using HuggingFace - training from scratch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Class Balancing & Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Augmentation functions defined\n"
     ]
    }
   ],
   "source": [
    "# ECG Data Augmentation Functions\n",
    "def augment_ecg_signal(ecg, augmentation_type='noise'):\n",
    "    \"\"\"\n",
    "    Apply augmentation to ECG signal.\n",
    "    \n",
    "    Types:\n",
    "    - 'noise': Add Gaussian noise\n",
    "    - 'scale': Amplitude scaling\n",
    "    - 'shift': Time shifting\n",
    "    - 'stretch': Time stretching\n",
    "    \"\"\"\n",
    "    ecg_aug = ecg.copy()\n",
    "    \n",
    "    if augmentation_type == 'noise':\n",
    "        # Add small Gaussian noise (SNR ~20-30dB)\n",
    "        noise_level = 0.05 * np.std(ecg_aug)\n",
    "        noise = np.random.normal(0, noise_level, ecg_aug.shape)\n",
    "        ecg_aug = ecg_aug + noise\n",
    "        \n",
    "    elif augmentation_type == 'scale':\n",
    "        # Amplitude scaling (0.9-1.1x)\n",
    "        scale_factor = np.random.uniform(0.9, 1.1)\n",
    "        ecg_aug = ecg_aug * scale_factor\n",
    "        \n",
    "    elif augmentation_type == 'shift':\n",
    "        # Time shift (up to 5% of length)\n",
    "        shift_samples = int(0.05 * len(ecg_aug) * np.random.uniform(-1, 1))\n",
    "        ecg_aug = np.roll(ecg_aug, shift_samples, axis=0)\n",
    "        \n",
    "    elif augmentation_type == 'stretch':\n",
    "        # Time stretching (0.95-1.05x speed)\n",
    "        from scipy.signal import resample\n",
    "        stretch_factor = np.random.uniform(0.95, 1.05)\n",
    "        new_length = int(len(ecg_aug) * stretch_factor)\n",
    "        ecg_stretched = np.column_stack([resample(ecg_aug[:, i], new_length) \n",
    "                                         for i in range(ecg_aug.shape[1])])\n",
    "        # Crop or pad to original length\n",
    "        if len(ecg_stretched) > len(ecg_aug):\n",
    "            start = (len(ecg_stretched) - len(ecg_aug)) // 2\n",
    "            ecg_aug = ecg_stretched[start:start+len(ecg_aug)]\n",
    "        else:\n",
    "            pad_before = (len(ecg_aug) - len(ecg_stretched)) // 2\n",
    "            pad_after = len(ecg_aug) - len(ecg_stretched) - pad_before\n",
    "            ecg_aug = np.pad(ecg_stretched, ((pad_before, pad_after), (0, 0)), mode='edge')\n",
    "    \n",
    "    return ecg_aug.astype(np.float16)\n",
    "\n",
    "\n",
    "print(\"✓ Augmentation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from HDF5...\n",
      "Loaded 14189 samples\n",
      "Length range: 5.0s - 112.0s\n",
      "Mean length: 27.0s\n",
      "\n",
      "================================================================================\n",
      "CLASS DISTRIBUTION - BEFORE BALANCING\n",
      "================================================================================\n",
      "Fulminant/Viral Myocarditis             :     0 samples\n",
      "Acute Myocarditis                       :    70 samples\n",
      "Myocarditis Unspecified                 :   186 samples\n",
      "Dilated Cardiomyopathy                  :    86 samples\n",
      "Hypertrophic Cardiomyopathy             :    24 samples\n",
      "Cardiomyopathy Unspecified              :    18 samples\n",
      "Noncompaction Ventricular Myocardium    :    22 samples\n",
      "Kawasaki Disease                        :   194 samples\n",
      "Ventricular Septal Defect               :  1062 samples\n",
      "Atrial Septal Defect                    :   936 samples\n",
      "Atrioventricular Septal Defect          :    61 samples\n",
      "Tetralogy of Fallot                     :    89 samples\n",
      "Pulmonary Valve Stenosis                :    24 samples\n",
      "Patent Ductus Arteriosus                :   348 samples\n",
      "Pulmonary Artery Stenosis               :    18 samples\n",
      "Pulmonary Valve Regurgitation           :    81 samples\n",
      "Mitral Valve Insufficiency              :   125 samples\n",
      "Congenital Heart Malformation           :   324 samples\n",
      "Healthy                                 : 11154 samples\n",
      "\n",
      "Total samples: 14189\n",
      "Healthy samples: 11154 (78.6%)\n",
      "Disease samples: 3035\n",
      "Largest disease class: 1062 samples\n"
     ]
    }
   ],
   "source": [
    "# Data loading function\n",
    "def load_data_from_hdf5(h5_file):\n",
    "    \"\"\"\n",
    "    Load all data from HDF5 file.\n",
    "    Returns X (list of variable-length arrays), y, lengths\n",
    "    \"\"\"\n",
    "    X_all = []\n",
    "    y_all = []\n",
    "    lengths_all = []\n",
    "    \n",
    "    with h5py.File(h5_file, 'r') as h5f:\n",
    "        for bin_name in sorted(h5f.keys()):\n",
    "            if not bin_name.startswith('bin_'):\n",
    "                continue\n",
    "            \n",
    "            grp = h5f[bin_name]\n",
    "            X_bin = grp['X'][:]\n",
    "            y_bin = grp['y'][:]\n",
    "            lengths_bin = grp['lengths'][:]\n",
    "            \n",
    "            # Trim each recording to actual length\n",
    "            for i in range(len(X_bin)):\n",
    "                X_all.append(X_bin[i, :lengths_bin[i], :])\n",
    "                y_all.append(y_bin[i])\n",
    "                lengths_all.append(lengths_bin[i])\n",
    "    \n",
    "    return X_all, np.array(y_all), np.array(lengths_all)\n",
    "\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data from HDF5...\")\n",
    "X_data_original, y_data_original, lengths_data_original = load_data_from_hdf5(HDF5_FILE)\n",
    "\n",
    "print(f\"Loaded {len(X_data_original)} samples\")\n",
    "print(f\"Length range: {lengths_data_original.min()/TARGET_FS:.1f}s - {lengths_data_original.max()/TARGET_FS:.1f}s\")\n",
    "print(f\"Mean length: {lengths_data_original.mean()/TARGET_FS:.1f}s\")\n",
    "\n",
    "# Analyze class distribution BEFORE balancing\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLASS DISTRIBUTION - BEFORE BALANCING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class_counts_before = y_data_original.sum(axis=0).astype(int)\n",
    "healthy_idx = ICD_CODES.index('Healthy')\n",
    "\n",
    "for i, (name, count) in enumerate(zip(DISEASE_NAMES, class_counts_before)):\n",
    "    print(f\"{name:40s}: {count:5d} samples\")\n",
    "\n",
    "print(f\"\\nTotal samples: {len(X_data_original)}\")\n",
    "print(f\"Healthy samples: {class_counts_before[healthy_idx]} ({100*class_counts_before[healthy_idx]/len(X_data_original):.1f}%)\")\n",
    "print(f\"Disease samples: {len(X_data_original) - class_counts_before[healthy_idx]}\")\n",
    "\n",
    "# Find largest disease class (excluding Healthy)\n",
    "disease_counts = [class_counts_before[i] for i in range(NUM_CLASSES) if i != healthy_idx]\n",
    "max_disease_count = max(disease_counts)\n",
    "print(f\"Largest disease class: {max_disease_count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "APPLYING CLASS BALANCING\n",
      "================================================================================\n",
      "\n",
      "Target distribution:\n",
      "  Healthy: 3186 samples (down from 11154)\n",
      "  Diseases: Augment to ~2124 samples each (where possible)\n",
      "\n",
      "1. Undersampling Healthy class...\n",
      "   Found 11154 healthy samples\n",
      "   ✓ Kept 3186 healthy samples\n",
      "\n",
      "2. Augmenting disease classes...\n",
      "   Acute Myocarditis                       :   70 →  280 samples (+210)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kiril\\Documents\\VSCode Projects\\Neural-Network-Project\\.venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:204: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Myocarditis Unspecified                 :  186 →  744 samples (+558)\n",
      "   Dilated Cardiomyopathy                  :   86 →  344 samples (+258)\n",
      "   Hypertrophic Cardiomyopathy             :   24 →   96 samples (+72)\n",
      "   Cardiomyopathy Unspecified              :   18 →   72 samples (+54)\n",
      "   Noncompaction Ventricular Myocardium    :   22 →   88 samples (+66)\n",
      "   Kawasaki Disease                        :  194 →  776 samples (+582)\n",
      "   Ventricular Septal Defect               : 1062 → 2124 samples (+1062)\n",
      "   Atrial Septal Defect                    :  936 → 2124 samples (+1188)\n",
      "   Atrioventricular Septal Defect          :   61 →  244 samples (+183)\n",
      "   Tetralogy of Fallot                     :   89 →  356 samples (+267)\n",
      "   Pulmonary Valve Stenosis                :   24 →   96 samples (+72)\n",
      "   Patent Ductus Arteriosus                :  348 → 1392 samples (+1044)\n",
      "   Pulmonary Artery Stenosis               :   18 →   72 samples (+54)\n",
      "   Pulmonary Valve Regurgitation           :   81 →  324 samples (+243)\n",
      "   Mitral Valve Insufficiency              :  125 →  500 samples (+375)\n",
      "   Congenital Heart Malformation           :  324 → 1296 samples (+972)\n",
      "\n",
      "✓ Balancing complete!\n",
      "  Total samples: 14114 (was 14189)\n",
      "\n",
      "================================================================================\n",
      "CLASS DISTRIBUTION - AFTER BALANCING\n",
      "================================================================================\n",
      "Fulminant/Viral Myocarditis             :     0 →     0 (   +0)\n",
      "Acute Myocarditis                       :    70 →   345 (+ +275)\n",
      "Myocarditis Unspecified                 :   186 →   751 (+ +565)\n",
      "Dilated Cardiomyopathy                  :    86 →   414 (+ +328)\n",
      "Hypertrophic Cardiomyopathy             :    24 →    98 (+  +74)\n",
      "Cardiomyopathy Unspecified              :    18 →    78 (+  +60)\n",
      "Noncompaction Ventricular Myocardium    :    22 →   105 (+  +83)\n",
      "Kawasaki Disease                        :   194 →   787 (+ +593)\n",
      "Ventricular Septal Defect               :  1062 →  3240 (++2178)\n",
      "Atrial Septal Defect                    :   936 →  3222 (++2286)\n",
      "Atrioventricular Septal Defect          :    61 →   407 (+ +346)\n",
      "Tetralogy of Fallot                     :    89 →   489 (+ +400)\n",
      "Pulmonary Valve Stenosis                :    24 →   160 (+ +136)\n",
      "Patent Ductus Arteriosus                :   348 →  1895 (++1547)\n",
      "Pulmonary Artery Stenosis               :    18 →   122 (+ +104)\n",
      "Pulmonary Valve Regurgitation           :    81 →   513 (+ +432)\n",
      "Mitral Valve Insufficiency              :   125 →   919 (+ +794)\n",
      "Congenital Heart Malformation           :   324 →  1971 (++1647)\n",
      "Healthy                                 : 11154 →  3186 (-7968)\n",
      "\n",
      "Total samples: 14114 (was 14189)\n",
      "Healthy: 3186 (22.6%)\n",
      "\n",
      "✓ Balanced dataset ready for training!\n"
     ]
    }
   ],
   "source": [
    "# CLASS BALANCING STRATEGY\n",
    "# 1. Undersample Healthy to 3x the largest disease class\n",
    "# 2. Augment disease classes to reach target count (50% of undersampled healthy)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"APPLYING CLASS BALANCING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Target counts\n",
    "target_healthy_count = max_disease_count * 3  # Undersample healthy to 3x largest disease\n",
    "target_disease_count = max_disease_count * 2  # Augment diseases to 2x their current size\n",
    "\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(f\"  Healthy: {target_healthy_count} samples (down from {class_counts_before[healthy_idx]})\")\n",
    "print(f\"  Diseases: Augment to ~{target_disease_count} samples each (where possible)\")\n",
    "\n",
    "# Initialize balanced dataset\n",
    "X_data_balanced = []\n",
    "y_data_balanced = []\n",
    "lengths_data_balanced = []\n",
    "\n",
    "# 1. UNDERSAMPLE HEALTHY CLASS\n",
    "print(f\"\\n1. Undersampling Healthy class...\")\n",
    "healthy_indices = [i for i in range(len(y_data_original)) if y_data_original[i][healthy_idx] == 1]\n",
    "print(f\"   Found {len(healthy_indices)} healthy samples\")\n",
    "\n",
    "# Randomly select subset\n",
    "np.random.seed(42)\n",
    "selected_healthy = np.random.choice(healthy_indices, size=min(target_healthy_count, len(healthy_indices)), replace=False)\n",
    "\n",
    "for idx in selected_healthy:\n",
    "    X_data_balanced.append(X_data_original[idx])\n",
    "    y_data_balanced.append(y_data_original[idx])\n",
    "    lengths_data_balanced.append(lengths_data_original[idx])\n",
    "\n",
    "print(f\"   ✓ Kept {len(selected_healthy)} healthy samples\")\n",
    "\n",
    "# 2. AUGMENT DISEASE CLASSES\n",
    "print(f\"\\n2. Augmenting disease classes...\")\n",
    "augmentation_types = ['noise', 'scale', 'shift', 'stretch']\n",
    "\n",
    "for class_idx in range(NUM_CLASSES):\n",
    "    if class_idx == healthy_idx:\n",
    "        continue  # Skip healthy\n",
    "    \n",
    "    disease_name = DISEASE_NAMES[class_idx]\n",
    "    \n",
    "    # Find all samples with this disease\n",
    "    disease_indices = [i for i in range(len(y_data_original)) \n",
    "                      if y_data_original[i][class_idx] == 1]\n",
    "    \n",
    "    if len(disease_indices) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Add original samples\n",
    "    for idx in disease_indices:\n",
    "        X_data_balanced.append(X_data_original[idx])\n",
    "        y_data_balanced.append(y_data_original[idx])\n",
    "        lengths_data_balanced.append(lengths_data_original[idx])\n",
    "    \n",
    "    # Calculate how many augmented samples to create\n",
    "    original_count = len(disease_indices)\n",
    "    augmented_needed = min(target_disease_count - original_count, original_count * 3)  # Max 3x augmentation\n",
    "    \n",
    "    if augmented_needed > 0:\n",
    "        print(f\"   {disease_name:40s}: {original_count:4d} → {original_count + augmented_needed:4d} samples (+{augmented_needed})\")\n",
    "        \n",
    "        # Create augmented samples\n",
    "        for _ in range(augmented_needed):\n",
    "            # Randomly select a sample to augment\n",
    "            idx = np.random.choice(disease_indices)\n",
    "            \n",
    "            # Randomly select augmentation type\n",
    "            aug_type = np.random.choice(augmentation_types)\n",
    "            \n",
    "            # Augment\n",
    "            ecg_aug = augment_ecg_signal(X_data_original[idx], augmentation_type=aug_type)\n",
    "            \n",
    "            X_data_balanced.append(ecg_aug)\n",
    "            y_data_balanced.append(y_data_original[idx].copy())\n",
    "            lengths_data_balanced.append(lengths_data_original[idx])\n",
    "\n",
    "# Convert to arrays\n",
    "y_data_balanced = np.array(y_data_balanced)\n",
    "lengths_data_balanced = np.array(lengths_data_balanced)\n",
    "\n",
    "print(f\"\\n✓ Balancing complete!\")\n",
    "print(f\"  Total samples: {len(X_data_balanced)} (was {len(X_data_original)})\")\n",
    "\n",
    "# Show AFTER balancing distribution\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLASS DISTRIBUTION - AFTER BALANCING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class_counts_after = y_data_balanced.sum(axis=0).astype(int)\n",
    "\n",
    "for i, (name, count_before, count_after) in enumerate(zip(DISEASE_NAMES, class_counts_before, class_counts_after)):\n",
    "    change = count_after - count_before\n",
    "    symbol = \"+\" if change > 0 else \"\"\n",
    "    print(f\"{name:40s}: {count_before:5d} → {count_after:5d} ({symbol}{change:+5d})\")\n",
    "\n",
    "print(f\"\\nTotal samples: {len(X_data_balanced)} (was {len(X_data_original)})\")\n",
    "print(f\"Healthy: {class_counts_after[healthy_idx]} ({100*class_counts_after[healthy_idx]/len(X_data_balanced):.1f}%)\")\n",
    "\n",
    "# Use balanced data for training\n",
    "X_data = X_data_balanced\n",
    "y_data = y_data_balanced\n",
    "lengths_data = lengths_data_balanced\n",
    "\n",
    "print(f\"\\n✓ Balanced dataset ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from HDF5...\n",
      "Loaded 14189 samples\n",
      "Length range: 5.0s - 112.0s\n",
      "Mean length: 27.0s\n",
      "\n",
      "Data split:\n",
      "  Train: 9932 samples\n",
      "  Val:   2128 samples\n",
      "  Test:  2129 samples\n"
     ]
    }
   ],
   "source": [
    "# Data loading function\n",
    "def load_data_from_hdf5(h5_file):\n",
    "    \"\"\"\n",
    "    Load all data from HDF5 file.\n",
    "    Returns X (list of variable-length arrays), y, lengths\n",
    "    \"\"\"\n",
    "    X_all = []\n",
    "    y_all = []\n",
    "    lengths_all = []\n",
    "    \n",
    "    with h5py.File(h5_file, 'r') as h5f:\n",
    "        for bin_name in sorted(h5f.keys()):\n",
    "            if not bin_name.startswith('bin_'):\n",
    "                continue\n",
    "            \n",
    "            grp = h5f[bin_name]\n",
    "            X_bin = grp['X'][:]\n",
    "            y_bin = grp['y'][:]\n",
    "            lengths_bin = grp['lengths'][:]\n",
    "            \n",
    "            # Trim each recording to actual length\n",
    "            for i in range(len(X_bin)):\n",
    "                X_all.append(X_bin[i, :lengths_bin[i], :])\n",
    "                y_all.append(y_bin[i])\n",
    "                lengths_all.append(lengths_bin[i])\n",
    "    \n",
    "    return X_all, np.array(y_all), np.array(lengths_all)\n",
    "\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data from HDF5...\")\n",
    "X_data, y_data, lengths_data = load_data_from_hdf5(HDF5_FILE)\n",
    "\n",
    "print(f\"Loaded {len(X_data)} samples\")\n",
    "print(f\"Length range: {lengths_data.min()/TARGET_FS:.1f}s - {lengths_data.max()/TARGET_FS:.1f}s\")\n",
    "print(f\"Mean length: {lengths_data.mean()/TARGET_FS:.1f}s\")\n",
    "\n",
    "# Split data\n",
    "indices = np.arange(len(X_data))\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.3, random_state=42)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"\\nData split:\")\n",
    "print(f\"  Train: {len(train_idx)} samples\")\n",
    "print(f\"  Val:   {len(val_idx)} samples\")\n",
    "print(f\"  Test:  {len(test_idx)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TensorFlow datasets...\n",
      "✓ Datasets created\n"
     ]
    }
   ],
   "source": [
    "# Create tf.data.Dataset for training\n",
    "def create_dataset(X_list, y_array, indices, batch_size=16, shuffle=True):\n",
    "    \"\"\"\n",
    "    Create tf.data.Dataset for variable-length recordings.\n",
    "    \"\"\"\n",
    "    def generator():\n",
    "        idx_list = indices.copy()\n",
    "        if shuffle:\n",
    "            np.random.shuffle(idx_list)\n",
    "        \n",
    "        for idx in idx_list:\n",
    "            yield X_list[idx].astype(np.float32), y_array[idx].astype(np.float32)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, 12), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(NUM_CLASSES,), dtype=tf.float32)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Padded batch (pads to max length in batch)\n",
    "    dataset = dataset.padded_batch(\n",
    "        batch_size,\n",
    "        padded_shapes=([None, 12], [NUM_CLASSES]),\n",
    "        padding_values=(0.0, 0.0)\n",
    "    )\n",
    "    \n",
    "    return dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "print(\"Creating TensorFlow datasets...\")\n",
    "train_dataset = create_dataset(X_data, y_data, train_idx, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataset = create_dataset(X_data, y_data, val_idx, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataset = create_dataset(X_data, y_data, test_idx, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"✓ Datasets created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new model...\n",
      "Compiling model...\n",
      "✓ Model compiled\n",
      "✓ HuggingFace checkpoint callback added\n",
      "✓ Setup complete. Ready to train from epoch 0\n"
     ]
    }
   ],
   "source": [
    "# Build or load model\n",
    "if RESUME_TRAINING:\n",
    "    print(\"Loading model from checkpoint...\")\n",
    "    model = keras.models.load_model(\n",
    "        RESUME_CHECKPOINT_PATH,\n",
    "        custom_objects={'FocalLoss': FocalLoss},\n",
    "        safe_mode=False  # Required for Lambda layers\n",
    "    )\n",
    "    print(\"✓ Model loaded from checkpoint\")\n",
    "else:\n",
    "    print(\"Building new model...\")\n",
    "    # Model already built in previous cell\n",
    "\n",
    "    # Compile model\n",
    "    print(\"Compiling model...\")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss=FocalLoss(alpha=0.25, gamma=2.0),\n",
    "        metrics=[\n",
    "            keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "            keras.metrics.AUC(name='auc', multi_label=True, num_labels=NUM_CLASSES),\n",
    "            keras.metrics.Precision(name='precision'),\n",
    "            keras.metrics.Recall(name='recall'),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"✓ Model compiled\")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_auc',\n",
    "        patience=15,\n",
    "        mode='max',\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(OUTPUT_DIR, 'best_model_v3.keras'),\n",
    "        monitor='val_auc',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.CSVLogger(\n",
    "        os.path.join(OUTPUT_DIR, 'training_history.csv')\n",
    "    )\n",
    "]\n",
    "\n",
    "# Add HuggingFace checkpoint callback if enabled\n",
    "if ENABLE_HF_UPLOAD and MODEL_REPO:\n",
    "    callbacks.insert(0, HuggingFaceCheckpoint(\n",
    "        repo_id=MODEL_REPO,\n",
    "        every_n_epochs=CHECKPOINT_EVERY\n",
    "    ))\n",
    "    print(\"✓ HuggingFace checkpoint callback added\")\n",
    "\n",
    "print(f\"✓ Setup complete. Ready to train from epoch {INITIAL_EPOCH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from epoch 0...\n",
      "\n",
      "Epoch 1/50\n",
      "    792/Unknown \u001b[1m1277s\u001b[0m 2s/step - accuracy: 0.6942 - auc: 0.4552 - loss: 4.1647 - precision: 0.0978 - recall: 0.4842"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "print(f\"Starting training from epoch {INITIAL_EPOCH}...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    initial_epoch=INITIAL_EPOCH,  # Resume from here if checkpoint exists\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history.history['loss'], label='Train Loss')\n",
    "axes[0, 0].plot(history.history['val_loss'], label='Val Loss')\n",
    "axes[0, 0].set_title('Loss', fontsize=12)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# AUC\n",
    "axes[0, 1].plot(history.history['auc'], label='Train AUC')\n",
    "axes[0, 1].plot(history.history['val_auc'], label='Val AUC')\n",
    "axes[0, 1].set_title('AUC', fontsize=12)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('AUC')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Accuracy\n",
    "axes[1, 0].plot(history.history['accuracy'], label='Train Accuracy')\n",
    "axes[1, 0].plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "axes[1, 0].set_title('Accuracy', fontsize=12)\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Precision & Recall\n",
    "axes[1, 1].plot(history.history['precision'], label='Train Precision', linestyle='--')\n",
    "axes[1, 1].plot(history.history['recall'], label='Train Recall', linestyle=':')\n",
    "axes[1, 1].plot(history.history['val_precision'], label='Val Precision', linestyle='--')\n",
    "axes[1, 1].plot(history.history['val_recall'], label='Val Recall', linestyle=':')\n",
    "axes[1, 1].set_title('Precision & Recall', fontsize=12)\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'training_history.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Training curves saved to {OUTPUT_DIR}/training_history.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation with Disease Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on test set\n",
    "print(\"Generating predictions on test set...\")\n",
    "\n",
    "y_true_test = y_data[test_idx]\n",
    "y_pred_proba_test = model.predict(test_dataset, verbose=1)\n",
    "y_pred_test = (y_pred_proba_test > 0.5).astype(int)\n",
    "\n",
    "print(\"✓ Predictions complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall metrics\n",
    "print(\"=\"*80)\n",
    "print(\"OVERALL TEST METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Subset accuracy (exact match)\n",
    "subset_acc = np.mean(np.all(y_true_test == y_pred_test, axis=1))\n",
    "print(f\"Subset Accuracy (Exact Match): {subset_acc:.4f}\")\n",
    "\n",
    "# Hamming loss\n",
    "hamming = hamming_loss(y_true_test, y_pred_test)\n",
    "print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "\n",
    "# F1 scores\n",
    "f1_micro = f1_score(y_true_test, y_pred_test, average='micro', zero_division=0)\n",
    "f1_macro = f1_score(y_true_test, y_pred_test, average='macro', zero_division=0)\n",
    "f1_weighted = f1_score(y_true_test, y_pred_test, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"F1-Score (Micro):    {f1_micro:.4f}\")\n",
    "print(f\"F1-Score (Macro):    {f1_macro:.4f}\")\n",
    "print(f\"F1-Score (Weighted): {f1_weighted:.4f}\")\n",
    "\n",
    "# Precision & Recall\n",
    "prec_micro = precision_score(y_true_test, y_pred_test, average='micro', zero_division=0)\n",
    "rec_micro = recall_score(y_true_test, y_pred_test, average='micro', zero_division=0)\n",
    "\n",
    "print(f\"Precision (Micro):   {prec_micro:.4f}\")\n",
    "print(f\"Recall (Micro):      {rec_micro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class metrics with DISEASE NAMES ONLY (no ICD codes)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PER-CLASS METRICS (Grouped by Disease Category)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "per_class_results = {}\n",
    "\n",
    "# Group by disease category\n",
    "for group_name, group_icds in DISEASE_GROUPS.items():\n",
    "    print(f\"\\n{group_name.upper()}:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for icd in group_icds:\n",
    "        if icd not in ICD_CODES:\n",
    "            continue\n",
    "        \n",
    "        idx = ICD_CODES.index(icd)\n",
    "        disease_name = DISEASE_NAMES[idx]  # Get disease name\n",
    "        \n",
    "        y_true_class = y_true_test[:, idx]\n",
    "        y_pred_class = y_pred_test[:, idx]\n",
    "        y_pred_proba_class = y_pred_proba_test[:, idx]\n",
    "        \n",
    "        support = int(y_true_class.sum())\n",
    "        \n",
    "        if support > 0:\n",
    "            precision = precision_score(y_true_class, y_pred_class, zero_division=0)\n",
    "            recall = recall_score(y_true_class, y_pred_class, zero_division=0)\n",
    "            f1 = f1_score(y_true_class, y_pred_class, zero_division=0)\n",
    "            \n",
    "            try:\n",
    "                roc_auc = roc_auc_score(y_true_class, y_pred_proba_class)\n",
    "            except:\n",
    "                roc_auc = 0.0\n",
    "            \n",
    "            per_class_results[disease_name] = {\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'roc_auc': roc_auc,\n",
    "                'support': support\n",
    "            }\n",
    "            \n",
    "            # Print ONLY disease name (no ICD code)\n",
    "            print(f\"\\n{disease_name}:\")  # Changed from showing ICD code\n",
    "            print(f\"  Support:   {support:4d}\")\n",
    "            print(f\"  Precision: {precision:.4f}\")\n",
    "            print(f\"  Recall:    {recall:.4f}\")\n",
    "            print(f\"  F1-Score:  {f1:.4f}\")\n",
    "            print(f\"  ROC-AUC:   {roc_auc:.4f}\")\n",
    "\n",
    "# Healthy class\n",
    "print(f\"\\nHEALTHY:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "idx = ICD_CODES.index('Healthy')\n",
    "disease_name = DISEASE_NAMES[idx]  # \"Healthy\"\n",
    "\n",
    "y_true_class = y_true_test[:, idx]\n",
    "y_pred_class = y_pred_test[:, idx]\n",
    "y_pred_proba_class = y_pred_proba_test[:, idx]\n",
    "\n",
    "support = int(y_true_class.sum())\n",
    "\n",
    "if support > 0:\n",
    "    precision = precision_score(y_true_class, y_pred_class, zero_division=0)\n",
    "    recall = recall_score(y_true_class, y_pred_class, zero_division=0)\n",
    "    f1 = f1_score(y_true_class, y_pred_class, zero_division=0)\n",
    "    \n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_true_class, y_pred_proba_class)\n",
    "    except:\n",
    "        roc_auc = 0.0\n",
    "    \n",
    "    per_class_results[disease_name] = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'support': support\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{disease_name}:\")  # Just \"Healthy\"\n",
    "    print(f\"  Support:   {support:4d}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "    print(f\"  ROC-AUC:   {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_prediction_with_names(sample_idx, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Display prediction with DISEASE NAMES ONLY (no ICD codes).\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Sample {sample_idx + 1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get data\n",
    "    test_sample_idx = test_idx[sample_idx]\n",
    "    y_true = y_true_test[sample_idx]\n",
    "    y_pred_proba = y_pred_proba_test[sample_idx]\n",
    "    \n",
    "    # True labels - DISEASE NAMES ONLY\n",
    "    true_indices = np.where(y_true == 1)[0]\n",
    "    print(f\"\\nTrue Diagnoses ({len(true_indices)}):\")\n",
    "    if len(true_indices) > 0:\n",
    "        for idx in true_indices:\n",
    "            print(f\"  - {DISEASE_NAMES[idx]}\")  # ONLY disease name\n",
    "    else:\n",
    "        print(\"  - None\")\n",
    "    \n",
    "    # Predicted labels - DISEASE NAMES ONLY\n",
    "    pred_indices = np.where(y_pred_proba > threshold)[0]\n",
    "    print(f\"\\nPredicted Diagnoses (threshold={threshold}):\")\n",
    "    if len(pred_indices) > 0:\n",
    "        for idx in sorted(pred_indices, key=lambda i: -y_pred_proba[i]):\n",
    "            prob = y_pred_proba[idx]\n",
    "            marker = \"✓\" if y_true[idx] == 1 else \" \"\n",
    "            print(f\"  {marker} {DISEASE_NAMES[idx]}: {prob:.4f}\")  # ONLY disease name\n",
    "    else:\n",
    "        print(\"  - None\")\n",
    "    \n",
    "    # Top 5 predictions - DISEASE NAMES ONLY\n",
    "    print(f\"\\nTop 5 Predictions (all probabilities):\")\n",
    "    top_5 = np.argsort(y_pred_proba)[-5:][::-1]\n",
    "    for idx in top_5:\n",
    "        prob = y_pred_proba[idx]\n",
    "        marker = \"✓\" if y_true[idx] == 1 else \" \"\n",
    "        print(f\"  {marker} {DISEASE_NAMES[idx]}: {prob:.4f}\")  # ONLY disease name\n",
    "\n",
    "\n",
    "# Show predictions for random samples\n",
    "print(\"\\nSample Predictions:\")\n",
    "random_samples = np.random.choice(len(test_idx), size=min(5, len(test_idx)), replace=False)\n",
    "for i in random_samples:\n",
    "    show_prediction_with_names(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ECG signal with predictions\n",
    "def plot_ecg_with_predictions(sample_idx, num_leads=3):\n",
    "    \"\"\"\n",
    "    Plot ECG signal with true and predicted labels (DISEASE NAMES ONLY).\n",
    "    \"\"\"\n",
    "    test_sample_idx = test_idx[sample_idx]\n",
    "    ecg_signal = X_data[test_sample_idx]\n",
    "    y_true = y_true_test[sample_idx]\n",
    "    y_pred_proba = y_pred_proba_test[sample_idx]\n",
    "    \n",
    "    # Get disease names\n",
    "    true_labels = [DISEASE_NAMES[j] for j in range(NUM_CLASSES) if y_true[j] == 1]\n",
    "    pred_labels = [(DISEASE_NAMES[j], y_pred_proba[j]) \n",
    "                  for j in range(NUM_CLASSES) if y_pred_proba[j] > 0.5]\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(num_leads, 1, figsize=(15, 8))\n",
    "    if num_leads == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    time = np.arange(len(ecg_signal)) / TARGET_FS\n",
    "    lead_names = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "    \n",
    "    for i in range(num_leads):\n",
    "        axes[i].plot(time, ecg_signal[:, i], linewidth=0.8)\n",
    "        axes[i].set_ylabel(f'Lead {lead_names[i]}', fontsize=10)\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[-1].set_xlabel('Time (s)', fontsize=10)\n",
    "    \n",
    "    # Title with disease names\n",
    "    title = f'Sample {sample_idx + 1} - ECG Signal ({len(ecg_signal)/TARGET_FS:.1f}s)\\n'\n",
    "    title += f'True: {\", \".join(true_labels) if true_labels else \"None\"}\\n'\n",
    "    if pred_labels:\n",
    "        pred_str = \", \".join([f\"{l} ({p:.2f})\" for l, p in pred_labels])\n",
    "        title += f'Predicted: {pred_str}'\n",
    "    else:\n",
    "        title += 'Predicted: None'\n",
    "    \n",
    "    plt.suptitle(title, fontsize=11, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot a few examples\n",
    "print(\"\\nECG Signal Visualizations:\")\n",
    "for i in random_samples[:3]:\n",
    "    plot_ecg_with_predictions(i, num_leads=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Model & Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model_path = os.path.join(OUTPUT_DIR, f'{MODEL_NAME}_final.keras')\n",
    "model.save(final_model_path)\n",
    "print(f\"✓ Model saved to {final_model_path}\")\n",
    "\n",
    "# Save comprehensive metadata with DISEASE NAMES\n",
    "results_metadata = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'version': NOTEBOOK_VERSION,\n",
    "    'created': datetime.now().isoformat(),\n",
    "    \n",
    "    # Classes - DISEASE NAMES ONLY in main list\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'disease_names': DISEASE_NAMES,  # Main output list - names only\n",
    "    'icd_codes': ICD_CODES,  # Reference only\n",
    "    'icd_to_disease_name': ICD_TO_DISEASE_NAME,  # Mapping reference\n",
    "    'disease_groups': DISEASE_GROUPS,\n",
    "    \n",
    "    # Architecture\n",
    "    'architecture': {\n",
    "        'type': 'Enhanced 1D CNN',\n",
    "        'blocks': '64→128→256→512 filters',\n",
    "        'features': ['Squeeze-Excitation', 'Temporal Attention', 'Residual Connections'],\n",
    "        'pooling': 'Adaptive (Global Avg + Max)',\n",
    "        'total_params': int(model.count_params())\n",
    "    },\n",
    "    \n",
    "    # Training\n",
    "    'training': {\n",
    "        'epochs': len(history.history['loss']),\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'loss': 'Focal Loss (alpha=0.25, gamma=2.0)',\n",
    "        'optimizer': 'Adam'\n",
    "    },\n",
    "    \n",
    "    # Data\n",
    "    'data': {\n",
    "        'total_samples': len(X_data),\n",
    "        'train_samples': len(train_idx),\n",
    "        'val_samples': len(val_idx),\n",
    "        'test_samples': len(test_idx),\n",
    "        'sampling_rate': TARGET_FS,\n",
    "        'num_channels': TARGET_CHANNELS,\n",
    "        'variable_length': True,\n",
    "        'length_range': f\"{lengths_data.min()/TARGET_FS:.1f}s - {lengths_data.max()/TARGET_FS:.1f}s\"\n",
    "    },\n",
    "    \n",
    "    # Test metrics\n",
    "    'test_metrics': {\n",
    "        'subset_accuracy': float(subset_acc),\n",
    "        'hamming_loss': float(hamming),\n",
    "        'f1_micro': float(f1_micro),\n",
    "        'f1_macro': float(f1_macro),\n",
    "        'f1_weighted': float(f1_weighted),\n",
    "        'precision_micro': float(prec_micro),\n",
    "        'recall_micro': float(rec_micro)\n",
    "    },\n",
    "    \n",
    "    # Per-class results (DISEASE NAMES ONLY)\n",
    "    'per_class_results': per_class_results\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = os.path.join(OUTPUT_DIR, 'model_results.json')\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(results_metadata, f, indent=2)\n",
    "\n",
    "print(f\"✓ Metadata saved to {metadata_path}\")\n",
    "\n",
    "# Upload to HuggingFace (if enabled)\n",
    "if ENABLE_HF_UPLOAD and MODEL_REPO and api:\n",
    "    print(f\"\\nUploading final model to HuggingFace...\")\n",
    "\n",
    "    try:\n",
    "        # Upload model\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=final_model_path,\n",
    "            path_in_repo=\"final_model.keras\",\n",
    "            repo_id=MODEL_REPO,\n",
    "            repo_type=\"model\",\n",
    "            commit_message=\"Final trained model\"\n",
    "        )\n",
    "        print(\"✓ Final model uploaded\")\n",
    "\n",
    "        # Upload metadata\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=metadata_path,\n",
    "            path_in_repo=\"model_results.json\",\n",
    "            repo_id=MODEL_REPO,\n",
    "            repo_type=\"model\",\n",
    "            commit_message=\"Add model metadata\"\n",
    "        )\n",
    "        print(\"✓ Metadata uploaded\")\n",
    "\n",
    "        # Upload training history\n",
    "        history_csv = os.path.join(OUTPUT_DIR, 'training_history.csv')\n",
    "        if os.path.exists(history_csv):\n",
    "            api.upload_file(\n",
    "                path_or_fileobj=history_csv,\n",
    "                path_in_repo=\"training_history.csv\",\n",
    "                repo_id=MODEL_REPO,\n",
    "                repo_type=\"model\",\n",
    "                commit_message=\"Add training history\"\n",
    "            )\n",
    "            print(\"✓ Training history uploaded\")\n",
    "\n",
    "        # Upload training plot\n",
    "        history_plot = os.path.join(OUTPUT_DIR, 'training_history.png')\n",
    "        if os.path.exists(history_plot):\n",
    "            api.upload_file(\n",
    "                path_or_fileobj=history_plot,\n",
    "                path_in_repo=\"training_history.png\",\n",
    "                repo_id=MODEL_REPO,\n",
    "                repo_type=\"model\",\n",
    "                commit_message=\"Add training visualization\"\n",
    "            )\n",
    "            print(\"✓ Training visualization uploaded\")\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"✓ All artifacts uploaded to HuggingFace!\")\n",
    "        print(f\"View at: https://huggingface.co/{MODEL_REPO}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not upload to HF: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: {final_model_path}\")\n",
    "print(f\"Metadata: {metadata_path}\")\n",
    "print(f\"Training history: {os.path.join(OUTPUT_DIR, 'training_history.csv')}\")\n",
    "print(f\"\\nTest F1-Score (Macro): {f1_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Generate Model Card (HuggingFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate model card for HuggingFace (if enabled)\n",
    "if ENABLE_HF_UPLOAD and MODEL_REPO:\n",
    "    disease_names_str = '\\n'.join([f\"{i+1}. {name}\" for i, name in enumerate(DISEASE_NAMES)])\n",
    "\n",
    "    model_card = f\"\"\"---\n",
    "language: en\n",
    "tags:\n",
    "- ecg\n",
    "- cardiology\n",
    "- medical\n",
    "- pediatric\n",
    "- time-series\n",
    "- multi-label-classification\n",
    "- tensorflow\n",
    "- cnn\n",
    "datasets:\n",
    "- Neural-Network-Project/ECG-database\n",
    "metrics:\n",
    "- f1\n",
    "- auc\n",
    "- precision\n",
    "- recall\n",
    "library_name: tensorflow\n",
    "---\n",
    "\n",
    "# ECG Disease Classifier - 19 Cardiac Conditions\n",
    "\n",
    "Multi-label classification model for detecting 19 cardiac conditions from pediatric ECG signals.\n",
    "\n",
    "## Model Description\n",
    "\n",
    "Enhanced 1D CNN with Squeeze-Excitation blocks and temporal attention for variable-length ECG classification.\n",
    "\n",
    "**Architecture:** 64→128→256→512 filters with residual connections\n",
    "**Training:** Focal loss for class imbalance\n",
    "**Input:** Variable-length 12-lead ECG (5-120 seconds at 500 Hz)\n",
    "\n",
    "## Disease Classes\n",
    "\n",
    "{disease_names_str}\n",
    "\n",
    "## Performance\n",
    "\n",
    "- F1-Score (Macro): {f1_macro:.4f}\n",
    "- F1-Score (Micro): {f1_micro:.4f}\n",
    "- Subset Accuracy: {subset_acc:.4f}\n",
    "\n",
    "## Intended Use\n",
    "\n",
    "⚠️ **Research and educational purposes only** - NOT for clinical diagnosis\n",
    "\n",
    "## Training Details\n",
    "\n",
    "- Batch Size: {BATCH_SIZE}\n",
    "- Epochs: {len(history.history['loss'])}\n",
    "- Loss: Focal Loss (α=0.25, γ=2.0)\n",
    "- Optimizer: Adam (lr={LEARNING_RATE})\n",
    "\n",
    "## Citation\n",
    "\n",
    "```bibtex\n",
    "@misc{{ecg-classifier-2025,\n",
    "  author = {{Neural-Network-Project}},\n",
    "  title = {{ECG Disease Classifier}},\n",
    "  year = {{2025}},\n",
    "  publisher = {{Hugging Face}},\n",
    "  url = {{https://huggingface.co/{MODEL_REPO}}}\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    # Save locally\n",
    "    readme_path = os.path.join(OUTPUT_DIR, 'README.md')\n",
    "    with open(readme_path, 'w') as f:\n",
    "        f.write(model_card)\n",
    "\n",
    "    print(\"✓ Model card created\")\n",
    "\n",
    "    # Upload to HuggingFace\n",
    "    try:\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=readme_path,\n",
    "            path_in_repo=\"README.md\",\n",
    "            repo_id=MODEL_REPO,\n",
    "            repo_type=\"model\",\n",
    "            commit_message=\"Add model card\"\n",
    "        )\n",
    "        print(\"✓ Model card uploaded to HuggingFace\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not upload model card: {e}\")\n",
    "else:\n",
    "    print(\"Skipping model card generation (ENABLE_HF_UPLOAD=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements an enhanced 1D CNN for 19-class multi-label ECG classification:\n",
    "\n",
    "**Key Features:**\n",
    "- ✅ Processes full-length variable recordings (5-120 seconds, no windowing)\n",
    "- ✅ 19 classes (18 cardiac diseases + Healthy)\n",
    "- ✅ Enhanced architecture with SE blocks and temporal attention\n",
    "- ✅ Focal loss for class imbalance\n",
    "- ✅ **All outputs show disease names only** (not ICD codes)\n",
    "\n",
    "**Architecture:**\n",
    "- 64→128→256→512 filters with residual connections\n",
    "- Squeeze-Excitation blocks for channel attention\n",
    "- Temporal attention for time-series focus\n",
    "- Adaptive pooling for variable lengths\n",
    "\n",
    "**Next Steps:**\n",
    "- Fine-tune threshold per class\n",
    "- Analyze attention weights\n",
    "- Deploy for inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
